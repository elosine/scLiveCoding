{
    "docs": [
        {
            "location": "/", 
            "text": "howto_co34pt_liveCode\n\n\n\n\nAbout\n\n\nhowto_co34pt_liveCode is an attempt to extensively document my live-coding music practice. I live code as \nco\u00ef\u00bf\u00a5\u00ef\u00be\u00a1pt\n, mostly making dance music (including for \nAlgoraves\n), but I have also employed live coding as part of some \nother projects\n. This repo contains a number of articles and essays listed in this contents page which cover various aspects of my live coding practice from the ground up, and also contains a number of files to support your use of SuperCollider in the way that I use it. More info about this in 'What This Repo Is'\n\n\nThis resource is hosted both \non GitHub\n and \non GitHub Pages\n (you will likely already be on one of these). I'd recommend browsing articles on GitHub pages, and if you want to use any of the examples, see any SuperCollider code and use my setup I'd recommend downloading the repo, this will be covered in 'How To Use This Repo'\n\n\nThis repository is currently mostly finished, but does need proofreading, which I will be doing soon. It will be continually updated as my live coding practice develops.\n\n\nI'm always keen to know the ways in which this has been helpful to anyone, or any comments you have at all.\nDrop me a line on \nTwitter\n, \nGitHub\n or via \nEmail\n\n\nCONTENTS\n\n\n\n\nIntroduction:\n\n\nWhat this Repo Is\n\n\nWhy I live Code\n\n\nWhy SuperCollider?\n\n\nHow To Use This Repo\n\n\n\n\n\n\nBasics:\n\n\nRecommended Addons\n\n\nProxySpace - Live Coding in SuperCollider\n\n\nSetup Code: Making Performance Easier\n\n\nPbinds and Patterns: The Basics\n\n\n\n\n\n\nRhythm:\n\n\nRhythmic Construction for Algorave Sets\n\n\nBasic Rhythms\n\n\nTechniques for Modifying Rhythm\n\n\nEuclidean Rhythms\n\n\nStageLimiter Abuse\n\n\nL-Systems for Rhythm\n\n\nLooping\n\n\n\n\n\n\nMelody and Pitch:\n\n\nPitch and Patterns\n\n\nTypes of Pitch Arrangement\n\n\nRiffs\n\n\nPitch and Static Synths\n\n\nBetween Pitch and Noise\n\n\nGood SynthDef Writing for co34pt_livecode\n\n\nUsing MIDI\n\n\n\n\n\n\nNon-Pattern Techniques:\n\n\nDrones\n\n\nSuperCollider as a Modular Synth\n\n\n\n\n\n\nVisuals:\n\n\nFreqScope and Visuals\n\n\nOSC and Data Streams\n\n\nUsing Datasets\n\n\n\n\n\n\n\n\nIf any content does not work, please open an issue/pull request. These examples have been tested on *ubuntu 16.10 only thus far.", 
            "title": "Contents"
        }, 
        {
            "location": "/#howto_co34pt_livecode", 
            "text": "", 
            "title": "howto_co34pt_liveCode"
        }, 
        {
            "location": "/#about", 
            "text": "howto_co34pt_liveCode is an attempt to extensively document my live-coding music practice. I live code as  co\u00ef\u00bf\u00a5\u00ef\u00be\u00a1pt , mostly making dance music (including for  Algoraves ), but I have also employed live coding as part of some  other projects . This repo contains a number of articles and essays listed in this contents page which cover various aspects of my live coding practice from the ground up, and also contains a number of files to support your use of SuperCollider in the way that I use it. More info about this in 'What This Repo Is'  This resource is hosted both  on GitHub  and  on GitHub Pages  (you will likely already be on one of these). I'd recommend browsing articles on GitHub pages, and if you want to use any of the examples, see any SuperCollider code and use my setup I'd recommend downloading the repo, this will be covered in 'How To Use This Repo'  This repository is currently mostly finished, but does need proofreading, which I will be doing soon. It will be continually updated as my live coding practice develops.  I'm always keen to know the ways in which this has been helpful to anyone, or any comments you have at all.\nDrop me a line on  Twitter ,  GitHub  or via  Email", 
            "title": "About"
        }, 
        {
            "location": "/#contents", 
            "text": "Introduction:  What this Repo Is  Why I live Code  Why SuperCollider?  How To Use This Repo    Basics:  Recommended Addons  ProxySpace - Live Coding in SuperCollider  Setup Code: Making Performance Easier  Pbinds and Patterns: The Basics    Rhythm:  Rhythmic Construction for Algorave Sets  Basic Rhythms  Techniques for Modifying Rhythm  Euclidean Rhythms  StageLimiter Abuse  L-Systems for Rhythm  Looping    Melody and Pitch:  Pitch and Patterns  Types of Pitch Arrangement  Riffs  Pitch and Static Synths  Between Pitch and Noise  Good SynthDef Writing for co34pt_livecode  Using MIDI    Non-Pattern Techniques:  Drones  SuperCollider as a Modular Synth    Visuals:  FreqScope and Visuals  OSC and Data Streams  Using Datasets     If any content does not work, please open an issue/pull request. These examples have been tested on *ubuntu 16.10 only thus far.", 
            "title": "CONTENTS"
        }, 
        {
            "location": "/todo/", 
            "text": "A list of techniques, subjects, etc I haven't yet covered\n\n\n\n\nField recordings and longer samples, and how to use them\n\n\nFor this I need to put some field recordings inside of the repo\n\n\n\n\n\n\nAdding Effects? - New Snippets\n\n\nUpload my entire sample library online", 
            "title": "To-Do list"
        }, 
        {
            "location": "/todo/#a-list-of-techniques-subjects-etc-i-havent-yet-covered", 
            "text": "Field recordings and longer samples, and how to use them  For this I need to put some field recordings inside of the repo    Adding Effects? - New Snippets  Upload my entire sample library online", 
            "title": "A list of techniques, subjects, etc I haven't yet covered"
        }, 
        {
            "location": "/legalcode/", 
            "text": "Creative Commons Legal Code\n\n\nAttribution-ShareAlike 3.0 Unported\n\n\nCREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\nLEGAL SERVICES. DISTRIBUTION OF THIS LICENSE DOES NOT CREATE AN\nATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\nINFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\nREGARDING THE INFORMATION PROVIDED, AND DISCLAIMS LIABILITY FOR\nDAMAGES RESULTING FROM ITS USE.\n\n\n\nLicense\n\n\nTHE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE\nCOMMONS PUBLIC LICENSE (\"CCPL\" OR \"LICENSE\"). THE WORK IS PROTECTED BY\nCOPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS\nAUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.\n\n\nBY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE\nTO BE BOUND BY THE TERMS OF THIS LICENSE. TO THE EXTENT THIS LICENSE MAY\nBE CONSIDERED TO BE A CONTRACT, THE LICENSOR GRANTS YOU THE RIGHTS\nCONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND\nCONDITIONS.\n\n\n\n\nDefinitions\n\n\n\n\na. \"Adaptation\" means a work based upon the Work, or upon the Work and\n    other pre-existing works, such as a translation, adaptation,\n    derivative work, arrangement of music or other alterations of a\n    literary or artistic work, or phonogram or performance and includes\n    cinematographic adaptations or any other form in which the Work may be\n    recast, transformed, or adapted including in any form recognizably\n    derived from the original, except that a work that constitutes a\n    Collection will not be considered an Adaptation for the purpose of\n    this License. For the avoidance of doubt, where the Work is a musical\n    work, performance or phonogram, the synchronization of the Work in\n    timed-relation with a moving image (\"synching\") will be considered an\n    Adaptation for the purpose of this License.\n b. \"Collection\" means a collection of literary or artistic works, such as\n    encyclopedias and anthologies, or performances, phonograms or\n    broadcasts, or other works or subject matter other than works listed\n    in Section 1(f) below, which, by reason of the selection and\n    arrangement of their contents, constitute intellectual creations, in\n    which the Work is included in its entirety in unmodified form along\n    with one or more other contributions, each constituting separate and\n    independent works in themselves, which together are assembled into a\n    collective whole. A work that constitutes a Collection will not be\n    considered an Adaptation (as defined below) for the purposes of this\n    License.\n c. \"Creative Commons Compatible License\" means a license that is listed\n    at https://creativecommons.org/compatiblelicenses that has been\n    approved by Creative Commons as being essentially equivalent to this\n    License, including, at a minimum, because that license: (i) contains\n    terms that have the same purpose, meaning and effect as the License\n    Elements of this License; and, (ii) explicitly permits the relicensing\n    of adaptations of works made available under that license under this\n    License or a Creative Commons jurisdiction license with the same\n    License Elements as this License.\n d. \"Distribute\" means to make available to the public the original and\n    copies of the Work or Adaptation, as appropriate, through sale or\n    other transfer of ownership.\n e. \"License Elements\" means the following high-level license attributes\n    as selected by Licensor and indicated in the title of this License:\n    Attribution, ShareAlike.\n f. \"Licensor\" means the individual, individuals, entity or entities that\n    offer(s) the Work under the terms of this License.\n g. \"Original Author\" means, in the case of a literary or artistic work,\n    the individual, individuals, entity or entities who created the Work\n    or if no individual or entity can be identified, the publisher; and in\n    addition (i) in the case of a performance the actors, singers,\n    musicians, dancers, and other persons who act, sing, deliver, declaim,\n    play in, interpret or otherwise perform literary or artistic works or\n    expressions of folklore; (ii) in the case of a phonogram the producer\n    being the person or legal entity who first fixes the sounds of a\n    performance or other sounds; and, (iii) in the case of broadcasts, the\n    organization that transmits the broadcast.\n h. \"Work\" means the literary and/or artistic work offered under the terms\n    of this License including without limitation any production in the\n    literary, scientific and artistic domain, whatever may be the mode or\n    form of its expression including digital form, such as a book,\n    pamphlet and other writing; a lecture, address, sermon or other work\n    of the same nature; a dramatic or dramatico-musical work; a\n    choreographic work or entertainment in dumb show; a musical\n    composition with or without words; a cinematographic work to which are\n    assimilated works expressed by a process analogous to cinematography;\n    a work of drawing, painting, architecture, sculpture, engraving or\n    lithography; a photographic work to which are assimilated works\n    expressed by a process analogous to photography; a work of applied\n    art; an illustration, map, plan, sketch or three-dimensional work\n    relative to geography, topography, architecture or science; a\n    performance; a broadcast; a phonogram; a compilation of data to the\n    extent it is protected as a copyrightable work; or a work performed by\n    a variety or circus performer to the extent it is not otherwise\n    considered a literary or artistic work.\n i. \"You\" means an individual or entity exercising rights under this\n    License who has not previously violated the terms of this License with\n    respect to the Work, or who has received express permission from the\n    Licensor to exercise rights under this License despite a previous\n    violation.\n j. \"Publicly Perform\" means to perform public recitations of the Work and\n    to communicate to the public those public recitations, by any means or\n    process, including by wire or wireless means or public digital\n    performances; to make available to the public Works in such a way that\n    members of the public may access these Works from a place and at a\n    place individually chosen by them; to perform the Work to the public\n    by any means or process and the communication to the public of the\n    performances of the Work, including by public digital performance; to\n    broadcast and rebroadcast the Work by any means including signs,\n    sounds or images.\n k. \"Reproduce\" means to make copies of the Work by any means including\n    without limitation by sound or visual recordings and the right of\n    fixation and reproducing fixations of the Work, including storage of a\n    protected performance or phonogram in digital form or other electronic\n    medium.\n\n\n\n\n\n\nFair Dealing Rights. Nothing in this License is intended to reduce,\nlimit, or restrict any uses free from copyright or rights arising from\nlimitations or exceptions that are provided for in connection with the\ncopyright protection under copyright law or other applicable laws.\n\n\n\n\n\n\nLicense Grant. Subject to the terms and conditions of this License,\nLicensor hereby grants You a worldwide, royalty-free, non-exclusive,\nperpetual (for the duration of the applicable copyright) license to\nexercise the rights in the Work as stated below:\n\n\n\n\n\n\na. to Reproduce the Work, to incorporate the Work into one or more\n    Collections, and to Reproduce the Work as incorporated in the\n    Collections;\n b. to create and Reproduce Adaptations provided that any such Adaptation,\n    including any translation in any medium, takes reasonable steps to\n    clearly label, demarcate or otherwise identify that changes were made\n    to the original Work. For example, a translation could be marked \"The\n    original work was translated from English to Spanish,\" or a\n    modification could indicate \"The original work has been modified.\";\n c. to Distribute and Publicly Perform the Work including as incorporated\n    in Collections; and,\n d. to Distribute and Publicly Perform Adaptations.\n e. For the avoidance of doubt:\n\n\n i. Non-waivable Compulsory License Schemes. In those jurisdictions in\n    which the right to collect royalties through any statutory or\n    compulsory licensing scheme cannot be waived, the Licensor\n    reserves the exclusive right to collect such royalties for any\n    exercise by You of the rights granted under this License;\nii. Waivable Compulsory License Schemes. In those jurisdictions in\n    which the right to collect royalties through any statutory or\n    compulsory licensing scheme can be waived, the Licensor waives the\n    exclusive right to collect such royalties for any exercise by You\n    of the rights granted under this License; and,\n\n\n\niii. Voluntary License Schemes. The Licensor waives the right to\n        collect royalties, whether individually or, in the event that the\n        Licensor is a member of a collecting society that administers\n        voluntary licensing schemes, via that society, from any exercise\n        by You of the rights granted under this License.\n\n\nThe above rights may be exercised in all media and formats whether now\nknown or hereafter devised. The above rights include the right to make\nsuch modifications as are technically necessary to exercise the rights in\nother media and formats. Subject to Section 8(f), all rights not expressly\ngranted by Licensor are hereby reserved.\n\n\n\n\nRestrictions. The license granted in Section 3 above is expressly made\nsubject to and limited by the following restrictions:\n\n\n\n\na. You may Distribute or Publicly Perform the Work only under the terms\n    of this License. You must include a copy of, or the Uniform Resource\n    Identifier (URI) for, this License with every copy of the Work You\n    Distribute or Publicly Perform. You may not offer or impose any terms\n    on the Work that restrict the terms of this License or the ability of\n    the recipient of the Work to exercise the rights granted to that\n    recipient under the terms of the License. You may not sublicense the\n    Work. You must keep intact all notices that refer to this License and\n    to the disclaimer of warranties with every copy of the Work You\n    Distribute or Publicly Perform. When You Distribute or Publicly\n    Perform the Work, You may not impose any effective technological\n    measures on the Work that restrict the ability of a recipient of the\n    Work from You to exercise the rights granted to that recipient under\n    the terms of the License. This Section 4(a) applies to the Work as\n    incorporated in a Collection, but this does not require the Collection\n    apart from the Work itself to be made subject to the terms of this\n    License. If You create a Collection, upon notice from any Licensor You\n    must, to the extent practicable, remove from the Collection any credit\n    as required by Section 4(c), as requested. If You create an\n    Adaptation, upon notice from any Licensor You must, to the extent\n    practicable, remove from the Adaptation any credit as required by\n    Section 4(c), as requested.\n b. You may Distribute or Publicly Perform an Adaptation only under the\n    terms of: (i) this License; (ii) a later version of this License with\n    the same License Elements as this License; (iii) a Creative Commons\n    jurisdiction license (either this or a later license version) that\n    contains the same License Elements as this License (e.g.,\n    Attribution-ShareAlike 3.0 US)); (iv) a Creative Commons Compatible\n    License. If you license the Adaptation under one of the licenses\n    mentioned in (iv), you must comply with the terms of that license. If\n    you license the Adaptation under the terms of any of the licenses\n    mentioned in (i), (ii) or (iii) (the \"Applicable License\"), you must\n    comply with the terms of the Applicable License generally and the\n    following provisions: (I) You must include a copy of, or the URI for,\n    the Applicable License with every copy of each Adaptation You\n    Distribute or Publicly Perform; (II) You may not offer or impose any\n    terms on the Adaptation that restrict the terms of the Applicable\n    License or the ability of the recipient of the Adaptation to exercise\n    the rights granted to that recipient under the terms of the Applicable\n    License; (III) You must keep intact all notices that refer to the\n    Applicable License and to the disclaimer of warranties with every copy\n    of the Work as included in the Adaptation You Distribute or Publicly\n    Perform; (IV) when You Distribute or Publicly Perform the Adaptation,\n    You may not impose any effective technological measures on the\n    Adaptation that restrict the ability of a recipient of the Adaptation\n    from You to exercise the rights granted to that recipient under the\n    terms of the Applicable License. This Section 4(b) applies to the\n    Adaptation as incorporated in a Collection, but this does not require\n    the Collection apart from the Adaptation itself to be made subject to\n    the terms of the Applicable License.\n c. If You Distribute, or Publicly Perform the Work or any Adaptations or\n    Collections, You must, unless a request has been made pursuant to\n    Section 4(a), keep intact all copyright notices for the Work and\n    provide, reasonable to the medium or means You are utilizing: (i) the\n    name of the Original Author (or pseudonym, if applicable) if supplied,\n    and/or if the Original Author and/or Licensor designate another party\n    or parties (e.g., a sponsor institute, publishing entity, journal) for\n    attribution (\"Attribution Parties\") in Licensor's copyright notice,\n    terms of service or by other reasonable means, the name of such party\n    or parties; (ii) the title of the Work if supplied; (iii) to the\n    extent reasonably practicable, the URI, if any, that Licensor\n    specifies to be associated with the Work, unless such URI does not\n    refer to the copyright notice or licensing information for the Work;\n    and (iv) , consistent with Ssection 3(b), in the case of an\n    Adaptation, a credit identifying the use of the Work in the Adaptation\n    (e.g., \"French translation of the Work by Original Author,\" or\n    \"Screenplay based on original Work by Original Author\"). The credit\n    required by this Section 4(c) may be implemented in any reasonable\n    manner; provided, however, that in the case of a Adaptation or\n    Collection, at a minimum such credit will appear, if a credit for all\n    contributing authors of the Adaptation or Collection appears, then as\n    part of these credits and in a manner at least as prominent as the\n    credits for the other contributing authors. For the avoidance of\n    doubt, You may only use the credit required by this Section for the\n    purpose of attribution in the manner set out above and, by exercising\n    Your rights under this License, You may not implicitly or explicitly\n    assert or imply any connection with, sponsorship or endorsement by the\n    Original Author, Licensor and/or Attribution Parties, as appropriate,\n    of You or Your use of the Work, without the separate, express prior\n    written permission of the Original Author, Licensor and/or Attribution\n    Parties.\n d. Except as otherwise agreed in writing by the Licensor or as may be\n    otherwise permitted by applicable law, if You Reproduce, Distribute or\n    Publicly Perform the Work either by itself or as part of any\n    Adaptations or Collections, You must not distort, mutilate, modify or\n    take other derogatory action in relation to the Work which would be\n    prejudicial to the Original Author's honor or reputation. Licensor\n    agrees that in those jurisdictions (e.g. Japan), in which any exercise\n    of the right granted in Section 3(b) of this License (the right to\n    make Adaptations) would be deemed to be a distortion, mutilation,\n    modification or other derogatory action prejudicial to the Original\n    Author's honor and reputation, the Licensor will waive or not assert,\n    as appropriate, this Section, to the fullest extent permitted by the\n    applicable national law, to enable You to reasonably exercise Your\n    right under Section 3(b) of this License (right to make Adaptations)\n    but not otherwise.\n\n\n\n\nRepresentations, Warranties and Disclaimer\n\n\n\n\nUNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR\nOFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY\nKIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE,\nINCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY,\nFITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF\nLATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS,\nWHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION\nOF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.\n\n\n\n\n\n\nLimitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE\nLAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR\nANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES\nARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS\nBEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\n\n\n\nTermination\n\n\n\n\n\n\na. This License and the rights granted hereunder will terminate\n    automatically upon any breach by You of the terms of this License.\n    Individuals or entities who have received Adaptations or Collections\n    from You under this License, however, will not have their licenses\n    terminated provided such individuals or entities remain in full\n    compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will\n    survive any termination of this License.\n b. Subject to the above terms and conditions, the license granted here is\n    perpetual (for the duration of the applicable copyright in the Work).\n    Notwithstanding the above, Licensor reserves the right to release the\n    Work under different license terms or to stop distributing the Work at\n    any time; provided, however that any such election will not serve to\n    withdraw this License (or any other license that has been, or is\n    required to be, granted under the terms of this License), and this\n    License will continue in full force and effect unless terminated as\n    stated above.\n\n\n\n\nMiscellaneous\n\n\n\n\na. Each time You Distribute or Publicly Perform the Work or a Collection,\n    the Licensor offers to the recipient a license to the Work on the same\n    terms and conditions as the license granted to You under this License.\n b. Each time You Distribute or Publicly Perform an Adaptation, Licensor\n    offers to the recipient a license to the original Work on the same\n    terms and conditions as the license granted to You under this License.\n c. If any provision of this License is invalid or unenforceable under\n    applicable law, it shall not affect the validity or enforceability of\n    the remainder of the terms of this License, and without further action\n    by the parties to this agreement, such provision shall be reformed to\n    the minimum extent necessary to make such provision valid and\n    enforceable.\n d. No term or provision of this License shall be deemed waived and no\n    breach consented to unless such waiver or consent shall be in writing\n    and signed by the party to be charged with such waiver or consent.\n e. This License constitutes the entire agreement between the parties with\n    respect to the Work licensed here. There are no understandings,\n    agreements or representations with respect to the Work not specified\n    here. Licensor shall not be bound by any additional provisions that\n    may appear in any communication from You. This License may not be\n    modified without the mutual written agreement of the Licensor and You.\n f. The rights granted under, and the subject matter referenced, in this\n    License were drafted utilizing the terminology of the Berne Convention\n    for the Protection of Literary and Artistic Works (as amended on\n    September 28, 1979), the Rome Convention of 1961, the WIPO Copyright\n    Treaty of 1996, the WIPO Performances and Phonograms Treaty of 1996\n    and the Universal Copyright Convention (as revised on July 24, 1971).\n    These rights and subject matter take effect in the relevant\n    jurisdiction in which the License terms are sought to be enforced\n    according to the corresponding provisions of the implementation of\n    those treaty provisions in the applicable national law. If the\n    standard suite of rights granted under applicable copyright law\n    includes additional rights not granted under this License, such\n    additional rights are deemed to be included in the License; this\n    License is not intended to restrict the license of any rights under\n    applicable law.\n\n\nCreative Commons Notice\n\n\nCreative Commons is not a party to this License, and makes no warranty\nwhatsoever in connection with the Work. Creative Commons will not be\nliable to You or any party on any legal theory for any damages\nwhatsoever, including without limitation any general, special,\nincidental or consequential damages arising in connection to this\nlicense. Notwithstanding the foregoing two (2) sentences, if Creative\nCommons has expressly identified itself as the Licensor hereunder, it\nshall have all rights and obligations of Licensor.\n\nExcept for the limited purpose of indicating to the public that the\nWork is licensed under the CCPL, Creative Commons does not authorize\nthe use by either party of the trademark \"Creative Commons\" or any\nrelated trademark or logo of Creative Commons without the prior\nwritten consent of Creative Commons. Any permitted use will be in\ncompliance with Creative Commons' then-current trademark usage\nguidelines, as may be published on its website or otherwise made\navailable upon request from time to time. For the avoidance of doubt,\nthis trademark restriction does not form part of the License.\n\nCreative Commons may be contacted at https://creativecommons.org/.", 
            "title": "License"
        }, 
        {
            "location": "/1-1-What-This-Repo-Is/", 
            "text": "howto_co34pt_liveCode\n\n\nWhat this repo is\n\n\nHello!\n\n\nI live code under the name co\u00ef\u00bf\u00a5\u00ef\u00be\u00a1pt (\nbandcamp\n, \nsoundcloud\n), and have been performing since 2014, including for \nAlgoraves\n, \ngigs\n, \ntheatre\n and \ndance\n.\n\n\nThis repo is an ongoing collection of materials about my live coding practice using SuperCollider. I'm hoping to use it to post writing, how-tos and guides, helpful code, techniques, frameworks and whatever else might be useful to anyone wanting to learn to live code using SuperCollider, or anyone who already does. It's a clone/version of and companion to my co34pt_livecode repo, which contains finished sets from gigs alongside my setup code and samples.\n\n\nI'm putting together this repo because I wish there'd been such a thing when I had been learning to live code in SuperCollider, (or at least I wish there'd been one i'd been able to find). There are a bunch of great tutorials for SuperCollider out there (I'd particularly recommend this set by \nEli Fieldsteel\n), but I found advice on live coding in SuperCollider reasonably hard to come by, and was fortunate enough to have an experienced live coder as a teacher. An awful lot of the examples I give and techniques I talk about can be found in the documentation of SuperCollider, but finding it can be a little frustrating and examples can often be written in quite different styles. I'm aiming for this repo to be a curated set of resources following a central style, with various parts of the language explained in the context of the kind of live coding I do, with musical examples of how I would use these techniques in actual live sets.\n\n\nFor the resources here i'm assuming a basic knowledge of how to use SuperCollider and programming concepts generally, because i'm not a great person to explain this. If you're totally new to SuperCollider and programming i'd either recommend the Eli Fieldsteel videos previously mentioned, Supercollider's \ninbuilt turotials\n, or \nNick Collins's\n tutorials. The more advanced tutorials won't be necessary for the most part, but a familiarity with executing SuperCollider code, what UGens are and basic syntax will be super helpful.\n\n\nThe materials in this repo are by no means the \nbest\n way to live code using SuperCollider, or the most efficient way to solve any problems, but they are the ways that have worked for me personally. If you spot any glaring issues in this repo, please fork and change/correct! I'd also welcome guest contributions, although I maybe should rename this repo if i'm going to do that so as not to have it under my 'umbrella' name.\n\n\nI'm also continually looking for a better way to manage this repo as a learning resource. I've figured GitHub is probably the best way to do it as there is version control, all resources can be hosted here along with details on how to use them, and it can easily be packaged and downloaded, the only thing missing being a 'comments' section. \n\n\nIf you want to get in touch with me about this repo, please do so on Github, Twitter (same username) or via theseancotterill atsymbol live period com. If there is anything not working, please open an issue/make a pull request and I will look at it as soon as I can. \n\n\nA note about formats: The examples and code in this repo will be tested using Linux, in my case Kubuntu 16.04+, but as SuperCollider is cross-platform this shouldn't matter too much for the most part.", 
            "title": "What this Repo Is"
        }, 
        {
            "location": "/1-1-What-This-Repo-Is/#howto_co34pt_livecode", 
            "text": "", 
            "title": "howto_co34pt_liveCode"
        }, 
        {
            "location": "/1-1-What-This-Repo-Is/#what-this-repo-is", 
            "text": "Hello!  I live code under the name co\u00ef\u00bf\u00a5\u00ef\u00be\u00a1pt ( bandcamp ,  soundcloud ), and have been performing since 2014, including for  Algoraves ,  gigs ,  theatre  and  dance .  This repo is an ongoing collection of materials about my live coding practice using SuperCollider. I'm hoping to use it to post writing, how-tos and guides, helpful code, techniques, frameworks and whatever else might be useful to anyone wanting to learn to live code using SuperCollider, or anyone who already does. It's a clone/version of and companion to my co34pt_livecode repo, which contains finished sets from gigs alongside my setup code and samples.  I'm putting together this repo because I wish there'd been such a thing when I had been learning to live code in SuperCollider, (or at least I wish there'd been one i'd been able to find). There are a bunch of great tutorials for SuperCollider out there (I'd particularly recommend this set by  Eli Fieldsteel ), but I found advice on live coding in SuperCollider reasonably hard to come by, and was fortunate enough to have an experienced live coder as a teacher. An awful lot of the examples I give and techniques I talk about can be found in the documentation of SuperCollider, but finding it can be a little frustrating and examples can often be written in quite different styles. I'm aiming for this repo to be a curated set of resources following a central style, with various parts of the language explained in the context of the kind of live coding I do, with musical examples of how I would use these techniques in actual live sets.  For the resources here i'm assuming a basic knowledge of how to use SuperCollider and programming concepts generally, because i'm not a great person to explain this. If you're totally new to SuperCollider and programming i'd either recommend the Eli Fieldsteel videos previously mentioned, Supercollider's  inbuilt turotials , or  Nick Collins's  tutorials. The more advanced tutorials won't be necessary for the most part, but a familiarity with executing SuperCollider code, what UGens are and basic syntax will be super helpful.  The materials in this repo are by no means the  best  way to live code using SuperCollider, or the most efficient way to solve any problems, but they are the ways that have worked for me personally. If you spot any glaring issues in this repo, please fork and change/correct! I'd also welcome guest contributions, although I maybe should rename this repo if i'm going to do that so as not to have it under my 'umbrella' name.  I'm also continually looking for a better way to manage this repo as a learning resource. I've figured GitHub is probably the best way to do it as there is version control, all resources can be hosted here along with details on how to use them, and it can easily be packaged and downloaded, the only thing missing being a 'comments' section.   If you want to get in touch with me about this repo, please do so on Github, Twitter (same username) or via theseancotterill atsymbol live period com. If there is anything not working, please open an issue/make a pull request and I will look at it as soon as I can.   A note about formats: The examples and code in this repo will be tested using Linux, in my case Kubuntu 16.04+, but as SuperCollider is cross-platform this shouldn't matter too much for the most part.", 
            "title": "What this repo is"
        }, 
        {
            "location": "/1-2-Why-I-Live-Code/", 
            "text": "Why I Live Code - by co34pt\n\n\nAs an exiled classical violinist, dormant guitarist, habitual electronic tinkerer and (as of 2014) live coder, I got interested in making electronic music when I listened to Portishead's 'Dummy' and Boards of Canada's 'Geogaddi' (among others) in my early teens. I began learning how to produce it as soon as I could by experimenting with FL Studio alongside early YouTube tutorials, the first milestone of this being the release of my first 'album' of 'Ambient music' as a .zip of 128kbps .mp3 files on MediaFire.\n\n\nThese Digital Audio Workstation (hereafter DAW) compositions and arrangements were a lot of fun to make, and enabled me to experiment with many techniques and genres, but I couldn't 'perform' them. This was of course until I discovered Ableton Live. As someone who had been confined to static DAW arrangements for some time, Ableton with its emphasis on live performance through alternative interfaces/controller mechanisms was my platform of choice for around five years. Ableton's emphasis on performance initially allowed me to compose music in a performative manner by using loops, triggers and controllers, and eventually gave me the confidence to take specific compositions to a stage, with varying degrees of success. I then began composing and performing music using a mix few proprietary DAWs and programs.\n\n\nhere's an old performance of mine\n\n\nAfter a while I had some reservations about my continuing use of proprietary DAWs, for a few reasons.\n\n\nFirst was the inflexible nature of the kinds of performances I was delivering. I had a set of compositions (or 'songs', if you will), which were arranged into a set of loops which could be triggered in \ntheoretically\n any combination, but in order for the songs to make sense as pieces of music, the order had to be reasonably strictly obeyed. I had some flexibility in the way I applied effects to individual channels, but this to me did not translate to directly 'performing' tracks in the way I would 'perform' with a traditional instrument - I felt as if my performances had become glorified button pushing ceremonies. I am very aware that there are much more 'live' ways to play with various DAWs than the methods I used, but this was not how I had ended up performing. Around the time I decided to give up on proprietary DAWs I was pretty immersed in playing improvised music with guitar/violin/electronics/various media during my Music degree, and I wanted to be able to bring an improvisatory instrumental spirit to my performances of electronic music. In performing with proprietary DAWs however I personally fell far short.\n\n\nSecond was the fact that the software was --h  u  g  e--, and \n!DEMANDING!\n. My performance DAW suite of choice took up around 54GB of hard disk space, and became very difficult for my laptop to handle if I used any external software instruments at all. As a result of this, each individual track was an unwieldy bundle of samples and instruments, which would take a large amount of processing power to render. If I then wanted to perform a set of these tracks, I'd often have to combine a number of live 'projects' together and save them as one large project, as having to load each individual song before I played it would take minutes, breaking the flow of performance. What resulted were metaprojects which would be utterly enormous, unresponsive and would sometimes crash on loading. They could also be quite buggy, and performances felt 'risky' in the sense that any movement could topple them and bring my entire performance with it. While i'm all for embracing the possibility of a crash, this possibility being a structural feature of a performance without that being my intention was not an enjoyable way to perform.\n\n\nThird is that the software is proprietary, and I was unhappy with what that represents. Leading up to the time I eventually gave up with proprietary DAWs (and subsequently proprietary software in general, where possible) I had been watching a number of lectures by \nRichard Stallman\n discussing proprietary software and user freedom. This, coupled with the work of glitch artists (particularly \nRosa Menkman\n and \nNick Briz\n) focusing on the role of platforms and softwares as often unacknowledged intermediaries in our material experiences of technology presented me with a set of issues I could not personally resolve. While I released all of my music under creative commons in disagreement with copyright legislation, I was producing music using tools that were not only bound by the legislation I disagreed with, but tools that were purposefully restricted the way that I could use them. In the words of \nRichard Stallman\n:\n\n\n\n\n'With software, either the users control the program (free software) or the program controls the users (proprietary or nonfree software).'\nThe proprietary nature of the software also means that it can only be run on certain systems by those with the financial ability to run it (or willingness to break various laws), on top of having to have access to a computer. The copyleft approach I had to the works I produced were very difficult (if not impossible) to apply to the materials used to make the works themselves.\n\n\n\n\nFourth was my relationship to traditions of performance in 'laptop music'. Even with controllers, performances I would deliver would always be me staring into a \nblack box\n in the form of a laptop, occasionally triggering things on a controller. While I attempted to get around this in some ways by projecting a video of my controller during sets as part of the visuals during sets, this didn't alleviate the problem of obfuscation. I was very used to a direct cause-and-effect relationship between actions and sounds, and for that relationship to be apparent to an audience. Whether I was bowing a violin, chugging away at 12/8 swing, or playing guitar with a handheld fan and a wood file (actually happened), the cause-effect relationship between myself and any potential audience was pretty clear. I felt as if my performances of electronic music did not have this kind of immediacy, and I didn't like that at all\u2020. I'm very aware that this kind of immediacy isn't something that everyone strives for in laptop performance, but I missed it dearly. In addition to this, performances of electronic music of this type offered no opportunities for me as an audience member to learn about its construction besides how it sounded. I've always been fascinated by the construction of music and art, and the ability to deconstruct this in real-time is something I really value, much like the YouTube FL Studio tutorials I followed to learn how to make electronic music in the first place (I did this because I didn't realise the software actually had a manual, and I didn't realise my performance DAW even had a manual until I had been using it for three years). With this 'black boxing' of the performance setup, I had no layers to peel back - if a performer did something cool and I wanted to do it, tough luck, time to go home and reverse-engineer it without any idea what tools were used in its construction! I've never been enamoured of obfuscation or secrecy around technique. Why should techniques be a big secret? Much like the copyrighting and locking-down of the software, performance traditions that obscure the mechanisms one can use to do 'cool things' are pretty frustrating for me, whether or not that is the intention of the performer. \n\n\nWith these issues in mind, what was the answer to my problems with digital music performance? The best answer I have found is live coding, but it took me a while to get there. \n\n\nUntil around 2014, I had been dead-set against 'music-programming' (at the time I meant \nPure Data\n and \nMax/MSP\n), as I was convinced that the integration of programming and music would take the 'human element' out of the music I was performing. Needless to say this was short-sighted and incorrect, and was probably a hangover from my education in the classical music tradition through the British schooling system, in which electronic music was often derided as a something not to be taken seriously, and not as 'real music'. I had overcome this once I learned that my university took electronic music pretty seriously, however the idea of programming still stuck around as 'non-musical'. As was reasonably common among my peers, I found programming to be an alienating concept, with its syntax, language, args/ints/strings/longs and so on, it seemed the exact opposite of what I considered music creation to be - intuitive, tactile, 'musical'. How could \n\n\n{SinOsc.ar(LFSaw.ar(XLine.kr([0.01,0.02],[400,500],100)).range(1,2000).round(200))}.play; \n\n\n\n\nbe music if it didn't look like any music I had ever played before?\n\n\nAround the time I was considering these issues and starting to look for alternatives I was fortunate enough to audit some classes by \nJohn Bowers\n where I learned how to use Pure Data and Arduino for multimedia performance and installation work. As a result, I actually learned how programming worked and what it was capable of, and began producing interactive digital works and performances. In addition, I was using free and open soure software almost exclusively to create these works (with the exception of Max/MSP for video). It turned out that by using programming I could not only escape the trappings of limited systems for artistic expression by creating my own, but could extend outside of audio and into video, graphics and electronics through the use of open standards. I had overcome my fear of code!\n\n\nWhile this was great for developing artworks, and provided a way out of using proprietary software (again, with the exception of Max/MSP), it didn't provide me with a solution for the music performance problem. \n\n\nHowever, a \nhousemate of mine\n at the time had been teaching me a little \nSuperCollider\n, a platform for audio synthesis and algorithmic composition. SuperCollider seemed to be the best platform for applying my newfound programming enthusiasm to electronic music, with the ability to operate outside of proprietary software, and the ability to choose the terms on which I would interact with the music I created (what DAW environment will let you play 1,000 copies of a three minute sound at random speeds with one action?). Around the time that I learned basic SuperCollider skills I had to complete my final year of my undergraduate music course, where I elected to do a 40-minute performance in place of a formal written dissertation. I figured the best thing to do would be to put my money where my mouth is (so to speak) and take the plunge away from proprietary DAWs into performing music with code. When I decided to do this \nAlgorave\n had been in my periphery for a little while as live-coding's answer to electronic music performance. The \nTOPLAP Draft Manifesto\n alongside some events I had attended in Newcastle and Sheffield featuring live coding musicians piqued my interest in Algorave and what it could offer me by way of an approach to electronic music performance, and it turned out to be a great working answer to my main gripes with performing electronic music with proprietary DAWs.\n\n\n\"First was the inflexible nature of the kinds of performances I was delivering\" - Live Coding tends to revolve around wholly or partly improvised performances, and the ability to write code in a non-linear way and execute it in real time and have the results instantly rendered as audio opened the playing field for me hugely. While it is possible to have live coding performances with a very set trajectory which evolve in the manner of a meticulous composition, it's equally possible to start from literally nothing except a running synthesis server. With a language as broad as SuperCollider, I could integrate anything from blistering noise based on non-linear maps through to 5/4 kick drums through to complex sample manipulation through to 4/4 kick-snare-clap patterns within one performance. While of course it's not always productive (or possible) to draw on such wildly disparate techniques during performances, the fact that the possibilities exist is very important. In addition to this, there are a plethora of live coding languages that can all be networked to one degree or another (although I usually stick to SuperCollider for reasons I'll detail in a later post).\n\n\n\"Second was the fact that the software was --h  u  g  e--, and \n!DEMANDING!\n\" - In switching to a programming platform like SuperCollider to make music, one is presented with the ability to start from basically zero. The SuperCollider source code is currently (as of March 2017) an \n14.6 MB download from GitHub\n, and runs without any GUI by default, meaning that system load is very low out of the box (SuperCollider comfortably runs on Raspberry Pi), with the loading of extended functionality and libraries at the discretion of the user. In addition, projects are written and loaded as text files, which take up very little disk space and can be loaded near-instantly. By switching out my proprietary DAW for a live coding setup, I wouldn't have to wait minutes for projects to load (or have them crash outright after loading), and the separation of editor/server/interpreter in SuperCollider makes the management of any crashes much easier. If i need to, I can also perform on low-cost, low-power hardware, or use SuperCollider to create embedded installation works.\n\n\nAs it is a programming language, SuperCollider can be (\nand has been\n) built up to a fully-functioning DAW-type environment if necessary. With this I could try to like-for-like replace a proprietary DAW environment if I wanted, but doing so would, for me, partially defeat the point of learning how to live code in the first place. In live coding I can build and maintain an environment that suits me as a performer, keeping a simple, effective workflow to articulate my ideas within.\n\n\n\"Third is that the software is proprietary\" - With a few exceptions (notably \nMax/MSP\n), live coding draws from rich ecosystem of free and open source tools, often with practitioners being active contributors to the software packages that they use (a good example being Alex McLean and \nTidalCycles\n). In adopting Live Coding as a method for electronic music performance I could finally leave the \nApple ecosystem\n and the proprietary DAW paradigm in favour of using GNU/Linux and open source tools. I could now have full access to the tools I would be using to create music and the ability to modify these tools as I wished. In addition, so can anyone else! I can happily write a set of tutorials on how I live code electronic music knowing that anyone who has access to a computer running a compatible operating system should have the ability to follow that tutorial without them having to have access to hundreds of pounds worth of software and a license for Windows or an Apple machine. Live Coding was the last piece of the puzzle in my transition to a fully open source art practice, both in the tools I use and the work I create, which is now the focus of my PhD research. I try to keep an updated GitHub repo containing my live coding setup and sets, and I am going to be writing some docs/guides on how I live code dance music using SuperCollider and my own custom boilerplate code. The repo can be found \nhere\n and a set of resources on how to live code in SuperCollider can be found \nhere\n.\n\n\n\"Fourth was my relationship to traditions of performance in 'laptop music'\" - I'm \nfar\n from the first person to pick up on this, but the TOPLAP manifesto's 'Obscurantism is dangerous. Show us your screens.' seemed like a beautiful answer to the kinds of indecipherable laptop performances that frustrated me as a concert-goer. Important to 'Show us your screens' too is its corollary:\n\n\n\n\n'It is not necessary for a lay audience to understand the code to appreciate it, much as it is not necessary to know how to play guitar in order to appreciate watching a guitar performance.'\nBy adopting a text-based interface to perform and also projecting that text-based interface for an audience to see during a performance, a number of things are achieved. First, for anybody interested the text makeup of a performance is shown, showcasing the inner workings of a performance as it comes together, live on stage. This is useful for me as a live coder myself because I can see how 'cool things' are done as the 'black box' of the performance laptop is removed to some degree - I've learned a whole bunch of techniques by going along to algoraves and following the projections to see what is being done by the performer (this also includes live streaming one's sets, \nwhich I have done a decent amount of\n). In addition to this, for anyone who doesn't understand the specifics of the language being used (or isn't interested) this opening of the laptop performance ecology serves the purpose of exposing the materiality of the performance - in watching a performer type and execute code you are seeing the performer at work, how they respond to various stimuli during performance, and how their thoughts are translated to text. In addition to this, through the selective writing of, navigation through, and execution of text, the kinetic intent of the music is demonstrated. Much as an instrumentalist stamping their foot to a beat more than likely shows the path of their playing, a live coder hurriedly typing \n~kickdrum.play\n (or equivalent) shows their vision of the music in real time. \n\n\n\n\nMore significantly though, I'd argue this projection of text is more than the fleeting glimpse one can see when observing a traditional instrumentalist at work. In watching a performer articulate their music as a text file on screen, I feel as if I am watching a performer build and manipulate a sculpture over the course of a performance, with the form of that sculpture being mirrored in the changes in the music heard throughout the performer's set. Whether that involves a performer starting from absolutely nothing and building a performance from minimal roots, regularly deleting their entire text and starting again, or a performer loading a pre-written text and selectively executing/modifying it, drawing on an extensive codebase to craft a detailed performance (both of which I've seen \nYaxu\n alone do), or anything in between. As I perform using SuperCollider, the level of verbosity required means I often type and navigate through text a \nlot\n, however I am always shocked at how little code I actually have at the end of a performance. My performances are usually composed of a select few carefully-maintained symbiotic micro-structures which I edit extensively. I don't write an awful lot from scratch, but I fairly meticulously edit and re-edit what I do write, executing the same piece of code many times in one performance with slight changes to fit the other few running pieces of code.\n\n\nIn watching a live coding performance, you can see the performer not only deal with the environment of performance in real time in a way that is potentially useful to practitioners and (relatively) transparent to 'lay-persons', but see them dealing with both the history of, and potential futures of their performance in an engaging way. \n\n\nIt's also undeniably eye-catching.\n\n\nSo with all of this in mind I decided to take the plunge and learn to live code. I was fortunate enough to have a great opportunity to uproot everything I knew about performing electronic music in the form of my final-year undergraduate dissertation, which I used as an opportunity to deliver a 40-minute live coding performance. I was also fortunate enough to have some teaching on how to live coding using SuperCollider from Data Musician and \nAlgobabe\n \nShelly Knotts\n. I've since played a bunch of Algoraves and live shows (a lot of which can be found \nhere\n), streamed a whole bunch of sets, and applied live coding approaches to \nother\n \nprojects\n. \n\n\nReasonably quickly Live Coding became 'how I made music', and a few realisations followed:\n\n\nIn live coding I could not only embrace alternate traditions of laptop performance, but also paradigms of laptop music. The way I had worked in DAW software had always been dominated by audio loops, MIDI data and VST plugins, and these methods are much less immediately accessible in live coding performance with SuperCollider. Much is made in the live coding community of the role of the \nalgorithm\n in performance, and I've only recently realised what that \nactually meant\n, after initially being quite scared by the 'maths-ness' of the term. In creating a drum pattern in a DAW environment, I would layer together drum loops and play instrumental lines using a keyboard to achieve the desired rhythms, but in a live coding environment I specify a bunch of behaviours to determine how drums are 'played', and similarly with melodies, textures and bass. In performing I am creating multiple rule-governed self-managing instrumental 'players', and shepherding them around to create a performance, rather than 'playing' the music in a traditional sense - this is something that is intuitively quite easy to achieve through live coding in SuperCollider, but something I found quite difficult to achieve in a DAW environment. Incidentally I find this method of performance much more tactile and 'instrumental' than the DAW paradigm, after this method of performance was the very thing I was afraid would take the 'human element' out of music!\n\n\nAspects of music as fundamental as pitch and rhythm organisation are easy to experiment with too. I'm a big fan of using \nEuclidean rhythms\n and some constrained randomness to generate compound rhythmic patterns, as well as using the \nHarmonic Series\n to determine pitch for melodies and textures, and the bare-bones 'do it yourself' nature of live coding in SuperCollider means that I can fairly easily build performance systems based around non-standard musical techniques. \n\n\nElectronic Music also has \nproblems with diversity\n, and there are a number of facets of the live coding community that are actively addressing this. There are groups such as \nSoNA\n and \nYSWN\n encouraging the involvement of women in the live coding community, and socially-concerned organisations such as \nAccess Space\n are also actively involved. My experience both attending and taking part in live coding events shows commitment to addressing these issues too - while there is no formal code of conduct, a general commitment to inclusivity in participation (no all-male bills at Algoraves), attitudes and language are commonplace. With the recent \n#Algofive\n stream showcasing not only a diverse global network of artists but a diversity of approaches to live coding too, it's a community I'm very proud to be a part of. \n\n\nLike everything, Live Coding does have its problems. I've realised that all of the freedom that live coding in SuperCollider offers also comes with the drawback that I have to build my own frameworks to perform with, starting from the basics, which is sometimes pretty paralysing. If I'm stuck for inspiration, it's actually quite hard to get myself out of a rut, and discovering how to use different features is actually quite difficult without having the software having a 'manual'. Further to this, Open Source software and libraries can sometimes be scantily documented, with incredibly useful tools remaining difficult to access because only the creator of those tools knows how to use them properly. In addition, the issue of performative transparency isn't quite as clear cut as 'I'm projecting code, therefore my intent, action and gesture in performance are immediately and clearly articulated' - in '[showing] your screens', the black box has just been shifted to the processes underlying the code itself. There's also the issue of 'code literacy' presenting a barrier to entry to live coding, however this is addressed both through the publishing of learning tools by the community and languages that require less specialist knowledge to use effectively, as well as workshops by the community to engage those unfamiliar with live coding and programming in general. I am also very aware that my somewhat idealistic notions of what \nI\n want to demonstrate through performance may well not matter to other performers, and this is fine too.\n\n\nAll things considered, I live code because it allows me to use free/libre/open source tools to create flexible musical environments that allow me to perform electronic music in a way that I feel gives me the ability to think and play like an improviser. My initial fears that coding music would lead me to academic 'maths music' turned out to be completely the opposite - performing with live coding is by far and away the closest I have come to an 'instrumental' way of performing electronic music. Let's keep going with those repetitive conditionals!\n\n\nI have written (and am continuing to write) resources/guides/tutorials/docs etc on live coding with SuperCollider \nhere\n.\nMy website is \nhere\n.\n\n\n\u2020 As a caveat to this, the closest I probably came to this cause-effect relationship becoming clear while using DAW software was with \nMutual Process\n, an improvised music project with Adam Denton of \nTrans/Human\n. For Mutual Process I performed manipulations of live-recorded samples of Denton's guitar, which were fed back to him - and I used a number of controllers to live-patch effects and record/process samples. I had a huge amount of control over this setup to the point where I felt as if I could impact upon the performance with physical control gestures, and embody my action within the music somewhat. Interestingly enough this performance setup was a complete 'hack' of Ableton's core functionality.", 
            "title": "Why I live Code"
        }, 
        {
            "location": "/1-2-Why-I-Live-Code/#why-i-live-code-by-co34pt", 
            "text": "As an exiled classical violinist, dormant guitarist, habitual electronic tinkerer and (as of 2014) live coder, I got interested in making electronic music when I listened to Portishead's 'Dummy' and Boards of Canada's 'Geogaddi' (among others) in my early teens. I began learning how to produce it as soon as I could by experimenting with FL Studio alongside early YouTube tutorials, the first milestone of this being the release of my first 'album' of 'Ambient music' as a .zip of 128kbps .mp3 files on MediaFire.  These Digital Audio Workstation (hereafter DAW) compositions and arrangements were a lot of fun to make, and enabled me to experiment with many techniques and genres, but I couldn't 'perform' them. This was of course until I discovered Ableton Live. As someone who had been confined to static DAW arrangements for some time, Ableton with its emphasis on live performance through alternative interfaces/controller mechanisms was my platform of choice for around five years. Ableton's emphasis on performance initially allowed me to compose music in a performative manner by using loops, triggers and controllers, and eventually gave me the confidence to take specific compositions to a stage, with varying degrees of success. I then began composing and performing music using a mix few proprietary DAWs and programs.  here's an old performance of mine  After a while I had some reservations about my continuing use of proprietary DAWs, for a few reasons.  First was the inflexible nature of the kinds of performances I was delivering. I had a set of compositions (or 'songs', if you will), which were arranged into a set of loops which could be triggered in  theoretically  any combination, but in order for the songs to make sense as pieces of music, the order had to be reasonably strictly obeyed. I had some flexibility in the way I applied effects to individual channels, but this to me did not translate to directly 'performing' tracks in the way I would 'perform' with a traditional instrument - I felt as if my performances had become glorified button pushing ceremonies. I am very aware that there are much more 'live' ways to play with various DAWs than the methods I used, but this was not how I had ended up performing. Around the time I decided to give up on proprietary DAWs I was pretty immersed in playing improvised music with guitar/violin/electronics/various media during my Music degree, and I wanted to be able to bring an improvisatory instrumental spirit to my performances of electronic music. In performing with proprietary DAWs however I personally fell far short.  Second was the fact that the software was --h  u  g  e--, and  !DEMANDING! . My performance DAW suite of choice took up around 54GB of hard disk space, and became very difficult for my laptop to handle if I used any external software instruments at all. As a result of this, each individual track was an unwieldy bundle of samples and instruments, which would take a large amount of processing power to render. If I then wanted to perform a set of these tracks, I'd often have to combine a number of live 'projects' together and save them as one large project, as having to load each individual song before I played it would take minutes, breaking the flow of performance. What resulted were metaprojects which would be utterly enormous, unresponsive and would sometimes crash on loading. They could also be quite buggy, and performances felt 'risky' in the sense that any movement could topple them and bring my entire performance with it. While i'm all for embracing the possibility of a crash, this possibility being a structural feature of a performance without that being my intention was not an enjoyable way to perform.  Third is that the software is proprietary, and I was unhappy with what that represents. Leading up to the time I eventually gave up with proprietary DAWs (and subsequently proprietary software in general, where possible) I had been watching a number of lectures by  Richard Stallman  discussing proprietary software and user freedom. This, coupled with the work of glitch artists (particularly  Rosa Menkman  and  Nick Briz ) focusing on the role of platforms and softwares as often unacknowledged intermediaries in our material experiences of technology presented me with a set of issues I could not personally resolve. While I released all of my music under creative commons in disagreement with copyright legislation, I was producing music using tools that were not only bound by the legislation I disagreed with, but tools that were purposefully restricted the way that I could use them. In the words of  Richard Stallman :   'With software, either the users control the program (free software) or the program controls the users (proprietary or nonfree software).'\nThe proprietary nature of the software also means that it can only be run on certain systems by those with the financial ability to run it (or willingness to break various laws), on top of having to have access to a computer. The copyleft approach I had to the works I produced were very difficult (if not impossible) to apply to the materials used to make the works themselves.   Fourth was my relationship to traditions of performance in 'laptop music'. Even with controllers, performances I would deliver would always be me staring into a  black box  in the form of a laptop, occasionally triggering things on a controller. While I attempted to get around this in some ways by projecting a video of my controller during sets as part of the visuals during sets, this didn't alleviate the problem of obfuscation. I was very used to a direct cause-and-effect relationship between actions and sounds, and for that relationship to be apparent to an audience. Whether I was bowing a violin, chugging away at 12/8 swing, or playing guitar with a handheld fan and a wood file (actually happened), the cause-effect relationship between myself and any potential audience was pretty clear. I felt as if my performances of electronic music did not have this kind of immediacy, and I didn't like that at all\u2020. I'm very aware that this kind of immediacy isn't something that everyone strives for in laptop performance, but I missed it dearly. In addition to this, performances of electronic music of this type offered no opportunities for me as an audience member to learn about its construction besides how it sounded. I've always been fascinated by the construction of music and art, and the ability to deconstruct this in real-time is something I really value, much like the YouTube FL Studio tutorials I followed to learn how to make electronic music in the first place (I did this because I didn't realise the software actually had a manual, and I didn't realise my performance DAW even had a manual until I had been using it for three years). With this 'black boxing' of the performance setup, I had no layers to peel back - if a performer did something cool and I wanted to do it, tough luck, time to go home and reverse-engineer it without any idea what tools were used in its construction! I've never been enamoured of obfuscation or secrecy around technique. Why should techniques be a big secret? Much like the copyrighting and locking-down of the software, performance traditions that obscure the mechanisms one can use to do 'cool things' are pretty frustrating for me, whether or not that is the intention of the performer.   With these issues in mind, what was the answer to my problems with digital music performance? The best answer I have found is live coding, but it took me a while to get there.   Until around 2014, I had been dead-set against 'music-programming' (at the time I meant  Pure Data  and  Max/MSP ), as I was convinced that the integration of programming and music would take the 'human element' out of the music I was performing. Needless to say this was short-sighted and incorrect, and was probably a hangover from my education in the classical music tradition through the British schooling system, in which electronic music was often derided as a something not to be taken seriously, and not as 'real music'. I had overcome this once I learned that my university took electronic music pretty seriously, however the idea of programming still stuck around as 'non-musical'. As was reasonably common among my peers, I found programming to be an alienating concept, with its syntax, language, args/ints/strings/longs and so on, it seemed the exact opposite of what I considered music creation to be - intuitive, tactile, 'musical'. How could   {SinOsc.ar(LFSaw.ar(XLine.kr([0.01,0.02],[400,500],100)).range(1,2000).round(200))}.play;   be music if it didn't look like any music I had ever played before?  Around the time I was considering these issues and starting to look for alternatives I was fortunate enough to audit some classes by  John Bowers  where I learned how to use Pure Data and Arduino for multimedia performance and installation work. As a result, I actually learned how programming worked and what it was capable of, and began producing interactive digital works and performances. In addition, I was using free and open soure software almost exclusively to create these works (with the exception of Max/MSP for video). It turned out that by using programming I could not only escape the trappings of limited systems for artistic expression by creating my own, but could extend outside of audio and into video, graphics and electronics through the use of open standards. I had overcome my fear of code!  While this was great for developing artworks, and provided a way out of using proprietary software (again, with the exception of Max/MSP), it didn't provide me with a solution for the music performance problem.   However, a  housemate of mine  at the time had been teaching me a little  SuperCollider , a platform for audio synthesis and algorithmic composition. SuperCollider seemed to be the best platform for applying my newfound programming enthusiasm to electronic music, with the ability to operate outside of proprietary software, and the ability to choose the terms on which I would interact with the music I created (what DAW environment will let you play 1,000 copies of a three minute sound at random speeds with one action?). Around the time that I learned basic SuperCollider skills I had to complete my final year of my undergraduate music course, where I elected to do a 40-minute performance in place of a formal written dissertation. I figured the best thing to do would be to put my money where my mouth is (so to speak) and take the plunge away from proprietary DAWs into performing music with code. When I decided to do this  Algorave  had been in my periphery for a little while as live-coding's answer to electronic music performance. The  TOPLAP Draft Manifesto  alongside some events I had attended in Newcastle and Sheffield featuring live coding musicians piqued my interest in Algorave and what it could offer me by way of an approach to electronic music performance, and it turned out to be a great working answer to my main gripes with performing electronic music with proprietary DAWs.  \"First was the inflexible nature of the kinds of performances I was delivering\" - Live Coding tends to revolve around wholly or partly improvised performances, and the ability to write code in a non-linear way and execute it in real time and have the results instantly rendered as audio opened the playing field for me hugely. While it is possible to have live coding performances with a very set trajectory which evolve in the manner of a meticulous composition, it's equally possible to start from literally nothing except a running synthesis server. With a language as broad as SuperCollider, I could integrate anything from blistering noise based on non-linear maps through to 5/4 kick drums through to complex sample manipulation through to 4/4 kick-snare-clap patterns within one performance. While of course it's not always productive (or possible) to draw on such wildly disparate techniques during performances, the fact that the possibilities exist is very important. In addition to this, there are a plethora of live coding languages that can all be networked to one degree or another (although I usually stick to SuperCollider for reasons I'll detail in a later post).  \"Second was the fact that the software was --h  u  g  e--, and  !DEMANDING! \" - In switching to a programming platform like SuperCollider to make music, one is presented with the ability to start from basically zero. The SuperCollider source code is currently (as of March 2017) an  14.6 MB download from GitHub , and runs without any GUI by default, meaning that system load is very low out of the box (SuperCollider comfortably runs on Raspberry Pi), with the loading of extended functionality and libraries at the discretion of the user. In addition, projects are written and loaded as text files, which take up very little disk space and can be loaded near-instantly. By switching out my proprietary DAW for a live coding setup, I wouldn't have to wait minutes for projects to load (or have them crash outright after loading), and the separation of editor/server/interpreter in SuperCollider makes the management of any crashes much easier. If i need to, I can also perform on low-cost, low-power hardware, or use SuperCollider to create embedded installation works.  As it is a programming language, SuperCollider can be ( and has been ) built up to a fully-functioning DAW-type environment if necessary. With this I could try to like-for-like replace a proprietary DAW environment if I wanted, but doing so would, for me, partially defeat the point of learning how to live code in the first place. In live coding I can build and maintain an environment that suits me as a performer, keeping a simple, effective workflow to articulate my ideas within.  \"Third is that the software is proprietary\" - With a few exceptions (notably  Max/MSP ), live coding draws from rich ecosystem of free and open source tools, often with practitioners being active contributors to the software packages that they use (a good example being Alex McLean and  TidalCycles ). In adopting Live Coding as a method for electronic music performance I could finally leave the  Apple ecosystem  and the proprietary DAW paradigm in favour of using GNU/Linux and open source tools. I could now have full access to the tools I would be using to create music and the ability to modify these tools as I wished. In addition, so can anyone else! I can happily write a set of tutorials on how I live code electronic music knowing that anyone who has access to a computer running a compatible operating system should have the ability to follow that tutorial without them having to have access to hundreds of pounds worth of software and a license for Windows or an Apple machine. Live Coding was the last piece of the puzzle in my transition to a fully open source art practice, both in the tools I use and the work I create, which is now the focus of my PhD research. I try to keep an updated GitHub repo containing my live coding setup and sets, and I am going to be writing some docs/guides on how I live code dance music using SuperCollider and my own custom boilerplate code. The repo can be found  here  and a set of resources on how to live code in SuperCollider can be found  here .  \"Fourth was my relationship to traditions of performance in 'laptop music'\" - I'm  far  from the first person to pick up on this, but the TOPLAP manifesto's 'Obscurantism is dangerous. Show us your screens.' seemed like a beautiful answer to the kinds of indecipherable laptop performances that frustrated me as a concert-goer. Important to 'Show us your screens' too is its corollary:   'It is not necessary for a lay audience to understand the code to appreciate it, much as it is not necessary to know how to play guitar in order to appreciate watching a guitar performance.'\nBy adopting a text-based interface to perform and also projecting that text-based interface for an audience to see during a performance, a number of things are achieved. First, for anybody interested the text makeup of a performance is shown, showcasing the inner workings of a performance as it comes together, live on stage. This is useful for me as a live coder myself because I can see how 'cool things' are done as the 'black box' of the performance laptop is removed to some degree - I've learned a whole bunch of techniques by going along to algoraves and following the projections to see what is being done by the performer (this also includes live streaming one's sets,  which I have done a decent amount of ). In addition to this, for anyone who doesn't understand the specifics of the language being used (or isn't interested) this opening of the laptop performance ecology serves the purpose of exposing the materiality of the performance - in watching a performer type and execute code you are seeing the performer at work, how they respond to various stimuli during performance, and how their thoughts are translated to text. In addition to this, through the selective writing of, navigation through, and execution of text, the kinetic intent of the music is demonstrated. Much as an instrumentalist stamping their foot to a beat more than likely shows the path of their playing, a live coder hurriedly typing  ~kickdrum.play  (or equivalent) shows their vision of the music in real time.    More significantly though, I'd argue this projection of text is more than the fleeting glimpse one can see when observing a traditional instrumentalist at work. In watching a performer articulate their music as a text file on screen, I feel as if I am watching a performer build and manipulate a sculpture over the course of a performance, with the form of that sculpture being mirrored in the changes in the music heard throughout the performer's set. Whether that involves a performer starting from absolutely nothing and building a performance from minimal roots, regularly deleting their entire text and starting again, or a performer loading a pre-written text and selectively executing/modifying it, drawing on an extensive codebase to craft a detailed performance (both of which I've seen  Yaxu  alone do), or anything in between. As I perform using SuperCollider, the level of verbosity required means I often type and navigate through text a  lot , however I am always shocked at how little code I actually have at the end of a performance. My performances are usually composed of a select few carefully-maintained symbiotic micro-structures which I edit extensively. I don't write an awful lot from scratch, but I fairly meticulously edit and re-edit what I do write, executing the same piece of code many times in one performance with slight changes to fit the other few running pieces of code.  In watching a live coding performance, you can see the performer not only deal with the environment of performance in real time in a way that is potentially useful to practitioners and (relatively) transparent to 'lay-persons', but see them dealing with both the history of, and potential futures of their performance in an engaging way.   It's also undeniably eye-catching.  So with all of this in mind I decided to take the plunge and learn to live code. I was fortunate enough to have a great opportunity to uproot everything I knew about performing electronic music in the form of my final-year undergraduate dissertation, which I used as an opportunity to deliver a 40-minute live coding performance. I was also fortunate enough to have some teaching on how to live coding using SuperCollider from Data Musician and  Algobabe   Shelly Knotts . I've since played a bunch of Algoraves and live shows (a lot of which can be found  here ), streamed a whole bunch of sets, and applied live coding approaches to  other   projects .   Reasonably quickly Live Coding became 'how I made music', and a few realisations followed:  In live coding I could not only embrace alternate traditions of laptop performance, but also paradigms of laptop music. The way I had worked in DAW software had always been dominated by audio loops, MIDI data and VST plugins, and these methods are much less immediately accessible in live coding performance with SuperCollider. Much is made in the live coding community of the role of the  algorithm  in performance, and I've only recently realised what that  actually meant , after initially being quite scared by the 'maths-ness' of the term. In creating a drum pattern in a DAW environment, I would layer together drum loops and play instrumental lines using a keyboard to achieve the desired rhythms, but in a live coding environment I specify a bunch of behaviours to determine how drums are 'played', and similarly with melodies, textures and bass. In performing I am creating multiple rule-governed self-managing instrumental 'players', and shepherding them around to create a performance, rather than 'playing' the music in a traditional sense - this is something that is intuitively quite easy to achieve through live coding in SuperCollider, but something I found quite difficult to achieve in a DAW environment. Incidentally I find this method of performance much more tactile and 'instrumental' than the DAW paradigm, after this method of performance was the very thing I was afraid would take the 'human element' out of music!  Aspects of music as fundamental as pitch and rhythm organisation are easy to experiment with too. I'm a big fan of using  Euclidean rhythms  and some constrained randomness to generate compound rhythmic patterns, as well as using the  Harmonic Series  to determine pitch for melodies and textures, and the bare-bones 'do it yourself' nature of live coding in SuperCollider means that I can fairly easily build performance systems based around non-standard musical techniques.   Electronic Music also has  problems with diversity , and there are a number of facets of the live coding community that are actively addressing this. There are groups such as  SoNA  and  YSWN  encouraging the involvement of women in the live coding community, and socially-concerned organisations such as  Access Space  are also actively involved. My experience both attending and taking part in live coding events shows commitment to addressing these issues too - while there is no formal code of conduct, a general commitment to inclusivity in participation (no all-male bills at Algoraves), attitudes and language are commonplace. With the recent  #Algofive  stream showcasing not only a diverse global network of artists but a diversity of approaches to live coding too, it's a community I'm very proud to be a part of.   Like everything, Live Coding does have its problems. I've realised that all of the freedom that live coding in SuperCollider offers also comes with the drawback that I have to build my own frameworks to perform with, starting from the basics, which is sometimes pretty paralysing. If I'm stuck for inspiration, it's actually quite hard to get myself out of a rut, and discovering how to use different features is actually quite difficult without having the software having a 'manual'. Further to this, Open Source software and libraries can sometimes be scantily documented, with incredibly useful tools remaining difficult to access because only the creator of those tools knows how to use them properly. In addition, the issue of performative transparency isn't quite as clear cut as 'I'm projecting code, therefore my intent, action and gesture in performance are immediately and clearly articulated' - in '[showing] your screens', the black box has just been shifted to the processes underlying the code itself. There's also the issue of 'code literacy' presenting a barrier to entry to live coding, however this is addressed both through the publishing of learning tools by the community and languages that require less specialist knowledge to use effectively, as well as workshops by the community to engage those unfamiliar with live coding and programming in general. I am also very aware that my somewhat idealistic notions of what  I  want to demonstrate through performance may well not matter to other performers, and this is fine too.  All things considered, I live code because it allows me to use free/libre/open source tools to create flexible musical environments that allow me to perform electronic music in a way that I feel gives me the ability to think and play like an improviser. My initial fears that coding music would lead me to academic 'maths music' turned out to be completely the opposite - performing with live coding is by far and away the closest I have come to an 'instrumental' way of performing electronic music. Let's keep going with those repetitive conditionals!  I have written (and am continuing to write) resources/guides/tutorials/docs etc on live coding with SuperCollider  here .\nMy website is  here .  \u2020 As a caveat to this, the closest I probably came to this cause-effect relationship becoming clear while using DAW software was with  Mutual Process , an improvised music project with Adam Denton of  Trans/Human . For Mutual Process I performed manipulations of live-recorded samples of Denton's guitar, which were fed back to him - and I used a number of controllers to live-patch effects and record/process samples. I had a huge amount of control over this setup to the point where I felt as if I could impact upon the performance with physical control gestures, and embody my action within the music somewhat. Interestingly enough this performance setup was a complete 'hack' of Ableton's core functionality.", 
            "title": "Why I Live Code - by co34pt"
        }, 
        {
            "location": "/1-3-Why-SuperCollider/", 
            "text": "Why Live Code in SuperCollider?\n\n\n\n\n'You're brave to use SuperCollider!'\n - Anonymous, after a performance of mine, also probably slightly misremembered\n\n\nLooking at lvm's \nawesome-livecoding\n list, there are currently a whole bunch of live coding languages and platforms built around a whole bunch of paradigms, suited to many different users with varying aims, mediums, skillsets and abilities.\n\n\nSuperCollider sits on the back-end of a few live coding-specific languages, including \nFoxDot\n, \nTidalCycles\n (with \nSuperDirt\n), \nOvertone\n, \nixi lang\n and probably some I've forgotten, but within SuperCollider there is ample support for live coding in the form of various libraries and techniques (I use \nJITLib\n), and I've been using it since 2014 for \nperformances\n, \ncomposition\n and for building \nother projects\n.\n\n\nI've tried (and performed with) a bunch of other live coding platforms (mostly \nTidalCycles\n and \nFoxDot\n), and have repeatedly settled on SuperCollider over and over for live coding. As someone mostly performing metre-driven beat-based dance music, this can seem like an odd choice. Tidalcycles, for example, is specifically built around rhythmic cycles, and is a fast, efficient way to create complex rhythmic units. \n\n\nSuperCollider on the other hand has no one central method to produce rhythmic patterns or loops - instead there are a number of different ways to leverage \npattern classes\n, some of which are really quite unwieldy and not at all suited to live coding and rely on a lot of pretty complicated nesting. SuperCollider is also \nreally\n verbose - when creating patterns basic arguments need to be manually specified, which requires a lot of typing. In addition to this, SuperCollider has no real 'built-in' mechanisms for live performance - these often have to be built by the user and imported as libraries. This repo contains a number of \nSynthDefs\n, or 'instruments' that I have had to build myself or copy from elsewhere in order to perform basic functions within patterns - want to play a kickdrum sample? Better build a way to do that yourself! Want a square wave you can trigger as part of a pattern? Better go write that synth! \n\n\nIt's also full of strange \nundocumented methods and classes\n, which can hold keys to performance techniques that I'll never find because I don't know what they are - I had to catch someone using the method \n.stutter\n during a live set to figure out its potential uses for me. People who live code in SuperCollider also often do it very differently from each other, using different, sometimes not transferable sets of techniques - this is a result of SuperCollider being a comparatively enormous language, but as a result I had quite a bit of difficulty learning how to use it, especially for a musician who hadn't been coding very long (myself, when I first learned SuperCollider). \na mess\n\n\nSuperCollider can also be pretty unforgiving. With no built-in limiter, one incorrect argument can be absolutely devastating - The main perceptual difference between \nSinOsc.ar(400,0,1)\n and \nSinOsc.ar(400,0,10)\n, is pain. Especially when you're wearing headphones. It's also pretty easy to bring the whole server to a halt with a mis-typed \n\\dur\n argument.\n\n\nThe results of this? \n\n\nFrom absolutely nothing,\n\n\nd1 $ sound \nbd sn\n\n\n\n\n\nin Tidalcycles produces a kick-snare pattern, which can very easily be extended to \n\n\nd1 $ sound \nbd sn cp\n\n\n\n\n\nto produce a kick-snare-clap pattern. \n\n\nIn SuperCollider however, producing a kick-snare pattern can take a number of forms, but this is how I would end up doing it from boot-up (without any of the setup code in this repo).\n\n\na = Buffer.read(s,\n/path/to/kick/kick.wav\n);\nb = Buffer.read(s,\n/path/to/snare/snare.wav\n);\nSynthDef(\\bplay,\n    {arg out = 0, buf = 0, rate = 1, amp = 0.5, pan = 0, pos = 0, rel=15;\n    var sig,env ;\n    sig = Pan2.ar(PlayBuf.ar(2,buf,BufRateScale.ir(buf) * rate,1,BufDur.kr(buf)*pos*44100,doneAction:2),pan);\n    env = EnvGen.ar(Env.linen(0.0,rel,0),doneAction:2);\n    sig = sig * env;\n    sig = sig * amp;\n    Out.ar(out,sig);\n    }).add;\np = ProxySpace.push(s);\np.makeTempoClock;\n~k = Pbind(\\instrument,\\bplay,\\buf,a,\\dur,0.5,\\amp,1);\n~s = Pbind(\\instrument,\\bplay,\\buf,b,\\dur,1,\\amp,1);\n~k.play;\n~s.play;\n\n\n\n\nAnd in order to do the kick-snare-clap pattern I would have to add\n\n\nc = Buffer.read(s,\n/path/to/clap/clap.wav\n);\n~k = Pbind(\\instrument,\\bplay,\\buf,a,\\dur,1/3,\\amp,1);\n~s = Pbind(\\instrument,\\bplay,\\buf,b,\\dur,1/3,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,c,\\dur,1/3,\\amp,1);\n~c.play;\n\n\n\n\nSo \nwhy\n would I choose to use a system like this, when there are some that are much more efficient for the kinds of things I am doing? (I am being a \nlittle\n obtuse in the code example above for the sake of argument)\n\n\nThe answer is primarily, of course, \nbecause it works for me\n, but here's why.\n\n\nSuperCollider is a huge language, containing not only a really great set of pattern libraries and live coding functionality, but some of the best synthesis capabilities of any program I have ever used, and with extensions, the possible functionality I can draw upon is absolutely enormous. In this repo I'll be talking about how I use \nEuclidean Rhythms\n, \nNonlinear Maps\n, \nCommon fundamental frequencies\n \n53 tone scales\n, and many other techniques to make parts of music. SuperCollider's amazing array of native and extended functionality is not immediately usable for live coding from the time of installation, but with some reusable scaffolding in place  these features can be relatively easily leveraged. The issue of the verbosity of SuperCollider compared to Tidalcycles can be mitigated with setup code and extensions - it's taken me a while to build and work with structures to make using SuperCollider as a performer more effective, but once the framework is in place things get much easier, and can be tuned to suit any particular performance needs.\n\n\nThe lack of pre-built foundations is also liberating in some respects because if I want to get down to a 'lower level' during a performance it's trivial to do so. If I am hitting a wall during a performance of some heavy beats, the same library that allows me to change high-level pattern structures on the fly will also allow me to start multiplying bare sine waves and performing brutal additive synthesis live alongside these patterns. The code shown earlier of a kick, snare and clap all being run as separate 'instruments' is how I usually do my live coding, and while this seems very text-heavy and verbose, it allows me to create a number of small, relatively self-governing processes which will compute and play of their own accord, until I change them. Through this method, my performances usually involve building up musical textures and patterns through allowing each 'instrument' a small amount of its own variability - together each small amount of variability comes together to form a kind of emergent complexity, the sum of all of its (relatively) simple parts. Through the performance I'll then manage these units, building new ones as old ones become fatiguing, and injecting new life into stalwart units (such as kick/snare drums) by modifying their patterns/pitch/effects/etc. I like to think of this live coding performance setup as a kind of ecology of small units being constructed, managed, decommissioned, revamped and destroyed throughout a performance.\n\n\nThis kind of 'ecological' approach means that once the basis of a 'sound' during a performance are established (eg. hi-hat pattern, kick and two melody lines) I can spend some time building the next set of sounds, while the other sounds manage themselves and stay sonically interesting through some well(/poorly)-placed algorithmic transformations. The verbosity of the pattern language also helps in some respects too, having to type the names of individual parameters means I am forced to consider the nature of the sound I'm about to throw into the mix while I'm typing it. This is one of the reasons why I don't think I got on with Tidalcycles when I tried to perform solo with it - it's powerful enough to change the entire dynamic of a performance using a few characters, and I'm not responsible enough to wield that power.\n\n\nMy biggest gripe with SuperCollider is the pretty verbose Pattern syntax, as patterns are a huge part of my live performances. I think the pattern classes in SuperCollider are very powerful, but a lot of typing \ndoes\n need to be done. Fortunately the \nddwSnippets\n quark has finally arrived, delivering some snippets to the SuperCollider IDE! Before that, I would keep a bunch of 'default' patterns on hand in another document during performance to copy-paste and change. I've also heard that \nscvim\n is currently in active development, and as a vim user I'd love to integrate it as my SuperCollider editor.\n\n\nAll things considered, while not the most intuitive live coding platform, it's the one that works for me, and will probably continue to be so.", 
            "title": "Why SuperCollider?"
        }, 
        {
            "location": "/1-3-Why-SuperCollider/#why-live-code-in-supercollider", 
            "text": "'You're brave to use SuperCollider!'  - Anonymous, after a performance of mine, also probably slightly misremembered  Looking at lvm's  awesome-livecoding  list, there are currently a whole bunch of live coding languages and platforms built around a whole bunch of paradigms, suited to many different users with varying aims, mediums, skillsets and abilities.  SuperCollider sits on the back-end of a few live coding-specific languages, including  FoxDot ,  TidalCycles  (with  SuperDirt ),  Overtone ,  ixi lang  and probably some I've forgotten, but within SuperCollider there is ample support for live coding in the form of various libraries and techniques (I use  JITLib ), and I've been using it since 2014 for  performances ,  composition  and for building  other projects .  I've tried (and performed with) a bunch of other live coding platforms (mostly  TidalCycles  and  FoxDot ), and have repeatedly settled on SuperCollider over and over for live coding. As someone mostly performing metre-driven beat-based dance music, this can seem like an odd choice. Tidalcycles, for example, is specifically built around rhythmic cycles, and is a fast, efficient way to create complex rhythmic units.   SuperCollider on the other hand has no one central method to produce rhythmic patterns or loops - instead there are a number of different ways to leverage  pattern classes , some of which are really quite unwieldy and not at all suited to live coding and rely on a lot of pretty complicated nesting. SuperCollider is also  really  verbose - when creating patterns basic arguments need to be manually specified, which requires a lot of typing. In addition to this, SuperCollider has no real 'built-in' mechanisms for live performance - these often have to be built by the user and imported as libraries. This repo contains a number of  SynthDefs , or 'instruments' that I have had to build myself or copy from elsewhere in order to perform basic functions within patterns - want to play a kickdrum sample? Better build a way to do that yourself! Want a square wave you can trigger as part of a pattern? Better go write that synth!   It's also full of strange  undocumented methods and classes , which can hold keys to performance techniques that I'll never find because I don't know what they are - I had to catch someone using the method  .stutter  during a live set to figure out its potential uses for me. People who live code in SuperCollider also often do it very differently from each other, using different, sometimes not transferable sets of techniques - this is a result of SuperCollider being a comparatively enormous language, but as a result I had quite a bit of difficulty learning how to use it, especially for a musician who hadn't been coding very long (myself, when I first learned SuperCollider).  a mess  SuperCollider can also be pretty unforgiving. With no built-in limiter, one incorrect argument can be absolutely devastating - The main perceptual difference between  SinOsc.ar(400,0,1)  and  SinOsc.ar(400,0,10) , is pain. Especially when you're wearing headphones. It's also pretty easy to bring the whole server to a halt with a mis-typed  \\dur  argument.  The results of this?   From absolutely nothing,  d1 $ sound  bd sn   in Tidalcycles produces a kick-snare pattern, which can very easily be extended to   d1 $ sound  bd sn cp   to produce a kick-snare-clap pattern.   In SuperCollider however, producing a kick-snare pattern can take a number of forms, but this is how I would end up doing it from boot-up (without any of the setup code in this repo).  a = Buffer.read(s, /path/to/kick/kick.wav );\nb = Buffer.read(s, /path/to/snare/snare.wav );\nSynthDef(\\bplay,\n    {arg out = 0, buf = 0, rate = 1, amp = 0.5, pan = 0, pos = 0, rel=15;\n    var sig,env ;\n    sig = Pan2.ar(PlayBuf.ar(2,buf,BufRateScale.ir(buf) * rate,1,BufDur.kr(buf)*pos*44100,doneAction:2),pan);\n    env = EnvGen.ar(Env.linen(0.0,rel,0),doneAction:2);\n    sig = sig * env;\n    sig = sig * amp;\n    Out.ar(out,sig);\n    }).add;\np = ProxySpace.push(s);\np.makeTempoClock;\n~k = Pbind(\\instrument,\\bplay,\\buf,a,\\dur,0.5,\\amp,1);\n~s = Pbind(\\instrument,\\bplay,\\buf,b,\\dur,1,\\amp,1);\n~k.play;\n~s.play;  And in order to do the kick-snare-clap pattern I would have to add  c = Buffer.read(s, /path/to/clap/clap.wav );\n~k = Pbind(\\instrument,\\bplay,\\buf,a,\\dur,1/3,\\amp,1);\n~s = Pbind(\\instrument,\\bplay,\\buf,b,\\dur,1/3,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,c,\\dur,1/3,\\amp,1);\n~c.play;  So  why  would I choose to use a system like this, when there are some that are much more efficient for the kinds of things I am doing? (I am being a  little  obtuse in the code example above for the sake of argument)  The answer is primarily, of course,  because it works for me , but here's why.  SuperCollider is a huge language, containing not only a really great set of pattern libraries and live coding functionality, but some of the best synthesis capabilities of any program I have ever used, and with extensions, the possible functionality I can draw upon is absolutely enormous. In this repo I'll be talking about how I use  Euclidean Rhythms ,  Nonlinear Maps ,  Common fundamental frequencies   53 tone scales , and many other techniques to make parts of music. SuperCollider's amazing array of native and extended functionality is not immediately usable for live coding from the time of installation, but with some reusable scaffolding in place  these features can be relatively easily leveraged. The issue of the verbosity of SuperCollider compared to Tidalcycles can be mitigated with setup code and extensions - it's taken me a while to build and work with structures to make using SuperCollider as a performer more effective, but once the framework is in place things get much easier, and can be tuned to suit any particular performance needs.  The lack of pre-built foundations is also liberating in some respects because if I want to get down to a 'lower level' during a performance it's trivial to do so. If I am hitting a wall during a performance of some heavy beats, the same library that allows me to change high-level pattern structures on the fly will also allow me to start multiplying bare sine waves and performing brutal additive synthesis live alongside these patterns. The code shown earlier of a kick, snare and clap all being run as separate 'instruments' is how I usually do my live coding, and while this seems very text-heavy and verbose, it allows me to create a number of small, relatively self-governing processes which will compute and play of their own accord, until I change them. Through this method, my performances usually involve building up musical textures and patterns through allowing each 'instrument' a small amount of its own variability - together each small amount of variability comes together to form a kind of emergent complexity, the sum of all of its (relatively) simple parts. Through the performance I'll then manage these units, building new ones as old ones become fatiguing, and injecting new life into stalwart units (such as kick/snare drums) by modifying their patterns/pitch/effects/etc. I like to think of this live coding performance setup as a kind of ecology of small units being constructed, managed, decommissioned, revamped and destroyed throughout a performance.  This kind of 'ecological' approach means that once the basis of a 'sound' during a performance are established (eg. hi-hat pattern, kick and two melody lines) I can spend some time building the next set of sounds, while the other sounds manage themselves and stay sonically interesting through some well(/poorly)-placed algorithmic transformations. The verbosity of the pattern language also helps in some respects too, having to type the names of individual parameters means I am forced to consider the nature of the sound I'm about to throw into the mix while I'm typing it. This is one of the reasons why I don't think I got on with Tidalcycles when I tried to perform solo with it - it's powerful enough to change the entire dynamic of a performance using a few characters, and I'm not responsible enough to wield that power.  My biggest gripe with SuperCollider is the pretty verbose Pattern syntax, as patterns are a huge part of my live performances. I think the pattern classes in SuperCollider are very powerful, but a lot of typing  does  need to be done. Fortunately the  ddwSnippets  quark has finally arrived, delivering some snippets to the SuperCollider IDE! Before that, I would keep a bunch of 'default' patterns on hand in another document during performance to copy-paste and change. I've also heard that  scvim  is currently in active development, and as a vim user I'd love to integrate it as my SuperCollider editor.  All things considered, while not the most intuitive live coding platform, it's the one that works for me, and will probably continue to be so.", 
            "title": "Why Live Code in SuperCollider?"
        }, 
        {
            "location": "/1-4-How-To-Use-This-Repo/", 
            "text": "How to use this repo\n\n\n\n\nThis repo is designed to be an interactive and explorable set of guides, as well as being browsable online and outside of SuperCollider. It's also designed to be as platform-agnostic as possible, and easily accessible with GitHub's basic tools.\n\n\nIf you are reading this online, and want to use any examples contained in these documents, or see any code I've written 'in the flesh', there's a few things you'll need to do.\n\n\n\n\nInstall \nSuperCollider\n\n\nInstall \nSC3-Plugins\n (optional but recommended, reasons for this are documented in 2.1)\n\n\nNOTE: steps 1 and 2 on *buntu (and probably other types of Linux too) can be performed with the \ninstall_supercollider_linux.sh\n script in the \nscripts\n folder. This installs both SuperCollider 3.8 and sc3-plugins as well as all relevant dependencies for *buntu \n\n\n\n\n\n\nInstall the recommended Quarks by evaluating the following code in SuperCollider (optional but recommended, reasons for this are documented in 2.1)\n\n\n\n\n(\nQuarks.install(\nBjorklund\n);\nQuarks.install(\nBatLib\n);\nQuarks.install(\nddwSnippets\n);\n)\n\n\n\n\n\n\nClone this repo, which can be done from \nthe repo page\n, using the 'Clone or Download' page\n\n\n\n\nThe articles in this repo are grouped into category folders, which are then contained in subfolders with the name of the article. Each subfolder will have in it a \n.md\n file, which contains the text of the article containing examples, and usually a separate \n.scd\n file, which will be the examples from the \n.md\n file extracted and packaged for direct use within SuperCollider. The \n.scd\n files are designed to be loaded and run directly, and will usually contain a \n.loadRelative\n which will Setup that you will need to get started. You should then be able to run all examples.\n\n\nIf you want to read online while playing with examples in SuperCollider, run the \nSetup.scd\n file within this repository and then copy-paste the examples on the site into a file in SuperCollider and it \nshould\n work. I need people to test these examples, so if it doesn't work, please raise an issue on GitHub with exactly what you've done and I'll look into it when I can, or get in touch with me.", 
            "title": "How To Use This Repo"
        }, 
        {
            "location": "/1-4-How-To-Use-This-Repo/#how-to-use-this-repo", 
            "text": "This repo is designed to be an interactive and explorable set of guides, as well as being browsable online and outside of SuperCollider. It's also designed to be as platform-agnostic as possible, and easily accessible with GitHub's basic tools.  If you are reading this online, and want to use any examples contained in these documents, or see any code I've written 'in the flesh', there's a few things you'll need to do.   Install  SuperCollider  Install  SC3-Plugins  (optional but recommended, reasons for this are documented in 2.1)  NOTE: steps 1 and 2 on *buntu (and probably other types of Linux too) can be performed with the  install_supercollider_linux.sh  script in the  scripts  folder. This installs both SuperCollider 3.8 and sc3-plugins as well as all relevant dependencies for *buntu     Install the recommended Quarks by evaluating the following code in SuperCollider (optional but recommended, reasons for this are documented in 2.1)   (\nQuarks.install( Bjorklund );\nQuarks.install( BatLib );\nQuarks.install( ddwSnippets );\n)   Clone this repo, which can be done from  the repo page , using the 'Clone or Download' page   The articles in this repo are grouped into category folders, which are then contained in subfolders with the name of the article. Each subfolder will have in it a  .md  file, which contains the text of the article containing examples, and usually a separate  .scd  file, which will be the examples from the  .md  file extracted and packaged for direct use within SuperCollider. The  .scd  files are designed to be loaded and run directly, and will usually contain a  .loadRelative  which will Setup that you will need to get started. You should then be able to run all examples.  If you want to read online while playing with examples in SuperCollider, run the  Setup.scd  file within this repository and then copy-paste the examples on the site into a file in SuperCollider and it  should  work. I need people to test these examples, so if it doesn't work, please raise an issue on GitHub with exactly what you've done and I'll look into it when I can, or get in touch with me.", 
            "title": "How to use this repo"
        }, 
        {
            "location": "/2-1-Recommended-Addons/", 
            "text": "SuperCollider Addons I'd recommend\n\n\n==============\n\n\nHere is a list of Extensions and Quarks that are crucial to my live performances. If you want to be able to use all of the resources in this repo, you should install them.\n\n\nExtensions\n\n\nExtensions have to be inserted into SuperCollider manually. See \nthis\n document for more information. Note sc3-plugins have to be compiled on Linux. See the sc3-plugins readme on GitHub for more information.\n\n\nsc3-Plugins\n\n\n'This repository contains the community collection of unit generator plugins for SuperCollider. An installation extends the functionality of SuperCollider by additional UGens that run on scsynth, the SuperCollider audio synthesis server.'\n\n\nsc3-plugins is a mixed bag of tools, and contains a lot of things I don't use, but it's pretty essential for getting the most out of SuperCollider. Some of the sc3-plugins are fairly scantily-documented, and fall into the 'sounds cool, but no idea what it does or how it works' category.\n\n\nParticular tools from sc3-plugins I use regularly:\n\n\n\n\n\n\nConcat\n and \nConcat2\n\nTools for \nconcatenative synthesis\n. Particularly useful when dealing with speech and sampling - I've used them to 'reconstruct' speech using existing samples.\n\n\n\n\n\n\nDecimator\n and \nSmoothDecimator\n\n\nBitcrushing\n effect Ugens for that classic digital destruction sound. SmoothDecimator has a smoothing option to take some of the digital bite out of the bitcrushing sound.\n\n\n\n\n\n\nSawDPW\n (and \nPulseDPW\n)\nAlternatives to SuperCollider's native \nSaw\n and \nPulse\n Ugens, which alias much less, use less CPU and sound an awful lot better especially during additive synthesis. Can also get really wild at unusual frequencies.\n\n\n\n\n\n\nDFM1\n\nA really fantastic sounding digitally-modelled analog filter. Great both as a scuzzy-sounding filter on existing sounds and when pushed into self oscillation to make rich drones. Sounds good both in moderation and absurdity.\n\n\n\n\n\n\nCrossoverDistortion\n\nA savage distortion. I don't really have a lot more to say about it.\n\n\n\n\n\n\nWaveLoss\n\nAn effect for dropping sections of waveforms in either a deterministic or random fashion. Produces a 'degradation' effect from slight dropouts all the way to isolated spluttering.\n\n\n\n\n\n\nBenoitLib\n\n\nA set of SuperCollider extensions used by Beno\u00eet and the Mandelbrots.\n\n\nThe main tool I install this for is Pkr, a pattern proxy for synchronising control rate Ugens inside of patterns, which is a technique I will be covering in this repo. It's a small part of the extension but is totally invaluable for my performances.\n\n\nThere's also some super useful stuff in BenoitLib for collaborative performance which I have used before in a performance with \nShelly Knotts\n, including MandelHub and MandelClock\n\n\nQuarks\n\n\nQuarks can be installed from within SuperCollider, either by installing them manually (\nQuarks.install('BatLib')\n for example), or using \nQuarks.gui\n to bring up a gui install them there.\n\n\nBjorklund Quark\n\n\nThe Bjorklund quark implements Euclidean Rhythms, a concept outlined in \nthis paper\n, involving taking a number of onsets and a number of possible steps, and spaces out the onsets as equally as possible in the given number of steps. A verbal explanation of this doesn't really do it any justice, so I'd encourage you to use \nthis cool web app\n which visually and aurally explains what these rhythms are. I've found Euclidean rhythms a great way to program rhythm that is dynamic and interesting, but also sits well within a set of metric dance music. The class I use from this quark is Pbjorklund2, which gives an array of durations for euclidean rhythms.\n\n\nBatLib Quark\n\n\nBatLib contains StageLimiter, a class that puts a basic \nlimiter\n across all sounds in the SuperCollider server. StageLimiter doesn't really have any effect on the sound the server makes unless you exceed an amplitude of +/- 1 (the top and bottom of the default SuperCollider scope), and when you do push harder than that, you can use it creatively to get 'side-chaining' type effects. I'd recommend always running StageLimiter unless you have a specific reason not to anyway, as an amplitude value that is accidentally out by a factor of ten can be \nreally\n painful.\n\n\nddwSnippets Quark\n\n\nddwSnippets is a 'Rudimentary snippets facility for ScIDE, implemented in sclang'. I've found \nsnippets\n are very useful for any piece of text that will be typed multiple times during a performance, or to lay the groundwork for 'basic' musical patterns without having to write them from scratch (see my comments in 0-2 about SuperCollider's verbosity). I use ddwSnippets to realise musical ideas more quickly when performing, especially using Ugens or patterns that have a lot of arguments, without having to copy-paste from a 'template' file containing the snippets.\n\n\n==================\n\n\nsc3-plugins and BenoitLib have to be installed manually.\na note for compiling sc3-plugins on Linux is that my /path/to/scsource is /usr/local/include/SuperCollider, and I would assume that would be a typical path for most users\nTo install all quarks listed in this document, execute the following in SuperCollider:\n\n\n(\nQuarks.install(\nBjorklund\n);\nQuarks.install(\nBatLib\n);\nQuarks.install(\nddwSnippets\n);\n)\n\n\n\n\nIf you have trouble installing \nddwSnippets\n, execute this too:\n\n\n(\nQuarks.install(\nhttps://github.com/jamshark70/ddwSnippets.git\n);\n)", 
            "title": "Recommended Addons"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#supercollider-addons-id-recommend", 
            "text": "==============  Here is a list of Extensions and Quarks that are crucial to my live performances. If you want to be able to use all of the resources in this repo, you should install them.", 
            "title": "SuperCollider Addons I'd recommend"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#extensions", 
            "text": "Extensions have to be inserted into SuperCollider manually. See  this  document for more information. Note sc3-plugins have to be compiled on Linux. See the sc3-plugins readme on GitHub for more information.", 
            "title": "Extensions"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#sc3-plugins", 
            "text": "'This repository contains the community collection of unit generator plugins for SuperCollider. An installation extends the functionality of SuperCollider by additional UGens that run on scsynth, the SuperCollider audio synthesis server.'  sc3-plugins is a mixed bag of tools, and contains a lot of things I don't use, but it's pretty essential for getting the most out of SuperCollider. Some of the sc3-plugins are fairly scantily-documented, and fall into the 'sounds cool, but no idea what it does or how it works' category.  Particular tools from sc3-plugins I use regularly:    Concat  and  Concat2 \nTools for  concatenative synthesis . Particularly useful when dealing with speech and sampling - I've used them to 'reconstruct' speech using existing samples.    Decimator  and  SmoothDecimator  Bitcrushing  effect Ugens for that classic digital destruction sound. SmoothDecimator has a smoothing option to take some of the digital bite out of the bitcrushing sound.    SawDPW  (and  PulseDPW )\nAlternatives to SuperCollider's native  Saw  and  Pulse  Ugens, which alias much less, use less CPU and sound an awful lot better especially during additive synthesis. Can also get really wild at unusual frequencies.    DFM1 \nA really fantastic sounding digitally-modelled analog filter. Great both as a scuzzy-sounding filter on existing sounds and when pushed into self oscillation to make rich drones. Sounds good both in moderation and absurdity.    CrossoverDistortion \nA savage distortion. I don't really have a lot more to say about it.    WaveLoss \nAn effect for dropping sections of waveforms in either a deterministic or random fashion. Produces a 'degradation' effect from slight dropouts all the way to isolated spluttering.", 
            "title": "sc3-Plugins"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#benoitlib", 
            "text": "A set of SuperCollider extensions used by Beno\u00eet and the Mandelbrots.  The main tool I install this for is Pkr, a pattern proxy for synchronising control rate Ugens inside of patterns, which is a technique I will be covering in this repo. It's a small part of the extension but is totally invaluable for my performances.  There's also some super useful stuff in BenoitLib for collaborative performance which I have used before in a performance with  Shelly Knotts , including MandelHub and MandelClock", 
            "title": "BenoitLib"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#quarks", 
            "text": "Quarks can be installed from within SuperCollider, either by installing them manually ( Quarks.install('BatLib')  for example), or using  Quarks.gui  to bring up a gui install them there.", 
            "title": "Quarks"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#bjorklund-quark", 
            "text": "The Bjorklund quark implements Euclidean Rhythms, a concept outlined in  this paper , involving taking a number of onsets and a number of possible steps, and spaces out the onsets as equally as possible in the given number of steps. A verbal explanation of this doesn't really do it any justice, so I'd encourage you to use  this cool web app  which visually and aurally explains what these rhythms are. I've found Euclidean rhythms a great way to program rhythm that is dynamic and interesting, but also sits well within a set of metric dance music. The class I use from this quark is Pbjorklund2, which gives an array of durations for euclidean rhythms.", 
            "title": "Bjorklund Quark"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#batlib-quark", 
            "text": "BatLib contains StageLimiter, a class that puts a basic  limiter  across all sounds in the SuperCollider server. StageLimiter doesn't really have any effect on the sound the server makes unless you exceed an amplitude of +/- 1 (the top and bottom of the default SuperCollider scope), and when you do push harder than that, you can use it creatively to get 'side-chaining' type effects. I'd recommend always running StageLimiter unless you have a specific reason not to anyway, as an amplitude value that is accidentally out by a factor of ten can be  really  painful.", 
            "title": "BatLib Quark"
        }, 
        {
            "location": "/2-1-Recommended-Addons/#ddwsnippets-quark", 
            "text": "ddwSnippets is a 'Rudimentary snippets facility for ScIDE, implemented in sclang'. I've found  snippets  are very useful for any piece of text that will be typed multiple times during a performance, or to lay the groundwork for 'basic' musical patterns without having to write them from scratch (see my comments in 0-2 about SuperCollider's verbosity). I use ddwSnippets to realise musical ideas more quickly when performing, especially using Ugens or patterns that have a lot of arguments, without having to copy-paste from a 'template' file containing the snippets.  ==================  sc3-plugins and BenoitLib have to be installed manually.\na note for compiling sc3-plugins on Linux is that my /path/to/scsource is /usr/local/include/SuperCollider, and I would assume that would be a typical path for most users\nTo install all quarks listed in this document, execute the following in SuperCollider:  (\nQuarks.install( Bjorklund );\nQuarks.install( BatLib );\nQuarks.install( ddwSnippets );\n)  If you have trouble installing  ddwSnippets , execute this too:  (\nQuarks.install( https://github.com/jamshark70/ddwSnippets.git );\n)", 
            "title": "ddwSnippets Quark"
        }, 
        {
            "location": "/2-2-Why-ProxySpace/", 
            "text": "ProxySpace - My Foundation for Live Coding in SuperCollider\n\n\n\n\nWhy ProxySpace?\n\n\nIf you haven't heard of or used it before, \nProxySpace\n and it's associated \nJITLib\n are \nwell\n worth knowing about, and are without exception what I use to live code in SuperCollider.\n\n\nAccording to the docs (see link above):\n\n\n\n\nGenerally a proxy is a placeholder for something. A node proxy is a placeholder for something playing on a server that writes to a limited number of busses (e.g. a synth or an event stream). NodeProxy objects can be replaced and recombined while they play. Also they can be used to build a larger structure which is used and modified later on.\n\n\n\n\nIn other words, ProxySpace opens up SuperCollider's language into a powerful performance tool by allowing individual functions/patterns/etc to become flexible and modifiable, as well as to make these patterns interact. When using ProxySpace, the traditional\n\n\n{SinOsc.ar(440,0,0.2)!2}.play\n\n\n\n\nis turned into an 'instrument' when given an arbitrary name and edited on the fly. It can also be used within other 'instruments', for example:\n\n\n(\n~sine1 = {SinOsc.ar(440,0,0.2)!2};\n~modulation = {Saw.ar(10,0,1)!2};\n~sine2 = {~sine1 * ~modulation};\n~sine2.play;\n)\n\n\n\n\nProxySpace can also be used for synchronising together \npatterns\n (including percussion, melodies, basses etc) in a quick and easy way, while allowing them to be edited and combined on-the-fly. Most of my live sets revolve around the creation (and destruction) of patterns, and ProxySpace makes this really quite easy. With ProxySpace I can build a performance using multiple self-managing 'instruments' and play them as I build them. By doing this I can think reasonably laterally about the performance, building up and packing down individual 'instruments' as I need them, while all of the existing 'instruments' continue playing. It also has some functionality such as automatic crossfading which is very useful for creating smooth performances.\n\n\nIn addition, while I don't use it very much, the \nProxyMixer\n class uses SuperCollider's GUI to automatically create a visual mixer to change the levels of all 'instruments' created.\n\n\nI've written two extended examples of how I use ProxySpace which are in this folder. They are musical examples that I would use in live performances I deliver. Open them up in your SuperCollider IDE and follow along. If you are browsing via GitHub Pages, the tutorials can be found \nhere for basics\n and \nhere for patterns\n\n\nProxySpace (and JITLib in general) also have \ngreat\n documentation, which i'd recommend:\n- \nProxySpace Examples\n\n- The \nJITLib Basic Concepts\n series\n- \nJITLib Overview", 
            "title": "ProxySpace - Live Coding in SuperCollider"
        }, 
        {
            "location": "/2-2-Why-ProxySpace/#proxyspace-my-foundation-for-live-coding-in-supercollider", 
            "text": "", 
            "title": "ProxySpace - My Foundation for Live Coding in SuperCollider"
        }, 
        {
            "location": "/2-2-Why-ProxySpace/#why-proxyspace", 
            "text": "If you haven't heard of or used it before,  ProxySpace  and it's associated  JITLib  are  well  worth knowing about, and are without exception what I use to live code in SuperCollider.  According to the docs (see link above):   Generally a proxy is a placeholder for something. A node proxy is a placeholder for something playing on a server that writes to a limited number of busses (e.g. a synth or an event stream). NodeProxy objects can be replaced and recombined while they play. Also they can be used to build a larger structure which is used and modified later on.   In other words, ProxySpace opens up SuperCollider's language into a powerful performance tool by allowing individual functions/patterns/etc to become flexible and modifiable, as well as to make these patterns interact. When using ProxySpace, the traditional  {SinOsc.ar(440,0,0.2)!2}.play  is turned into an 'instrument' when given an arbitrary name and edited on the fly. It can also be used within other 'instruments', for example:  (\n~sine1 = {SinOsc.ar(440,0,0.2)!2};\n~modulation = {Saw.ar(10,0,1)!2};\n~sine2 = {~sine1 * ~modulation};\n~sine2.play;\n)  ProxySpace can also be used for synchronising together  patterns  (including percussion, melodies, basses etc) in a quick and easy way, while allowing them to be edited and combined on-the-fly. Most of my live sets revolve around the creation (and destruction) of patterns, and ProxySpace makes this really quite easy. With ProxySpace I can build a performance using multiple self-managing 'instruments' and play them as I build them. By doing this I can think reasonably laterally about the performance, building up and packing down individual 'instruments' as I need them, while all of the existing 'instruments' continue playing. It also has some functionality such as automatic crossfading which is very useful for creating smooth performances.  In addition, while I don't use it very much, the  ProxyMixer  class uses SuperCollider's GUI to automatically create a visual mixer to change the levels of all 'instruments' created.  I've written two extended examples of how I use ProxySpace which are in this folder. They are musical examples that I would use in live performances I deliver. Open them up in your SuperCollider IDE and follow along. If you are browsing via GitHub Pages, the tutorials can be found  here for basics  and  here for patterns  ProxySpace (and JITLib in general) also have  great  documentation, which i'd recommend:\n-  ProxySpace Examples \n- The  JITLib Basic Concepts  series\n-  JITLib Overview", 
            "title": "Why ProxySpace?"
        }, 
        {
            "location": "/2-3-Setup-Code---Making-Performance-Easier/", 
            "text": "Setup Code - Making Performance Easier\n\n\n\n\nIn the root directory of this repo, there is a \nSetup\n folder, which contains some files, including \nSetup.scd\n, \nSynthDefs.scd\n and \nSnippets.scd\n. \n\n\nAs I mentioned in 'Why SuperCollider', one of my big gripes with SuperCollider and performing with it is the amount of pre-building that needs to be done in order to incorporate any higher level structures, such as playing samples, triggering instruments, and suchlike. This setup folder addresses that problem, and contains my personal SuperCollider performance setup, and can be loaded entirely by either running the \nSetup.scd\n file, or calling it from somewhere else (for example in line 14 of the \nsecond ProxySpace tutorial\n by specifying the relative filepath to the setup file and using the \n.loadRelative\n method on it. I can (and have) performed without this setup file, but for the most part I run this setup file before any performance I do.\n\n\nThe \nSetup.scd\n file does the following things:\n- Increase the number of buffers available for SuperCollider to load\n- Increase the amount of memory size available to the Server, to allow for more CPU-heavy work\n- Boot the server\n Display the server Oscilloscope (Which I regularly use as visuals in my set)\n- Start ProxySpace, and make a 60BPM proxy tempo clock\n- Lines 20-27:\n    - Creates a \nDictionary\n, \nd\n, to hold samples\n    - Recursively loads all samples of the correct set in the \nsamples\n folder. These samples are organised into folders which contain the samples. The name of the folder will be added as an entry to the dictionary, and the samples will be added as sub-entries. \n        - For example, if you wanted to reference the second sample in the kick drum folder you would use \nd[\"k\"][1]\n (\nd\n for the dictionary, \n\"k\"\n as kickdrums are held in directory \n\"k\"\n, and \n1\n as you are referencing the second sample)\n- Loads the \nSynthDefs.scd\n file, containing some custom SynthDefs which I use inside of patterns. Notably the necessary synthdef for playing samples \nbplay\n, and some instruments such as \nsinfb\n and \nring1\n.\n8. Loads the \nSnippets.scd\n file, which contains some snippets to be loaded into the \nddwSnippets Quark\n, for easy access during performance, which include basic percussion patterns, some functions and some patterns that have a lot of default arguments I might not remember while performing\n9. Starts \nStageLimiter\n from the BatLib quark, to protect everyone's ears\n10. Posts a message to show all the above have been completed\n\n\nOnce this setup file has been run, everything is set up to perform, all in one evaluation. The \n.loadRelative\ns in the Setup file also means if any SynthDefs or Snippets are added and saved, they will be loaded next time the setup file is loaded.\n\n\nIf you're following any examples/etc from this repo, and it doesn't work and I haven't said anything about the setup file, assume that you need to run it for the code to work!", 
            "title": "Setup Code: Making Performance Easier"
        }, 
        {
            "location": "/2-3-Setup-Code---Making-Performance-Easier/#setup-code-making-performance-easier", 
            "text": "In the root directory of this repo, there is a  Setup  folder, which contains some files, including  Setup.scd ,  SynthDefs.scd  and  Snippets.scd .   As I mentioned in 'Why SuperCollider', one of my big gripes with SuperCollider and performing with it is the amount of pre-building that needs to be done in order to incorporate any higher level structures, such as playing samples, triggering instruments, and suchlike. This setup folder addresses that problem, and contains my personal SuperCollider performance setup, and can be loaded entirely by either running the  Setup.scd  file, or calling it from somewhere else (for example in line 14 of the  second ProxySpace tutorial  by specifying the relative filepath to the setup file and using the  .loadRelative  method on it. I can (and have) performed without this setup file, but for the most part I run this setup file before any performance I do.  The  Setup.scd  file does the following things:\n- Increase the number of buffers available for SuperCollider to load\n- Increase the amount of memory size available to the Server, to allow for more CPU-heavy work\n- Boot the server\n Display the server Oscilloscope (Which I regularly use as visuals in my set)\n- Start ProxySpace, and make a 60BPM proxy tempo clock\n- Lines 20-27:\n    - Creates a  Dictionary ,  d , to hold samples\n    - Recursively loads all samples of the correct set in the  samples  folder. These samples are organised into folders which contain the samples. The name of the folder will be added as an entry to the dictionary, and the samples will be added as sub-entries. \n        - For example, if you wanted to reference the second sample in the kick drum folder you would use  d[\"k\"][1]  ( d  for the dictionary,  \"k\"  as kickdrums are held in directory  \"k\" , and  1  as you are referencing the second sample)\n- Loads the  SynthDefs.scd  file, containing some custom SynthDefs which I use inside of patterns. Notably the necessary synthdef for playing samples  bplay , and some instruments such as  sinfb  and  ring1 .\n8. Loads the  Snippets.scd  file, which contains some snippets to be loaded into the  ddwSnippets Quark , for easy access during performance, which include basic percussion patterns, some functions and some patterns that have a lot of default arguments I might not remember while performing\n9. Starts  StageLimiter  from the BatLib quark, to protect everyone's ears\n10. Posts a message to show all the above have been completed  Once this setup file has been run, everything is set up to perform, all in one evaluation. The  .loadRelative s in the Setup file also means if any SynthDefs or Snippets are added and saved, they will be loaded next time the setup file is loaded.  If you're following any examples/etc from this repo, and it doesn't work and I haven't said anything about the setup file, assume that you need to run it for the code to work!", 
            "title": "Setup Code - Making Performance Easier"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/", 
            "text": "Pbinds and Patterns\n\n\n\n\nIntroduction\n\n\nAccording to SuperCollider's \nPractical Guide to Patterns\n\n\n\n\nPatterns describe calculations without explicitly stating every step. They are a higher-level representation of a computational task. While patterns are not ideally suited for every type of calculation, when they are appropriate they free the user from worrying about every detail of the process. Using patterns, one writes what is supposed to happen, rather than how to accomplish it.\n\n\n\n\nA large part of my live coding performances involve using patterns, specifically \nPbinds\n as Proxies inside of ProxySpace (see section 2.2.1) to create rhythmic elements that are synchronised to ProxySpace's \nTempoClock\n. In this case, 'rhythmic elements' can mean percussion, melody, bass, pads, or generally anything that is played 'in tempo'.\n\n\nSynthDefs, Arguments and Pbinds\n\n\nThe Pbinds I perform with work in conjunction with a set of \nSynthDefs\n (these can be found in the SynthDefs.scd file of the Setup folder) which serve various musical purposes, and plays them with specifies arguments at a given duration. This isn't a particularly intuitive concept to explain, but an example can help illustrate how this works. Take the SynthDef I use the most, \nbplay\n:\n\n\nSynthDef(\\bplay,\n    {arg out = 0, buf = 0, rate = 1, amp = 0.5, pan = 0, pos = 0, rel=15;\n        var sig,env ;\n        sig = Pan2.ar(PlayBuf.ar(2,buf,BufRateScale.ir(buf) * rate,1,BufDur.kr(buf)*pos*44100,doneAction:2),pan);\n        env = EnvGen.ar(Env.linen(0.0,rel,0),doneAction:2);\n        sig = sig * env;\n        sig = sig * amp;\n        Out.ar(out,sig);\n}).add;\n\n\n\n\nbplay\n is a simple stereo-panned sample player driven by the \nPlayBuf\n class, which takes the following arguments:\n\n\n\n\nout\n: the bus to be played to (this is needed as an argument for the SynthDef to work correctly inside ProxySpace, and I don't usually touch it\n\n\nbuf\n: the buffer to be read by the synth (all of which are loaded into dictionary \nd\n by default\n\n\nrate\n: the speed the sample will be played at (with no compensation for pitch)\n\n\namp\n: how loud the sample is, with 1 being the original volume of the sample\n\n\npan\n: where the sound is placed in the stereo field, with \n0\n being centre\n\n\npos\n: the position from which the sample starts playing, normalised from \n0\n to \n1\n, e.g. a value of \n0.5\n will play the sample from the middle \n\n\nrel\n: in this case specifies how long the server is allowed to keep the instance open before freeing it. Normally the instance will be freed when the sample is finished playing, but in the case of very long samples or samples played backwards this freeing may not occur, leading to server load building in the background due to dead running processes. This default value of 15 \n\n\n\n\nPbinds also have some arguments that need satisfying:\n\n\n\n\ninstrument\n: the SynthDef that will be used to deliver this 'instance' in the pattern\n\n\ndur\n: The duration of each 'instance', if used directly in ProxySpace, a \ndur\n value of 1 results in an 'instance' once every clock cycle. \nNote: the default \ndur\n value of a Pbind is 1, and the default \ninstrument\n value is SuperCollider's built in Piano synth, but specifying both anyway (especially \ninstrument\n) is good practice.\n\n\n\n\nSo, if I wanted to have a kick drum playing once each beat in time with the ProxySpace timer, after I had run my setup file I would do the following:\n\n\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1);\n~k.play;\n)\n\n\n\n\nThis Pbind \n~k\n, spefifies that the instrument it will be using is \nbplay\n, the buffer \nbplay\n reads from will be the first index of the \nk\n entry in the dictionary (which contains kick drums), and that the \ndur\n/duration is \n1\n, once per cycle. If any arguments that the SynthDef takes are not specified as part of the Pbind, the SynthDef's default values will be used. Pbind arguments have to be given as key-value pairs, anything else will result in a syntax error, eg:\n\n\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\rate);\n\n\n\n\nAs part of these key-value pairs, Pbinds can take Pattern classes as inputs. \nPwhite\n gives random values between a minimum and maximum. If I wanted to specify a random pitch of the kick drum, I could add this to the pattern:\n\n\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\rate,Pwhite(1,1.2));\n\n\n\n\nNesting pattern classes\n\n\nPattern classes can also be \nnested\n. Here are a few examples of some more complex percussive patterns. Once you start nesting pattern classes, things can get complicated quite quickly.\n\n\n//to play with these examples, make sure the Setup File has been run\n\n//footwork kickdrums\n(\np.clock.tempo = 2.4;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,Pbjorklund2(Pseq([3,3,3,5],inf),8)/4,\\amp,1,\\rate,Pseq([1,1.2],inf));\n~k.play;\n)\n\n//skittery hi-hats\n(\np.clock.tempo = 1.5;\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pwrand([0.25,Pseq([0.125],2),0.5,Pseq([0.125/2],4)],[4,1,1,0.5].normalizeSum,inf),\\amp,Pwhite(0.2,1));\n~h.play;\n)\n\n//offset percussion patterns for techno feel behind a basic kick\n(\np.clock.tempo = 135/60;\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nsfx\n][6],\\dur,Pbjorklund2(Pexprand(2,15).round(1),16,inf,Pwhite(1,5).asStream)/4,\\amp,1,\\rate,2.2);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nsfx\n][6],\\dur,Pbjorklund2(Pexprand(2,15).round(1),16,inf,Pwhite(1,5).asStream)/4,\\amp,1,\\rate,1.9);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nsk\n][0],\\dur,1,\\amp,5);\n~c.play;\n~c2.play;\n~k.play;\n)\n\n//snare running forwards and back\n(\np.clock.tempo = 150/60;\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][4],\\dur,Pwhite(1,4)/2,\\amp,1,\\rate,Prand([1,-1],inf),\\pos,Pkey(\\rate).linlin(-2,2,0.9,0));\n~sn.play;\n)\n\n\n\n\nExtra arguments for melody/pitch\n\n\nPbinds also have some additional trickery for anything involving pitch.\n\n\nLet's look at the \nsinfb\n SynthDef (the arguments are listed in the code block for simplicity)\n\n\n//SinFB Bass\n(\nSynthDef(\\sinfb, {\n    arg freq = 440, atk = 0.01, sus = 0, rel = 1, fb = 0, amp = 0.3, out = 0, pan=0;\n    var sig, env;\n    env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,1,2);\n    sig = SinOscFB.ar(freq,fb,1);\n    sig = sig*env;\n    Out.ar(out,Pan2.ar(sig,pan,amp));\n}).add;\n);\n/*\nfreq: frequency\natk: attack\nsus: sustain\nrel: release\nfb: phase feedback\namp: amplitude\nout: output bus\npan: stereo panning\n*/\n\n\n\n\nHere, the \nfreq\n argument is the pitch of the oscillator. Pitch can be specified manually, like so:\n\n\n~sinfb = Pbind(\\instrument,\\sinfb,\\dur,0.25,\\freq,Pwhite(100,900));\n\n\n\n\nHowever, if a variable in a SynthDef is given the name \nfreq\n, Pbind allows the specification of the following in place of \nfreq\n to activate a 'scale mode':\n\n\n\n\nscale\n: the scale and tuning used - scales can be listed with \nScale.directory\n and tunings with \nTuning.directory\n (default \nScale.major(\\et12)\n)\n\n\ndegree\n: the degree of the scale to be played (default \n0\n)\n\n\noctave\n: the octave of the scale to be played (default \n5\n)\n\n\n\n\nOnly one of these arguments needs to be specified to be in 'scale mode', for example:\n\n\n//run up the major scale\n~sinfb = Pbind(\\instrument,\\sinfb,\\dur,0.25,\\degree,Pseq((0..7),inf));\n\n\n\n\nBut using all three gives full control over the parameters of the pitch used inside of a musical scale\n\n\n//run up and down chromatic scale one degree at a time\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.chromatic(\\et12),\\degree,Pseq((0..12).pyramid.mirror,inf),\\octave,6,\\dur,0.125/2,\\amp,0.3,\\fb,0.8,\\rel,0.1)\n\n\n\n\nBy using the 'scale mode' of Pbinds you can easily adopt pitch structures that are organised around any scale and tuning you wish - SuperCollider has a bunch bundled in, but way more can be added with the Scala Scale library through quarks such as \nTuningLib\n and \nTuningTheory\n, and arbitrary scales can be specified.\n\n\nWhy I don't use Pdefs\n\n\nAnother approach to using patterns is to make metapatterns by placing Pbinds (and Pmonos) inside of a \nPdef\n, but i've found this to be too verbose to use while performing, and i've personally had some problems getting them to sync for performances that reply on strict metric patterns.\n\n\nMore on Patterns\n\n\nPatterns form a huge part of my live sets, so I will be referencing them frequently throughout this repo, talking about their use in both rhythmic and melodic arrangement.", 
            "title": "Pbinds and Patterns: The Basics"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#pbinds-and-patterns", 
            "text": "", 
            "title": "Pbinds and Patterns"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#introduction", 
            "text": "According to SuperCollider's  Practical Guide to Patterns   Patterns describe calculations without explicitly stating every step. They are a higher-level representation of a computational task. While patterns are not ideally suited for every type of calculation, when they are appropriate they free the user from worrying about every detail of the process. Using patterns, one writes what is supposed to happen, rather than how to accomplish it.   A large part of my live coding performances involve using patterns, specifically  Pbinds  as Proxies inside of ProxySpace (see section 2.2.1) to create rhythmic elements that are synchronised to ProxySpace's  TempoClock . In this case, 'rhythmic elements' can mean percussion, melody, bass, pads, or generally anything that is played 'in tempo'.", 
            "title": "Introduction"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#synthdefs-arguments-and-pbinds", 
            "text": "The Pbinds I perform with work in conjunction with a set of  SynthDefs  (these can be found in the SynthDefs.scd file of the Setup folder) which serve various musical purposes, and plays them with specifies arguments at a given duration. This isn't a particularly intuitive concept to explain, but an example can help illustrate how this works. Take the SynthDef I use the most,  bplay :  SynthDef(\\bplay,\n    {arg out = 0, buf = 0, rate = 1, amp = 0.5, pan = 0, pos = 0, rel=15;\n        var sig,env ;\n        sig = Pan2.ar(PlayBuf.ar(2,buf,BufRateScale.ir(buf) * rate,1,BufDur.kr(buf)*pos*44100,doneAction:2),pan);\n        env = EnvGen.ar(Env.linen(0.0,rel,0),doneAction:2);\n        sig = sig * env;\n        sig = sig * amp;\n        Out.ar(out,sig);\n}).add;  bplay  is a simple stereo-panned sample player driven by the  PlayBuf  class, which takes the following arguments:   out : the bus to be played to (this is needed as an argument for the SynthDef to work correctly inside ProxySpace, and I don't usually touch it  buf : the buffer to be read by the synth (all of which are loaded into dictionary  d  by default  rate : the speed the sample will be played at (with no compensation for pitch)  amp : how loud the sample is, with 1 being the original volume of the sample  pan : where the sound is placed in the stereo field, with  0  being centre  pos : the position from which the sample starts playing, normalised from  0  to  1 , e.g. a value of  0.5  will play the sample from the middle   rel : in this case specifies how long the server is allowed to keep the instance open before freeing it. Normally the instance will be freed when the sample is finished playing, but in the case of very long samples or samples played backwards this freeing may not occur, leading to server load building in the background due to dead running processes. This default value of 15    Pbinds also have some arguments that need satisfying:   instrument : the SynthDef that will be used to deliver this 'instance' in the pattern  dur : The duration of each 'instance', if used directly in ProxySpace, a  dur  value of 1 results in an 'instance' once every clock cycle. \nNote: the default  dur  value of a Pbind is 1, and the default  instrument  value is SuperCollider's built in Piano synth, but specifying both anyway (especially  instrument ) is good practice.   So, if I wanted to have a kick drum playing once each beat in time with the ProxySpace timer, after I had run my setup file I would do the following:  (\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1);\n~k.play;\n)  This Pbind  ~k , spefifies that the instrument it will be using is  bplay , the buffer  bplay  reads from will be the first index of the  k  entry in the dictionary (which contains kick drums), and that the  dur /duration is  1 , once per cycle. If any arguments that the SynthDef takes are not specified as part of the Pbind, the SynthDef's default values will be used. Pbind arguments have to be given as key-value pairs, anything else will result in a syntax error, eg:  ~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\rate);  As part of these key-value pairs, Pbinds can take Pattern classes as inputs.  Pwhite  gives random values between a minimum and maximum. If I wanted to specify a random pitch of the kick drum, I could add this to the pattern:  ~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\rate,Pwhite(1,1.2));", 
            "title": "SynthDefs, Arguments and Pbinds"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#nesting-pattern-classes", 
            "text": "Pattern classes can also be  nested . Here are a few examples of some more complex percussive patterns. Once you start nesting pattern classes, things can get complicated quite quickly.  //to play with these examples, make sure the Setup File has been run\n\n//footwork kickdrums\n(\np.clock.tempo = 2.4;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,Pbjorklund2(Pseq([3,3,3,5],inf),8)/4,\\amp,1,\\rate,Pseq([1,1.2],inf));\n~k.play;\n)\n\n//skittery hi-hats\n(\np.clock.tempo = 1.5;\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pwrand([0.25,Pseq([0.125],2),0.5,Pseq([0.125/2],4)],[4,1,1,0.5].normalizeSum,inf),\\amp,Pwhite(0.2,1));\n~h.play;\n)\n\n//offset percussion patterns for techno feel behind a basic kick\n(\np.clock.tempo = 135/60;\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ sfx ][6],\\dur,Pbjorklund2(Pexprand(2,15).round(1),16,inf,Pwhite(1,5).asStream)/4,\\amp,1,\\rate,2.2);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ sfx ][6],\\dur,Pbjorklund2(Pexprand(2,15).round(1),16,inf,Pwhite(1,5).asStream)/4,\\amp,1,\\rate,1.9);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ sk ][0],\\dur,1,\\amp,5);\n~c.play;\n~c2.play;\n~k.play;\n)\n\n//snare running forwards and back\n(\np.clock.tempo = 150/60;\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][4],\\dur,Pwhite(1,4)/2,\\amp,1,\\rate,Prand([1,-1],inf),\\pos,Pkey(\\rate).linlin(-2,2,0.9,0));\n~sn.play;\n)", 
            "title": "Nesting pattern classes"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#extra-arguments-for-melodypitch", 
            "text": "Pbinds also have some additional trickery for anything involving pitch.  Let's look at the  sinfb  SynthDef (the arguments are listed in the code block for simplicity)  //SinFB Bass\n(\nSynthDef(\\sinfb, {\n    arg freq = 440, atk = 0.01, sus = 0, rel = 1, fb = 0, amp = 0.3, out = 0, pan=0;\n    var sig, env;\n    env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,1,2);\n    sig = SinOscFB.ar(freq,fb,1);\n    sig = sig*env;\n    Out.ar(out,Pan2.ar(sig,pan,amp));\n}).add;\n);\n/*\nfreq: frequency\natk: attack\nsus: sustain\nrel: release\nfb: phase feedback\namp: amplitude\nout: output bus\npan: stereo panning\n*/  Here, the  freq  argument is the pitch of the oscillator. Pitch can be specified manually, like so:  ~sinfb = Pbind(\\instrument,\\sinfb,\\dur,0.25,\\freq,Pwhite(100,900));  However, if a variable in a SynthDef is given the name  freq , Pbind allows the specification of the following in place of  freq  to activate a 'scale mode':   scale : the scale and tuning used - scales can be listed with  Scale.directory  and tunings with  Tuning.directory  (default  Scale.major(\\et12) )  degree : the degree of the scale to be played (default  0 )  octave : the octave of the scale to be played (default  5 )   Only one of these arguments needs to be specified to be in 'scale mode', for example:  //run up the major scale\n~sinfb = Pbind(\\instrument,\\sinfb,\\dur,0.25,\\degree,Pseq((0..7),inf));  But using all three gives full control over the parameters of the pitch used inside of a musical scale  //run up and down chromatic scale one degree at a time\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.chromatic(\\et12),\\degree,Pseq((0..12).pyramid.mirror,inf),\\octave,6,\\dur,0.125/2,\\amp,0.3,\\fb,0.8,\\rel,0.1)  By using the 'scale mode' of Pbinds you can easily adopt pitch structures that are organised around any scale and tuning you wish - SuperCollider has a bunch bundled in, but way more can be added with the Scala Scale library through quarks such as  TuningLib  and  TuningTheory , and arbitrary scales can be specified.", 
            "title": "Extra arguments for melody/pitch"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#why-i-dont-use-pdefs", 
            "text": "Another approach to using patterns is to make metapatterns by placing Pbinds (and Pmonos) inside of a  Pdef , but i've found this to be too verbose to use while performing, and i've personally had some problems getting them to sync for performances that reply on strict metric patterns.", 
            "title": "Why I don't use Pdefs"
        }, 
        {
            "location": "/2-4-Pbinds-and-Patterns/#more-on-patterns", 
            "text": "Patterns form a huge part of my live sets, so I will be referencing them frequently throughout this repo, talking about their use in both rhythmic and melodic arrangement.", 
            "title": "More on Patterns"
        }, 
        {
            "location": "/3-1-Rhythmic-Construction-For-Algorave-Sets/", 
            "text": "Introduction: Rhythmic Construction for Algorave Sets\n\n\n\n\nContext\n\n\nIn this document I'm going to talk about how I construct some basic rhythms for Algorave sets, but as my perception of rhythm is heavily influenced by the music I listen to, it's probably useful to list some of my influences to give some backdrop to the kinds of reference points I have when making repetitive electronic music designed for use in a dancefloor environment. These may or may not have manifested themselves at any point in any music I have played ever.\n\n\n\n\nBasic Channel\n \n\n\nDJ Rashad\n\n\nHolly Herndon\n\n\nsome of Mark Fell's performances\n (although the 'disinterested' performance aesthetic \nreally\n doesn't do it for me)\n\n\nmobilegirl's mixes\n\n\n'Dark DnB'\n\n\nSkepta \n Grime\n\n\nDrill \n Trap\n (for example a beat by Young Chop)\n\n\n\n\nConceptualising rhythm in live coding with SuperCollider\n\n\nA problem I had with rhythm when I first started live coding was how to manage rhythm was the lack of 'cycles' or 'loops'. In DAW environments this is handled by the entire environment being organised around the \ntime signature\n, and in TidalCycles it is handled by the whole musical language being structures around cycles.\n\n\nUsing Pbinds in SuperCollider, rhythms are specified on an individual basis using numerical representations of durations. I found this problematic as a 'natural' rhythmic progression was lacking, as well as any recognition of a 'time signature'.\n\n\nAs a result of this, I initially found creating rhythms faithful to dance music traditions quite difficult when live coding. Consider \nthis track\n by minimal techno legend Floorplan, AKA Robert Hood. The rhythms used here are very 'locked-in' to particular parts of a 4/4 groove in order to create a set of sounds that are idiomatically in-tune with Floorplan's particular sound, situated within the tradition of dance music he is creating. I have seen a number of these performances delivered using devices such as a \nElectribes\n, which are designed to place particular notes within a grid designed around a 4/4 groove. This approach to rhythm makes sense when designing dance music rhythms as specific rhythmic onsets have to be placed with reasonable precision in order to deliver a groove that is recognisably 'dance music'.\n\n\nWith SuperCollider however, I had to work out a way to specify these rhythms on a per-note basis, which required a bit of thought. It's quite difficult to delivery idiomatic grooves as whole units because they depend on the alignment of a number of drum sounds in particular configurations, with some onsets often falling at parts of a bar that are difficult to specify using \ndur\n values within Pbinds.\n\n\nWhile this is a setback in the instant production of very specific rhythmic units, it has allowed me to develop a more algorithmic approach to rhythm. In a live coding performance I draw upon a set of strategies that deliver rhythms reminiscent of particular aspects of dance music which I will detail in this section, and when appropriately applied these techniques yield grooves that are (to my ears and body) very dance-oriented in their construction -  take for example \nthis rehearsal excerpt\n. The advantage of using Pbinds for rhythm is that the aforementioned strategies can easily be modified to include extended algorithmic elements to extend or modify their functionality. If I want a kick drum that plays every beat 80% of the time, and plays a more complex rhythmic pattern 20% of the time, it is trivial to change:\n\n\n\\dur,1\n\n\n\n\nto\n\n\n\\dur,Pwrand([1,Pbjorklund2(5,16,1)/4],[0.8,0.2],inf)\n\n\n\n\nWith these kinds of techniques I can create dance music rhythms that algorithmically manage their own repetition (or lack thereof) to create variation in individual parts, which form grooves exhibiting a compound complexity from many small variations.\n\n\nI am still working on this, and I still find rhythmic complexity a difficult thing to establish in SuperCollider, especially when considering higher-level structures, or constructing rhythms in compound time signatures. This section will serve as a documentation of my continuing journey through making dance music with SuperCollider, with the intention that people will move their bodies to it in whatever way they see fit. \n\n\nDrum Samples\n\n\nAnother fundamentally important part of my approach to rhythm live coding is the samples that I use. I (for the most part) use drum samples for percussion of any kind for the simple reason that \nall of the hard work has already been done, and done well\n. I could synthesise my own percussion, but I'm not the greatest as synthesis and my results would probably sound lacklustre at best. If I use samples that have already been recorded, normalised (and possibly mastered) then I can be reasonably sure that they will penetrate a mix - and in adjusting their parameters I can be reasonably sure of their operation. If I used synthesised percussion it might oddly break under certain circumstances, or not cut through a mix for instance. The other advantage of using samples is that their impact on CPU use is reasonably small.\n\n\nSamples can also give a lot of context to the kinds of sounds that I'm attempting to emulate through performance. For instance, a set of 808 sounds will allow for the kind of 'rattling' hi-hat sounds common to trap and hip-hop, or distorted kicks will make it easy to draw on some gabber at some point.\n\n\nIn using samples I can also store a number of different 'types' of each sound, for instance sub kicks, harder kicks, softer kicks, distorted kicks and pitched kicks. I haven't quite figured out the best system both to store and categorise these samples for use in performance, but I'm getting there.", 
            "title": "Rhythmic Construction for Algorave Sets"
        }, 
        {
            "location": "/3-1-Rhythmic-Construction-For-Algorave-Sets/#introduction-rhythmic-construction-for-algorave-sets", 
            "text": "", 
            "title": "Introduction: Rhythmic Construction for Algorave Sets"
        }, 
        {
            "location": "/3-1-Rhythmic-Construction-For-Algorave-Sets/#context", 
            "text": "In this document I'm going to talk about how I construct some basic rhythms for Algorave sets, but as my perception of rhythm is heavily influenced by the music I listen to, it's probably useful to list some of my influences to give some backdrop to the kinds of reference points I have when making repetitive electronic music designed for use in a dancefloor environment. These may or may not have manifested themselves at any point in any music I have played ever.   Basic Channel    DJ Rashad  Holly Herndon  some of Mark Fell's performances  (although the 'disinterested' performance aesthetic  really  doesn't do it for me)  mobilegirl's mixes  'Dark DnB'  Skepta   Grime  Drill   Trap  (for example a beat by Young Chop)", 
            "title": "Context"
        }, 
        {
            "location": "/3-1-Rhythmic-Construction-For-Algorave-Sets/#conceptualising-rhythm-in-live-coding-with-supercollider", 
            "text": "A problem I had with rhythm when I first started live coding was how to manage rhythm was the lack of 'cycles' or 'loops'. In DAW environments this is handled by the entire environment being organised around the  time signature , and in TidalCycles it is handled by the whole musical language being structures around cycles.  Using Pbinds in SuperCollider, rhythms are specified on an individual basis using numerical representations of durations. I found this problematic as a 'natural' rhythmic progression was lacking, as well as any recognition of a 'time signature'.  As a result of this, I initially found creating rhythms faithful to dance music traditions quite difficult when live coding. Consider  this track  by minimal techno legend Floorplan, AKA Robert Hood. The rhythms used here are very 'locked-in' to particular parts of a 4/4 groove in order to create a set of sounds that are idiomatically in-tune with Floorplan's particular sound, situated within the tradition of dance music he is creating. I have seen a number of these performances delivered using devices such as a  Electribes , which are designed to place particular notes within a grid designed around a 4/4 groove. This approach to rhythm makes sense when designing dance music rhythms as specific rhythmic onsets have to be placed with reasonable precision in order to deliver a groove that is recognisably 'dance music'.  With SuperCollider however, I had to work out a way to specify these rhythms on a per-note basis, which required a bit of thought. It's quite difficult to delivery idiomatic grooves as whole units because they depend on the alignment of a number of drum sounds in particular configurations, with some onsets often falling at parts of a bar that are difficult to specify using  dur  values within Pbinds.  While this is a setback in the instant production of very specific rhythmic units, it has allowed me to develop a more algorithmic approach to rhythm. In a live coding performance I draw upon a set of strategies that deliver rhythms reminiscent of particular aspects of dance music which I will detail in this section, and when appropriately applied these techniques yield grooves that are (to my ears and body) very dance-oriented in their construction -  take for example  this rehearsal excerpt . The advantage of using Pbinds for rhythm is that the aforementioned strategies can easily be modified to include extended algorithmic elements to extend or modify their functionality. If I want a kick drum that plays every beat 80% of the time, and plays a more complex rhythmic pattern 20% of the time, it is trivial to change:  \\dur,1  to  \\dur,Pwrand([1,Pbjorklund2(5,16,1)/4],[0.8,0.2],inf)  With these kinds of techniques I can create dance music rhythms that algorithmically manage their own repetition (or lack thereof) to create variation in individual parts, which form grooves exhibiting a compound complexity from many small variations.  I am still working on this, and I still find rhythmic complexity a difficult thing to establish in SuperCollider, especially when considering higher-level structures, or constructing rhythms in compound time signatures. This section will serve as a documentation of my continuing journey through making dance music with SuperCollider, with the intention that people will move their bodies to it in whatever way they see fit.", 
            "title": "Conceptualising rhythm in live coding with SuperCollider"
        }, 
        {
            "location": "/3-1-Rhythmic-Construction-For-Algorave-Sets/#drum-samples", 
            "text": "Another fundamentally important part of my approach to rhythm live coding is the samples that I use. I (for the most part) use drum samples for percussion of any kind for the simple reason that  all of the hard work has already been done, and done well . I could synthesise my own percussion, but I'm not the greatest as synthesis and my results would probably sound lacklustre at best. If I use samples that have already been recorded, normalised (and possibly mastered) then I can be reasonably sure that they will penetrate a mix - and in adjusting their parameters I can be reasonably sure of their operation. If I used synthesised percussion it might oddly break under certain circumstances, or not cut through a mix for instance. The other advantage of using samples is that their impact on CPU use is reasonably small.  Samples can also give a lot of context to the kinds of sounds that I'm attempting to emulate through performance. For instance, a set of 808 sounds will allow for the kind of 'rattling' hi-hat sounds common to trap and hip-hop, or distorted kicks will make it easy to draw on some gabber at some point.  In using samples I can also store a number of different 'types' of each sound, for instance sub kicks, harder kicks, softer kicks, distorted kicks and pitched kicks. I haven't quite figured out the best system both to store and categorise these samples for use in performance, but I'm getting there.", 
            "title": "Drum Samples"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/", 
            "text": "Basic Rhythms\n\n\n\n\nThis document will be a list of some basic rhythmic techniques that are designed to deliver a simple, solid rhythmic base. Strategies for modifying these simple units will be detailed in the following document. \n\n\nPreamble: How to construct rhythms\n\n\nAccording to the \nPbind docs\n, duration using Pbinds are determined using the following:\n\n\n\n\ndelta\n  The time until the next event. Generally determined by:\n  dur\n      The time until next event in a sequence of events\n  stretch\n      Scales event timings (i.e. stretch == 2 =\n durations are twice as long)\n\n\n\n\nI generally use \n\\dur\n for basic rhythms, and when Pbinds are placed directly within ProxySpace, the \n\\dur\n argument is in sync with the ProxySpace \nTempoClock\n, which is specified in \nSetup.scd\n. This automatic synchronisation is very handy for keeping all of your rhythms running to the same tempo.\n\n\nAs stated in 3.1, for the most part all percussion I use will be sample-based. For playing samples using Pbinds, I have written two simple \nSynthDefs\n (which can be found in the \nSynthDefs.scd\n file in setup) - \nbplay\n and \nvplay\n. \nbplay\n is a simple buffer player that takes the following arguments:\n\n\n\n\nout\n: the bus to be played to (this is needed as an argument for the SynthDef to work correctly inside ProxySpace, and I don't usually touch it\n\n\nbuf\n: the buffer to be read by the synth (all of which are loaded into dictionary \nd\n by default\n\n\nrate\n: the speed the sample will be played at (with no compensation for pitch)\n\n\namp\n: how loud the sample is, with 1 being the original volume of the sample\n\n\npan\n: where the sound is placed in the stereo field, with \n0\n being centre\n\n\npos\n: the position from which the sample starts playing, normalised from \n0\n to \n1\n, e.g. a value of \n0.5\n will play the sample from the middle\n\n\nrel\n: in this case specifies how long the server is allowed to keep the instance open before freeing it. Normally the instance will be freed when the sample is finished playing, but in the case of very long samples or samples played backwards this freeing may not occur, leading to server load building in the background due to dead running processes. This default value of 15\n\n\n\n\nbplay\n is a general purpose sample player, which is designed for playing back percussive sounds. It is by far my most used SynthDef, and will almost inevitably be used to build the percussion in my sets.\n\n\nvplay\n also takes another argument:\n\n\n\n\nrel1\n: controls the amount of a sample played\n\n\n\n\nwhich is for playing specific sections of a sample, or to create particular effects by cutting the playing of percussive samples short.\n\n\n'The' kick\n\n\nThe iconic sound of a 4/4 kickdrum is probably a good starting point. A \ndur\n of 1 will play a kick on every beat of the clock\n\nStored as snippet \nkick\n\n\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~k.play;\n\n\n\n\nAlternate-beat snare\n\n\nPlayed alongside the 4/4 kick, a snare on every other beat\n\nStored as snippet \nsnare\n\n\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,2,\\amp,1);\n~sn.play;\n\n\n\n\nbasic hi-hat pattern\n\n\nQuarter-note closed hi-hats with random amplitude, good for fleshing out basic rhythms\n\nStored as snippet \nhat\n\n\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,0.25,\\amp,Pwhite(0.25,1);\n~h.play\n\n\n\n\n3/4 note clap\n\n\nA clap every 0.75 beat. When played against the rhythms above will add a nice polyryhthmic feel\n\nStored as snippet \nclap\n\n\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,0.75,\\amp,1);\n~c.play;\n\n\n\n\noff-beat open hi-hat\n\n\nAn open hi-hat played every beat, offset every half. I use it alongside straight kicks for a kick-hat-kick-hat pattern. The sample here also sounds \nreally\n good if it's switched out for some vocal chants. Note the \ndur\n uses an infinite Pbind inside of another Pbind to offset a regular pattern - this is a complexity of offsetting rhythms in SuperCollider, and is one of the only instances in which I currently do it.\n\nStored as snippet \noh\n.\n\nThe offset \ndur\n is stored as snippet \noffbeat\n\n\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][0],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1);\n~oh.play;", 
            "title": "Basic Rhythms"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#basic-rhythms", 
            "text": "This document will be a list of some basic rhythmic techniques that are designed to deliver a simple, solid rhythmic base. Strategies for modifying these simple units will be detailed in the following document.", 
            "title": "Basic Rhythms"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#preamble-how-to-construct-rhythms", 
            "text": "According to the  Pbind docs , duration using Pbinds are determined using the following:   delta\n  The time until the next event. Generally determined by:\n  dur\n      The time until next event in a sequence of events\n  stretch\n      Scales event timings (i.e. stretch == 2 =  durations are twice as long)   I generally use  \\dur  for basic rhythms, and when Pbinds are placed directly within ProxySpace, the  \\dur  argument is in sync with the ProxySpace  TempoClock , which is specified in  Setup.scd . This automatic synchronisation is very handy for keeping all of your rhythms running to the same tempo.  As stated in 3.1, for the most part all percussion I use will be sample-based. For playing samples using Pbinds, I have written two simple  SynthDefs  (which can be found in the  SynthDefs.scd  file in setup) -  bplay  and  vplay .  bplay  is a simple buffer player that takes the following arguments:   out : the bus to be played to (this is needed as an argument for the SynthDef to work correctly inside ProxySpace, and I don't usually touch it  buf : the buffer to be read by the synth (all of which are loaded into dictionary  d  by default  rate : the speed the sample will be played at (with no compensation for pitch)  amp : how loud the sample is, with 1 being the original volume of the sample  pan : where the sound is placed in the stereo field, with  0  being centre  pos : the position from which the sample starts playing, normalised from  0  to  1 , e.g. a value of  0.5  will play the sample from the middle  rel : in this case specifies how long the server is allowed to keep the instance open before freeing it. Normally the instance will be freed when the sample is finished playing, but in the case of very long samples or samples played backwards this freeing may not occur, leading to server load building in the background due to dead running processes. This default value of 15   bplay  is a general purpose sample player, which is designed for playing back percussive sounds. It is by far my most used SynthDef, and will almost inevitably be used to build the percussion in my sets.  vplay  also takes another argument:   rel1 : controls the amount of a sample played   which is for playing specific sections of a sample, or to create particular effects by cutting the playing of percussive samples short.", 
            "title": "Preamble: How to construct rhythms"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#the-kick", 
            "text": "The iconic sound of a 4/4 kickdrum is probably a good starting point. A  dur  of 1 will play a kick on every beat of the clock \nStored as snippet  kick  ~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~k.play;", 
            "title": "'The' kick"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#alternate-beat-snare", 
            "text": "Played alongside the 4/4 kick, a snare on every other beat \nStored as snippet  snare  ~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,2,\\amp,1);\n~sn.play;", 
            "title": "Alternate-beat snare"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#basic-hi-hat-pattern", 
            "text": "Quarter-note closed hi-hats with random amplitude, good for fleshing out basic rhythms \nStored as snippet  hat  ~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,0.25,\\amp,Pwhite(0.25,1);\n~h.play", 
            "title": "basic hi-hat pattern"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#34-note-clap", 
            "text": "A clap every 0.75 beat. When played against the rhythms above will add a nice polyryhthmic feel \nStored as snippet  clap  ~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,0.75,\\amp,1);\n~c.play;", 
            "title": "3/4 note clap"
        }, 
        {
            "location": "/3-2-Basic-Rhythms/#off-beat-open-hi-hat", 
            "text": "An open hi-hat played every beat, offset every half. I use it alongside straight kicks for a kick-hat-kick-hat pattern. The sample here also sounds  really  good if it's switched out for some vocal chants. Note the  dur  uses an infinite Pbind inside of another Pbind to offset a regular pattern - this is a complexity of offsetting rhythms in SuperCollider, and is one of the only instances in which I currently do it. \nStored as snippet  oh . \nThe offset  dur  is stored as snippet  offbeat  ~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][0],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1);\n~oh.play;", 
            "title": "off-beat open hi-hat"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/", 
            "text": "Techniques for Modifying Rhythm\n\n\n\n\nIn 3.2 I went over a few basic rhythmic units for some simple dance music rhythms, here I will elaborate on a few of the more simple techniques I use to get a bit of complexity in my rhythms.\n\n\nWhy I don't use (total) randomness\n\n\nThe \nPwhite\n class is a great way to incorporate randomness into patterns, and one of the first things I tried to do when adding complexity to rhythms was to simply randomise them, however the results were often quite disappointing, especially with multiple random rhythms played at once for sounds that are played regularly (i.e. snares, hats):\n\n\n//Random rhythm with Pwhite\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,Pwhite(1,5.0),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pwhite(0.25,0.75),\\amp,Pwhite(0.2,1));\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,Pwhite(0.75,2),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pwhite(1,5.0),\\amp,1);\n~sn.play;~h.play;~c.play;~t.play;\n)\n//even with a regular kickdrum the other rhythms don't sound good\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~k.play\n)\n\n\n\n\nWith rhythms that use random floating point numbers, the durations that are used have no relationship to any central pulse, and will end up cutting across the beat a lot of the time in a way that I feel does not make sense in dance music. Instead, randomness can be incorporated within various techniques (for a great example see the way that Pwhite is used in the section on Euclidean Rhythms), or constrained to fit within a more regular pattern by using methods such as .round (which can be found in the \nPattern Documentation\n).\n\n\nHere is an example of using methods to constrain Pwhite into a form that is more palatable:\n\n\n//same example but with all rhythms constrained\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,Pwhite(1,5.0).round(1),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pwhite(0.25,0.75).round(0.25),\\amp,Pwhite(0.2,1));\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,Pwhite(0.75,2).round(0.75),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pwhite(1,5.0).round(0.5),\\amp,1);\n~sn.play;~h.play;~c.play;~t.play;\n)\n//sounds more palatable with everything arranged properly\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~k.play;\n)\n\n\n\n\nPwhite also only gives floating point results if one of the values specified is a floating point number, so for quick whole-beat durations (especially useful for occasional longer sounds) I use Pwhite to generate whole beats:\n\n\n//same example again\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,Pwhite(1,5.0).round(1),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pwhite(0.25,0.75).round(0.25),\\amp,Pwhite(0.2,1));\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,Pwhite(0.75,2).round(0.75),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pwhite(1,5.0).round(0.5),\\amp,1);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~sn.play;~h.play;~c.play;~t.play;~k.play;\n)\n//added whole note fx, short, medium and long.\n(\n~fx1 = Pbind(\\instrument,\\bplay,\\buf,d[\nsfx\n][0],\\dur,Pwhite(1,5),\\amp,1);\n~fx2 = Pbind(\\instrument,\\bplay,\\buf,d[\nfx\n][0],\\dur,Pwhite(1,10),\\amp,1);\n~fx3 = Pbind(\\instrument,\\bplay,\\buf,d[\nlfx\n][0],\\dur,Pwhite(10,40),\\amp,1);\n~fx1.play;~fx2.play;~fx3.play;\n)\n\n\n\n\nLayering\n\n\nSome great advice I received from a lecturer was 'if one of them is good, lots of them will be great' (paraphrased), when talking about the work of \nzimoun\n. This works really well when applied to rhythmic percussion, particularly if each layer of similar percussion serves to re-contextualise the last.\n\n\nWhen I'm layering rhythms, there are generally a few techniques I employ to make doing so 'work', or just to sound better:\n\n\n\n\nLayer at different pitches:\n\n\n\n\n//layering at different pitches - kicks\n(\np.clock.tempo = 2.3;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1,1.2],inf));\n~k.play;\n)\n//kicks at a different pitch. Evaluate this a few times to get different permutations\n(\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1,1.8],inf)*4);\n~k2.play;\n)\n\n\n\n\n\n\nLayer very slightly different rhythms, rhythmic units of different lengths\n\n\n\n\n//layering of slightly different rhythms\n//rhythm 1\n(\np.clock.tempo = 1.7;\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pseq([1,1,1,0.5],inf),\\amp,1);\n~t.play;\n)\n//rhythm 2, using a different tom for contrast\n//also re-evaluating rhythm 1 to get them playing together\n(\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pseq([1,1,1,0.5],inf),\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][1],\\dur,Pseq([1,1,1,0.25],inf),\\amp,1);\n~t2.play;\n)\n//rhythm 3 for more\n(\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pseq([1,1,1,0.5],inf),\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][1],\\dur,Pseq([1,1,1,0.25],inf),\\amp,1);\n~t3 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][2],\\dur,Pseq([1,1,1,0.75],inf),\\amp,1);\n~t3.play;\n)\n//kick underneath to illustrate\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1,\\rate,1);\n~oh.play;\n~k.play;\n)\n\n\n\n\n\n\nLayer interlocking or complimentary rhythms\n\n\n\n\n//complimentary rhythms:\n//the 'polyrhythmic clap' from the Basics example\n(\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,0.75,\\amp,1);\n~c.play;\n)\n//clap added at a similar rhythm (euclidean 3,8)\n(\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1);\n~c2.play;\n)\n\n\n\n\n\n\nLink with StageLimiter to establish rhythms underneath other ones (more on this in the StageLimiter Abuse section)\n\n\n\n\n//StageLimiter throttling\n//a complex rhythm\n(\nl = Prewrite(1, // start with 1\n        (    1: [0.25,2],\n        0.25: [1,0.75,0.1,0.3,0.6,0.1],\n        0.1: [0.5,1,2],\n        2: [0.5,0.75,0.5,1]\n        ), 4);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,l/2,\\amp,1,\\rate,2);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l*2,\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,l,\\amp,1,\\rate,Pseq([1.2,1.4,1.7],inf));\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,l*4,\\amp,1,\\rate,0.8);\n~ding = Pbind(\\instrument,\\bplay,\\buf,d[\nding\n][0],\\dur,Pwhite(1,5),\\amp,1,\\rate,0.2);\n~h.play;~c.play;~t.play;~ding.play;~sn.play;\n)\n//extremely loud kick throttles everything else\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,4,\\amp,100,\\rate,0.5);\n~k.play;\n)\n\n\n\n\nPwrand - Weighted distribution and hassle-free controlled randomness\n\n\nA technique that I started using after being inspired by Trap instrumentals (such as Ace Hood's \nBugatti\n) was hi-hats that snapped between 1/4, 1/8, 1/6 and 1/16th note patterns. The best way I found to do this was to use \nPwrand\n. Pwrand takes an array of items, and will select those items randomly within a weighted distribution, allowing control over the frequency of occurrence of particular elements.\n\n\nThe trap hi-hats looked like this:\n\n\n//trap(ish) hi-hats\n//Has a choice of four rhythmic patterns with lesser chance for each, results in a mostly 0.25-duration hat which can potentially go quite quickly\n(\np.clock.tempo = 75/60;\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pwrand([0.25,Pseq([0.125],4),Pseq([0.25]/3,3),Pseq([0.125]/2,4)],[0.6,0.3,0.09,0.01],inf),\\amp,1,\\rate,2);\n~h.play;\n)\n\n\n\n\nPwrand is great to use whenever you want to control the occurrence of particular types of rhythm without explicitly specifying an order for these types of rhythm to occur. A one I've used quite a lot is to inject some variety into kick drums by switching out a straight \ndur\n of 1 with other values\n\n\n//occasional variation on 4/4 kick\n(\np.clock.tempo = 2.3;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,Pwrand([1,Pseq([0.75],4),Pbjorklund2(3,8,1)/4],[0.9,0.08,0.02],inf),\\amp,1);\n~k.play;\n)\n//open hat for reference\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1,\\rate,1.4);\n~oh.play;\n)\n\n\n\n\nClipped percussion - stuttering\n\n\nThe SynthDef \nvplay\n is designed to deliver samples controlled by an envelope which by default is a square - it will abruptly start and stop sample playback according to envelope settings:\n\n\n//cutoff percussion. This Pbind uses (0..100)/100 to split the sample into 100 sections of 0.03 and play over them\n(\np.clock.tempo = 2.4;\n~perc = Pbind(\\instrument,\\vplay,\\buf,d[\nfx\n][1],\\rel,0.03,\\dur,0.25,\\pos,Pseq((0..100)/100,inf));\n~perc.play;\n)\n\n\n\n\nThis is a useful technique for creating sputtering rhythms out of much longer sound effects or samples, which can be chopped up on-the-fly and recombined around a central rhythm with \nvplay\n. This approach tends to yield interesting results by each individual sample playback taking on irregular characteristics even when played inside a regular rhythm - some complexity with pretty minimal effort:\n\n\n//sputtering rhythms based on long percussion sounds\n//the Prand for \\buf is a flattened array of all fx sounds. If it wasn't flat it would play all sounds from any fx entry all at once\n(\np.clock.tempo = 2.3;\n~perc = Pbind(\\instrument,\\vplay,\\buf,Prand([d[\nfx\n],d[\nsfx\n],d[\nlfx\n]].flat,inf),\\rel,0.1,\\dur,0.25,\\pos,Pwhite(0,0.9),\\rate,Pwhite(1,3.0));\n~perc.play;\n)\n//choose from literally every sample there is in d. Buggy because it'll also play anything else that is in there, but good for a laugh.\n(\n~perc = Pbind(\\instrument,\\vplay,\\buf,Prand(d.values,inf),\\rel,0.1,\\dur,0.25,\\pos,Pwhite(0.0,0.9),\\rate,Pwhite(1,3.0));\n~perc.play;\n)\n\n\n\n\nBack-and-forth - Pkey and linking values\n\n\nPkey\n is a pattern class used to embed the same value multiple times in the same pattern - for example if the release value of a SynthDef needed to be the same as the duration of the note:\n\n\nPbind(\\instrument,\\something,\\dur,Pseq([2,3,4,5],inf),\\rel,Pkey(\\dur));\n\n\n\n\nOne way to use this in rhythm is to create sample playback that flips back and forth. Due to how the \nbplay\n SynthDef works, if a buffer is to be played backwards it will need to be started just before the end of the sample as the Synth will release once the sample is finished (for more information see \nUgen done-actions\n). Using the \n.linlin\n linear scaling method this value can then be scaled into the rate of playback to create a back-and-forth pattern in percussion, shown here on a snare:\n\n\n//back-and-forth snare\n(\n~sn = Pbind(\\instrument,\\vplay,\\buf,d[\ns\n][0],\\dur,Pbjorklund2(Pwhite(1,6),16)/4,\\amp,1,\\rate,Prand([-1,1],inf),\\pos,Pkey(\\rate).linlin(-1,1,0.99,0));\n~sn.play;\n)\n\n\n\n\n.normalizeSum and 'keeping it on 1'\n\n\nSometimes greater granularity or oddities of rhythm are needed, but still within the confines of some kind of regularity. This can be achieved with the \n.normalizeSum\n method, which will take an array and normalize all of its elements so that they add up to 1, for example \n[10,20,30].normalizeSum\n will produce the array \n[ 0.16666666666667, 0.33333333333333, 0.5 ]\n. This can be used to create arrays inside of \nPseq\n that can easily create complex rhythmic bursts that still resolve around the central beat. Particularly useful here is to generate a sequential array and normalize it to create a rhythmic spread:\n\n\n//.normalizeSum rhythmic spread\n//spreading 1-20 over four beats\n(\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pseq((1..20).normalizeSum,inf)*4,\\amp,Pwhite(0.2,1));\n~h.play;\n)\n//spreading 1-200 over sixteen beats (gives overtone)\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pseq((1..200).normalizeSum,inf)*16,\\amp,Pwhite(0.2,1));\n//spreading 1-18 over 8 beats\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pseq((1..18).normalizeSum,inf)*8,\\amp,Pwhite(0.2,1));\n\n\n\n\n\\stretch\n\n\nAnother option for rhythmic variation is to use the \n\\stretch\n argument built in to \nPbind\n. This argument will multiply the \n\\dur\n argument by \n\\stretch\n to create a final duration which will be used in the pattern. I don't use this too much as it stands (April 2017), but it can be used very effectively\n\n\n//using the \\stretch argument - each time a cycle completes change the stretch duration\n//a fake argument is created here - \\euclidNum is used to inform both \\dur and \\stretch to ensure both work with the same number of onsets\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\euclidNum,Pwhite(1,7),\\dur,Pbjorklund2(Pkey(\\euclidNum),8)/4,\\amp,1,\\rate,Pseq([3,4,5],inf),\\stretch,Pseq([1,0.5,0.25,2],inf).stutter(Pkey(\\euclidNum).asStream));\n~k.play;", 
            "title": "Techniques for Modifying Rhythm"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#techniques-for-modifying-rhythm", 
            "text": "In 3.2 I went over a few basic rhythmic units for some simple dance music rhythms, here I will elaborate on a few of the more simple techniques I use to get a bit of complexity in my rhythms.", 
            "title": "Techniques for Modifying Rhythm"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#why-i-dont-use-total-randomness", 
            "text": "The  Pwhite  class is a great way to incorporate randomness into patterns, and one of the first things I tried to do when adding complexity to rhythms was to simply randomise them, however the results were often quite disappointing, especially with multiple random rhythms played at once for sounds that are played regularly (i.e. snares, hats):  //Random rhythm with Pwhite\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,Pwhite(1,5.0),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pwhite(0.25,0.75),\\amp,Pwhite(0.2,1));\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,Pwhite(0.75,2),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pwhite(1,5.0),\\amp,1);\n~sn.play;~h.play;~c.play;~t.play;\n)\n//even with a regular kickdrum the other rhythms don't sound good\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~k.play\n)  With rhythms that use random floating point numbers, the durations that are used have no relationship to any central pulse, and will end up cutting across the beat a lot of the time in a way that I feel does not make sense in dance music. Instead, randomness can be incorporated within various techniques (for a great example see the way that Pwhite is used in the section on Euclidean Rhythms), or constrained to fit within a more regular pattern by using methods such as .round (which can be found in the  Pattern Documentation ).  Here is an example of using methods to constrain Pwhite into a form that is more palatable:  //same example but with all rhythms constrained\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,Pwhite(1,5.0).round(1),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pwhite(0.25,0.75).round(0.25),\\amp,Pwhite(0.2,1));\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,Pwhite(0.75,2).round(0.75),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pwhite(1,5.0).round(0.5),\\amp,1);\n~sn.play;~h.play;~c.play;~t.play;\n)\n//sounds more palatable with everything arranged properly\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~k.play;\n)  Pwhite also only gives floating point results if one of the values specified is a floating point number, so for quick whole-beat durations (especially useful for occasional longer sounds) I use Pwhite to generate whole beats:  //same example again\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,Pwhite(1,5.0).round(1),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pwhite(0.25,0.75).round(0.25),\\amp,Pwhite(0.2,1));\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,Pwhite(0.75,2).round(0.75),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pwhite(1,5.0).round(0.5),\\amp,1);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~sn.play;~h.play;~c.play;~t.play;~k.play;\n)\n//added whole note fx, short, medium and long.\n(\n~fx1 = Pbind(\\instrument,\\bplay,\\buf,d[ sfx ][0],\\dur,Pwhite(1,5),\\amp,1);\n~fx2 = Pbind(\\instrument,\\bplay,\\buf,d[ fx ][0],\\dur,Pwhite(1,10),\\amp,1);\n~fx3 = Pbind(\\instrument,\\bplay,\\buf,d[ lfx ][0],\\dur,Pwhite(10,40),\\amp,1);\n~fx1.play;~fx2.play;~fx3.play;\n)", 
            "title": "Why I don't use (total) randomness"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#layering", 
            "text": "Some great advice I received from a lecturer was 'if one of them is good, lots of them will be great' (paraphrased), when talking about the work of  zimoun . This works really well when applied to rhythmic percussion, particularly if each layer of similar percussion serves to re-contextualise the last.  When I'm layering rhythms, there are generally a few techniques I employ to make doing so 'work', or just to sound better:   Layer at different pitches:   //layering at different pitches - kicks\n(\np.clock.tempo = 2.3;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1,1.2],inf));\n~k.play;\n)\n//kicks at a different pitch. Evaluate this a few times to get different permutations\n(\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1,1.8],inf)*4);\n~k2.play;\n)   Layer very slightly different rhythms, rhythmic units of different lengths   //layering of slightly different rhythms\n//rhythm 1\n(\np.clock.tempo = 1.7;\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pseq([1,1,1,0.5],inf),\\amp,1);\n~t.play;\n)\n//rhythm 2, using a different tom for contrast\n//also re-evaluating rhythm 1 to get them playing together\n(\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pseq([1,1,1,0.5],inf),\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][1],\\dur,Pseq([1,1,1,0.25],inf),\\amp,1);\n~t2.play;\n)\n//rhythm 3 for more\n(\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pseq([1,1,1,0.5],inf),\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][1],\\dur,Pseq([1,1,1,0.25],inf),\\amp,1);\n~t3 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][2],\\dur,Pseq([1,1,1,0.75],inf),\\amp,1);\n~t3.play;\n)\n//kick underneath to illustrate\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1,\\rate,1);\n~oh.play;\n~k.play;\n)   Layer interlocking or complimentary rhythms   //complimentary rhythms:\n//the 'polyrhythmic clap' from the Basics example\n(\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,0.75,\\amp,1);\n~c.play;\n)\n//clap added at a similar rhythm (euclidean 3,8)\n(\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1);\n~c2.play;\n)   Link with StageLimiter to establish rhythms underneath other ones (more on this in the StageLimiter Abuse section)   //StageLimiter throttling\n//a complex rhythm\n(\nl = Prewrite(1, // start with 1\n        (    1: [0.25,2],\n        0.25: [1,0.75,0.1,0.3,0.6,0.1],\n        0.1: [0.5,1,2],\n        2: [0.5,0.75,0.5,1]\n        ), 4);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,l/2,\\amp,1,\\rate,2);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l*2,\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,l,\\amp,1,\\rate,Pseq([1.2,1.4,1.7],inf));\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,l*4,\\amp,1,\\rate,0.8);\n~ding = Pbind(\\instrument,\\bplay,\\buf,d[ ding ][0],\\dur,Pwhite(1,5),\\amp,1,\\rate,0.2);\n~h.play;~c.play;~t.play;~ding.play;~sn.play;\n)\n//extremely loud kick throttles everything else\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,4,\\amp,100,\\rate,0.5);\n~k.play;\n)", 
            "title": "Layering"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#pwrand-weighted-distribution-and-hassle-free-controlled-randomness", 
            "text": "A technique that I started using after being inspired by Trap instrumentals (such as Ace Hood's  Bugatti ) was hi-hats that snapped between 1/4, 1/8, 1/6 and 1/16th note patterns. The best way I found to do this was to use  Pwrand . Pwrand takes an array of items, and will select those items randomly within a weighted distribution, allowing control over the frequency of occurrence of particular elements.  The trap hi-hats looked like this:  //trap(ish) hi-hats\n//Has a choice of four rhythmic patterns with lesser chance for each, results in a mostly 0.25-duration hat which can potentially go quite quickly\n(\np.clock.tempo = 75/60;\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pwrand([0.25,Pseq([0.125],4),Pseq([0.25]/3,3),Pseq([0.125]/2,4)],[0.6,0.3,0.09,0.01],inf),\\amp,1,\\rate,2);\n~h.play;\n)  Pwrand is great to use whenever you want to control the occurrence of particular types of rhythm without explicitly specifying an order for these types of rhythm to occur. A one I've used quite a lot is to inject some variety into kick drums by switching out a straight  dur  of 1 with other values  //occasional variation on 4/4 kick\n(\np.clock.tempo = 2.3;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,Pwrand([1,Pseq([0.75],4),Pbjorklund2(3,8,1)/4],[0.9,0.08,0.02],inf),\\amp,1);\n~k.play;\n)\n//open hat for reference\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1,\\rate,1.4);\n~oh.play;\n)", 
            "title": "Pwrand - Weighted distribution and hassle-free controlled randomness"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#clipped-percussion-stuttering", 
            "text": "The SynthDef  vplay  is designed to deliver samples controlled by an envelope which by default is a square - it will abruptly start and stop sample playback according to envelope settings:  //cutoff percussion. This Pbind uses (0..100)/100 to split the sample into 100 sections of 0.03 and play over them\n(\np.clock.tempo = 2.4;\n~perc = Pbind(\\instrument,\\vplay,\\buf,d[ fx ][1],\\rel,0.03,\\dur,0.25,\\pos,Pseq((0..100)/100,inf));\n~perc.play;\n)  This is a useful technique for creating sputtering rhythms out of much longer sound effects or samples, which can be chopped up on-the-fly and recombined around a central rhythm with  vplay . This approach tends to yield interesting results by each individual sample playback taking on irregular characteristics even when played inside a regular rhythm - some complexity with pretty minimal effort:  //sputtering rhythms based on long percussion sounds\n//the Prand for \\buf is a flattened array of all fx sounds. If it wasn't flat it would play all sounds from any fx entry all at once\n(\np.clock.tempo = 2.3;\n~perc = Pbind(\\instrument,\\vplay,\\buf,Prand([d[ fx ],d[ sfx ],d[ lfx ]].flat,inf),\\rel,0.1,\\dur,0.25,\\pos,Pwhite(0,0.9),\\rate,Pwhite(1,3.0));\n~perc.play;\n)\n//choose from literally every sample there is in d. Buggy because it'll also play anything else that is in there, but good for a laugh.\n(\n~perc = Pbind(\\instrument,\\vplay,\\buf,Prand(d.values,inf),\\rel,0.1,\\dur,0.25,\\pos,Pwhite(0.0,0.9),\\rate,Pwhite(1,3.0));\n~perc.play;\n)", 
            "title": "Clipped percussion - stuttering"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#back-and-forth-pkey-and-linking-values", 
            "text": "Pkey  is a pattern class used to embed the same value multiple times in the same pattern - for example if the release value of a SynthDef needed to be the same as the duration of the note:  Pbind(\\instrument,\\something,\\dur,Pseq([2,3,4,5],inf),\\rel,Pkey(\\dur));  One way to use this in rhythm is to create sample playback that flips back and forth. Due to how the  bplay  SynthDef works, if a buffer is to be played backwards it will need to be started just before the end of the sample as the Synth will release once the sample is finished (for more information see  Ugen done-actions ). Using the  .linlin  linear scaling method this value can then be scaled into the rate of playback to create a back-and-forth pattern in percussion, shown here on a snare:  //back-and-forth snare\n(\n~sn = Pbind(\\instrument,\\vplay,\\buf,d[ s ][0],\\dur,Pbjorklund2(Pwhite(1,6),16)/4,\\amp,1,\\rate,Prand([-1,1],inf),\\pos,Pkey(\\rate).linlin(-1,1,0.99,0));\n~sn.play;\n)", 
            "title": "Back-and-forth - Pkey and linking values"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#normalizesum-and-keeping-it-on-1", 
            "text": "Sometimes greater granularity or oddities of rhythm are needed, but still within the confines of some kind of regularity. This can be achieved with the  .normalizeSum  method, which will take an array and normalize all of its elements so that they add up to 1, for example  [10,20,30].normalizeSum  will produce the array  [ 0.16666666666667, 0.33333333333333, 0.5 ] . This can be used to create arrays inside of  Pseq  that can easily create complex rhythmic bursts that still resolve around the central beat. Particularly useful here is to generate a sequential array and normalize it to create a rhythmic spread:  //.normalizeSum rhythmic spread\n//spreading 1-20 over four beats\n(\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pseq((1..20).normalizeSum,inf)*4,\\amp,Pwhite(0.2,1));\n~h.play;\n)\n//spreading 1-200 over sixteen beats (gives overtone)\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pseq((1..200).normalizeSum,inf)*16,\\amp,Pwhite(0.2,1));\n//spreading 1-18 over 8 beats\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pseq((1..18).normalizeSum,inf)*8,\\amp,Pwhite(0.2,1));", 
            "title": ".normalizeSum and 'keeping it on 1'"
        }, 
        {
            "location": "/3-3-Techniques-For-Modifying-Rhythm/#stretch", 
            "text": "Another option for rhythmic variation is to use the  \\stretch  argument built in to  Pbind . This argument will multiply the  \\dur  argument by  \\stretch  to create a final duration which will be used in the pattern. I don't use this too much as it stands (April 2017), but it can be used very effectively  //using the \\stretch argument - each time a cycle completes change the stretch duration\n//a fake argument is created here - \\euclidNum is used to inform both \\dur and \\stretch to ensure both work with the same number of onsets\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\euclidNum,Pwhite(1,7),\\dur,Pbjorklund2(Pkey(\\euclidNum),8)/4,\\amp,1,\\rate,Pseq([3,4,5],inf),\\stretch,Pseq([1,0.5,0.25,2],inf).stutter(Pkey(\\euclidNum).asStream));\n~k.play;", 
            "title": "\\stretch"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/", 
            "text": "Euclidean Rhythms\n\n\n\n\nIntroduction\n\n\nEuclidean Rhythms are described in a 2005 paper by Godfried Toussaint entitled \n'The Euclidean Algorithm Generates Traditional Musical Rhythms'\n, which describes the organisation of rhythm by placing onsets as evenly as possible within a number of possible spaces using Bjorklund's algorithm. It's not the easiest thing to verbally describe, but \nthis online tool\n explains it much better, and the paper contains a bunch of illustrated examples.\n\n\nAs mentioned in 3.1, When I was learning how to perform Live Coding I found creating compelling, complex rhythm in SuperCollider quite hard. Euclidean Rhythms and the \nBjorklund quark\n have ended up becoming major fixtures of my performance as a result as they handle a lot of the difficulties i have around developing rhythmic complexity in real-time as part of performance. I've always wanted to be able to make rhythms like \nDJ Rashad\n, and using Euclidean Rhythms has got me some way on that quest.\n\n\nEffort-free rhythmic complexity\n\n\nThe problem I had with rhythm was in the fact that all rhythms for all proxies had to be specified as \ndur\n values, and each one had to be specified independently. Constructing TidalCycles-like 'riffs' containing multiple percussion samples is really quite hard in SuperCollider. As a result, most rhythms I ended up creating involved either using simple on-beat/off-beat patterns, or constraining a \nPwhite\n or \nPexprand\n into producing random rhythms in time with the ProxySpace tempo clock, and random rhythms with a uniform distribution generally sound quite boring.\n\n\nThe Bjorklund quark contains a few classes that help in using Euclidean Rhythms. I particularly use \nPbjorklund2\n, which takes arguments for:\n\n\n\n\nk\n: Number of 'hits'\n\n\nn\n: Number of possible onsets\n\n\nlength\n: Number of repeats\n\n\noffset\n: Starting onset in the pattern\n\n\n\n\nand using this, outputs an array of durations for use as \ndur\n values in a pattern, for instance: \nPbjorklund2(3,8)\n would produce duration arrays of \n[ 3, 3, 2 ]\n.\n\n\nBecause \nPbjorklund2\n is a pattern class, it can be nested and have its arguments modulated by other pattern classes, using its inputs to generate sequences, rather than single values. In this way, 'random rhythms' create a much more interesting result, as random values will be used to create a network of onsets, which perceptually appear to be very complex interlocking rhythms.\n\n\n//four 'randomised' rhythms, sounds okay.\n(\np.clock.tempo = 2.2;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][1],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][1],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][1],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~k.play;\n~sn.play;\n~h.play;\n~t.play;\n)\n\n//four randomised euclidean rhythms with four different samples.\n//sounds better, producing a much greater variety of rhythmic forms.\n(\np.clock.tempo = 2.2;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][1],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][1],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][1],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~k.play;\n~sn.play;\n~h.play;\n~t.play;\n)\n\n\n\n\nEuclidean Rhythms vs 'the beat'\n\n\nThe benefit of using the Bjorklund quark like this is that it also lines up with the regular clock of ProxySpace, allowing for scattered, hypercomplex, undanceable rhythms to be established over time, and then in one movement unified under a regular rhythm, such as a straight kick drum with a \ndur\n of a subdivision of 1.\n\n\nHere's an example that's sort-of inspired by the lasting impression that Basic Channel's \nPhylyps Trak\n made on me some time ago.\n\n\n//Complex rhythm that obfuscates the central rhythmic centre\n(\np.clock.tempo = 1.5;\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pbjorklund2(Pwhite(10,35),41,inf,Pwhite(0,10).asStream)/8,\\amp,Pexprand(0.1,1),\\pan,-1);\n~h2 = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pbjorklund2(Pwhite(10,35),40,inf,Pwhite(0,10).asStream)/8,\\amp,Pexprand(0.1,1),\\pan,1);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,Pbjorklund2(Pwhite(1,5),Pwhite(1,32))/4,\\amp,1,\\rate,Pwrand([1,-1],[0.8,0.2],inf),\\pos,Pkey(\\rate).linlin(1,-1,0,0.999));\n~ding = Pbind(\\instrument,\\bplay,\\buf,d[\nding\n][0],\\dur,Pbjorklund2(Pwhite(1,3),25)/4,\\amp,0.6,\\rate,0.6,\\pan,-1);\n~ding2 = Pbind(\\instrument,\\bplay,\\buf,d[\nding\n][0],\\dur,Pbjorklund2(Pwhite(1,3),20)/4,\\amp,0.6,\\rate,0.7,\\pan,1);\n~t1 = Pbind(\\instrument,\\bplay,\\buf,d[\nmt\n][0],\\dur,Pbjorklund2(Pseq([1,1,1,Pwhite(10,15,1).asStream],inf),36,inf,Pwhite(0,2).asStream)/8,\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(Pseq([1,1,1,Pwhite(10,15,1).asStream],inf),40,inf,Pwhite(0,2).asStream)/8,\\amp,1,\\rate,2);\n~t1.play;~t2.play;~h.play;~h2.play;~sn.play;~ding.play;~ding2.play;\n)\n//a slightly more rhythmic element, tracing the rhythm out a bit more\n(\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Pwrand([0,4],[0.8,0.2],inf),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,0.125,\\d,0.25,\\a,Pexprand(0.0001,200),\\pan,0,\\amp,1);\n~ring1.play\n)\n//Add unce unce unce and simmer gently to unify flavours.\n(\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Pwrand([0,4],[0.8,0.2],inf),\\octave,Pwrand([2,3,4],[0.6,0.2,0.2],inf),\\dur,0.125,\\d,0.2,\\a,Pexprand(0.02,900),\\pan,0,\\amp,1);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][1],\\dur,0.5,\\amp,2);\n~k.play;\n)\n//offbeat hat because cheesy rhythms are good fun\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf)/2,\\amp,1)\n~oh.play\n)\n\n\n\n\nUsing offsets\n\n\nBy utilising the \noffset\n argument of \nPbjorklund2\n, small rhythmic elements can be used multiple times with slight variation to pretty powerful effect. \n\n\nThe following example shows what a few basic offsets can do to liven up a very simple rhythmic pattern\n\n\n//working with offsets - doing a lot with a little\n//basic kick\n(\np.clock.tempo = 2.13;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~k.play;\n)\n//Basic 5-16 euclidean rhythm\n(\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7);\n~c.play;\n)\n//add another layer at a different pitch\n//NOTE: These two might not sound at the same time even though they are the same rhythm, as the rhythmic cycle is longer than 1 beat\n(\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7,\\rate,1.1);\n~c2.play;\n)\n//if you want them to sound together, trigger them together\n(\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7,\\rate,1.1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7);\n)\n//offset both\n//Note: I am using .asStream here, because a standard Pwhite will not work in the offset argument of Pbjorklund2, as the values need to be embedded as a stream.\n//A general rule of mine is that if pattern classes don't work properly, use .asStream on the end of them and they likely will.\n(\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(5,16,inf,Pwhite(1,10).asStream)/4,\\amp,0.7);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(5,16,inf,Pwhite(1,15).asStream)/4,\\amp,0.7,\\rate,1.1);\n~c.play;\n~c2.play;\n)\n//and another, slightly different sample\n(\n~c3 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][1],\\dur,Pbjorklund2(5,16,inf,Pwhite(0,8).asStream)/4,\\amp,0.7,\\rate,0.9);\n~c3.play\n)\n//now do the same to the kick\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1,1.2],inf));\n)\n//another kick, slightly different rhythm\n(\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,Pbjorklund2(3,16,inf,Pwhite(1,10).asStream)/4,\\amp,1,\\rate,Pseq([1.1,1.4],inf));\n~k2.play;\n)\n//add sub kick on 1, and you have minimal techno.\n(\n~sk = Pbind(\\instrument,\\bplay,\\buf,d[\nsk\n][0],\\dur,1,\\amp,2);\n~sk.play;\n)\n\n\n\n\nConvergence \n Divergence, using variables inside ProxySpace\n\n\nAs I mentioned in my introduction to ProxySpace, ProxySpace reserved all global variables with the format \n~variableName\n. I use single letter variables (besides \ns\n which is reserved for the server and \np\n which is reserved for ProxySpace) to hold variables for use in patterns. This is handy for a technique that establishes a set of euclidean rhythms like above, and them unifies them under a central rhythm, which can be deviated from during performance.\n\n\nHere are the basics of the technique. Variable \nl\n is used to carry a pattern, which is evaluated alongside each pattern that it contains.\n\n\n//give a central rhythm to be used by other patterns\nl = Pbjorklund2(Pseq([3,3,3,4,3,3,3,5],inf),8)/4;\n//block-execute (Ctrl/Cmd+Enter) between these brackets\n(\np.clock.tempo = 2.1;\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1,\\rate,0.9);\n~c3 = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1,\\rate,1.1);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1);\n~c.play;\n~c2.play;\n~c3.play;\n)\n//now individually execute (Shift+Enter) some of these lines to refresh the 'dur'. Listen for variations in rhythm.\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1,\\rate,0.9);\n~c3 = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1,\\rate,1.1);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1);\n//if you want to reset, execute the block again\n\n\n\n\nHere is a fully fleshed-out example. As \nPwhite\n creates random values, each pattern will create random rhythms independently of one another. Then when they are unified under a \nPseq\n, they will all sound at the same time. With this technique I build up complex rhythms, then convert them to one single rhythm and texture, which I can then build structures on top of.\n\n\n//A more fleshed-out example\n//Start with a random central rhythm, to keep all of the individual parts\n//also using a scale as a one-letter variable for quickness\n(\np.clock.tempo = 2.32;\nl = Pbjorklund2(Pwhite(3,10),16)/4;\ne = Scale.chromatic(\\et53);\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,30),\\amp,0.5,\\pan,1);\n~ring2 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,30),\\amp,0.5,\\pan,-1);\n~ring3 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([4,5],[0.8,0.2],inf),\\dur,l,\\d,0.5,\\a,Pexprand(0.5,30),\\amp,0.5,\\pan,0);\n~ring4 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,l,\\d,0.2,\\a,Pexprand(0.5,200),\\amp,0.9,\\pan,0);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,l,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,l,\\amp,Pwhite(0.2,1));\n~ring1.play;~ring2.play;~ring3.play;~ring4.play;~sn.play;~c.play;~h.play;\n)\n//unify all of these rhythms\n//sounds very different\n//execute individual lines to make them diverge from this pattern\n(\np.clock.tempo = 2.32;\nl = Pbjorklund2(Pseq([3,8,2,5,9,10,14,3,5,5,4,9,14],inf),16)/4;\ne = Scale.chromatic(\\et53);\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,90),\\amp,0.5,\\pan,1);\n~ring2 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,90),\\amp,0.5,\\pan,-1);\n~ring3 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([4,5],[0.8,0.2],inf),\\dur,l,\\d,0.5,\\a,Pexprand(0.5,90),\\amp,0.5,\\pan,0);\n~ring4 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,l,\\d,Pexprand(0.2,0.6),\\a,Pexprand(1,200),\\amp,0.9,\\pan,0);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,l,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,l,\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,l,\\amp,Pwhite(0.2,1))\n)\n//throw some straight rhythms in to show where the beat lies\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][1],\\dur,1,\\rate,1,\\amp,3);\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,Pwhite(0.5,1),\\rate,0.8);\n~k.play;\n~oh.play;\n)", 
            "title": "Euclidean Rhythms"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/#euclidean-rhythms", 
            "text": "", 
            "title": "Euclidean Rhythms"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/#introduction", 
            "text": "Euclidean Rhythms are described in a 2005 paper by Godfried Toussaint entitled  'The Euclidean Algorithm Generates Traditional Musical Rhythms' , which describes the organisation of rhythm by placing onsets as evenly as possible within a number of possible spaces using Bjorklund's algorithm. It's not the easiest thing to verbally describe, but  this online tool  explains it much better, and the paper contains a bunch of illustrated examples.  As mentioned in 3.1, When I was learning how to perform Live Coding I found creating compelling, complex rhythm in SuperCollider quite hard. Euclidean Rhythms and the  Bjorklund quark  have ended up becoming major fixtures of my performance as a result as they handle a lot of the difficulties i have around developing rhythmic complexity in real-time as part of performance. I've always wanted to be able to make rhythms like  DJ Rashad , and using Euclidean Rhythms has got me some way on that quest.", 
            "title": "Introduction"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/#effort-free-rhythmic-complexity", 
            "text": "The problem I had with rhythm was in the fact that all rhythms for all proxies had to be specified as  dur  values, and each one had to be specified independently. Constructing TidalCycles-like 'riffs' containing multiple percussion samples is really quite hard in SuperCollider. As a result, most rhythms I ended up creating involved either using simple on-beat/off-beat patterns, or constraining a  Pwhite  or  Pexprand  into producing random rhythms in time with the ProxySpace tempo clock, and random rhythms with a uniform distribution generally sound quite boring.  The Bjorklund quark contains a few classes that help in using Euclidean Rhythms. I particularly use  Pbjorklund2 , which takes arguments for:   k : Number of 'hits'  n : Number of possible onsets  length : Number of repeats  offset : Starting onset in the pattern   and using this, outputs an array of durations for use as  dur  values in a pattern, for instance:  Pbjorklund2(3,8)  would produce duration arrays of  [ 3, 3, 2 ] .  Because  Pbjorklund2  is a pattern class, it can be nested and have its arguments modulated by other pattern classes, using its inputs to generate sequences, rather than single values. In this way, 'random rhythms' create a much more interesting result, as random values will be used to create a network of onsets, which perceptually appear to be very complex interlocking rhythms.  //four 'randomised' rhythms, sounds okay.\n(\np.clock.tempo = 2.2;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][1],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][1],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][1],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pwhite(0.25,1).round(0.25),\\amp,1);\n~k.play;\n~sn.play;\n~h.play;\n~t.play;\n)\n\n//four randomised euclidean rhythms with four different samples.\n//sounds better, producing a much greater variety of rhythmic forms.\n(\np.clock.tempo = 2.2;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][1],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][1],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][1],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(Pwhite(1,8),Pwhite(1,16))/4,\\amp,1);\n~k.play;\n~sn.play;\n~h.play;\n~t.play;\n)", 
            "title": "Effort-free rhythmic complexity"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/#euclidean-rhythms-vs-the-beat", 
            "text": "The benefit of using the Bjorklund quark like this is that it also lines up with the regular clock of ProxySpace, allowing for scattered, hypercomplex, undanceable rhythms to be established over time, and then in one movement unified under a regular rhythm, such as a straight kick drum with a  dur  of a subdivision of 1.  Here's an example that's sort-of inspired by the lasting impression that Basic Channel's  Phylyps Trak  made on me some time ago.  //Complex rhythm that obfuscates the central rhythmic centre\n(\np.clock.tempo = 1.5;\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pbjorklund2(Pwhite(10,35),41,inf,Pwhite(0,10).asStream)/8,\\amp,Pexprand(0.1,1),\\pan,-1);\n~h2 = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pbjorklund2(Pwhite(10,35),40,inf,Pwhite(0,10).asStream)/8,\\amp,Pexprand(0.1,1),\\pan,1);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,Pbjorklund2(Pwhite(1,5),Pwhite(1,32))/4,\\amp,1,\\rate,Pwrand([1,-1],[0.8,0.2],inf),\\pos,Pkey(\\rate).linlin(1,-1,0,0.999));\n~ding = Pbind(\\instrument,\\bplay,\\buf,d[ ding ][0],\\dur,Pbjorklund2(Pwhite(1,3),25)/4,\\amp,0.6,\\rate,0.6,\\pan,-1);\n~ding2 = Pbind(\\instrument,\\bplay,\\buf,d[ ding ][0],\\dur,Pbjorklund2(Pwhite(1,3),20)/4,\\amp,0.6,\\rate,0.7,\\pan,1);\n~t1 = Pbind(\\instrument,\\bplay,\\buf,d[ mt ][0],\\dur,Pbjorklund2(Pseq([1,1,1,Pwhite(10,15,1).asStream],inf),36,inf,Pwhite(0,2).asStream)/8,\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(Pseq([1,1,1,Pwhite(10,15,1).asStream],inf),40,inf,Pwhite(0,2).asStream)/8,\\amp,1,\\rate,2);\n~t1.play;~t2.play;~h.play;~h2.play;~sn.play;~ding.play;~ding2.play;\n)\n//a slightly more rhythmic element, tracing the rhythm out a bit more\n(\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Pwrand([0,4],[0.8,0.2],inf),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,0.125,\\d,0.25,\\a,Pexprand(0.0001,200),\\pan,0,\\amp,1);\n~ring1.play\n)\n//Add unce unce unce and simmer gently to unify flavours.\n(\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Pwrand([0,4],[0.8,0.2],inf),\\octave,Pwrand([2,3,4],[0.6,0.2,0.2],inf),\\dur,0.125,\\d,0.2,\\a,Pexprand(0.02,900),\\pan,0,\\amp,1);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][1],\\dur,0.5,\\amp,2);\n~k.play;\n)\n//offbeat hat because cheesy rhythms are good fun\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf)/2,\\amp,1)\n~oh.play\n)", 
            "title": "Euclidean Rhythms vs 'the beat'"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/#using-offsets", 
            "text": "By utilising the  offset  argument of  Pbjorklund2 , small rhythmic elements can be used multiple times with slight variation to pretty powerful effect.   The following example shows what a few basic offsets can do to liven up a very simple rhythmic pattern  //working with offsets - doing a lot with a little\n//basic kick\n(\np.clock.tempo = 2.13;\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~k.play;\n)\n//Basic 5-16 euclidean rhythm\n(\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7);\n~c.play;\n)\n//add another layer at a different pitch\n//NOTE: These two might not sound at the same time even though they are the same rhythm, as the rhythmic cycle is longer than 1 beat\n(\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7,\\rate,1.1);\n~c2.play;\n)\n//if you want them to sound together, trigger them together\n(\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7,\\rate,1.1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(5,16)/4,\\amp,0.7);\n)\n//offset both\n//Note: I am using .asStream here, because a standard Pwhite will not work in the offset argument of Pbjorklund2, as the values need to be embedded as a stream.\n//A general rule of mine is that if pattern classes don't work properly, use .asStream on the end of them and they likely will.\n(\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(5,16,inf,Pwhite(1,10).asStream)/4,\\amp,0.7);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(5,16,inf,Pwhite(1,15).asStream)/4,\\amp,0.7,\\rate,1.1);\n~c.play;\n~c2.play;\n)\n//and another, slightly different sample\n(\n~c3 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][1],\\dur,Pbjorklund2(5,16,inf,Pwhite(0,8).asStream)/4,\\amp,0.7,\\rate,0.9);\n~c3.play\n)\n//now do the same to the kick\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1,1.2],inf));\n)\n//another kick, slightly different rhythm\n(\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,Pbjorklund2(3,16,inf,Pwhite(1,10).asStream)/4,\\amp,1,\\rate,Pseq([1.1,1.4],inf));\n~k2.play;\n)\n//add sub kick on 1, and you have minimal techno.\n(\n~sk = Pbind(\\instrument,\\bplay,\\buf,d[ sk ][0],\\dur,1,\\amp,2);\n~sk.play;\n)", 
            "title": "Using offsets"
        }, 
        {
            "location": "/3-4-Euclidean-Rhythms/#convergence-divergence-using-variables-inside-proxyspace", 
            "text": "As I mentioned in my introduction to ProxySpace, ProxySpace reserved all global variables with the format  ~variableName . I use single letter variables (besides  s  which is reserved for the server and  p  which is reserved for ProxySpace) to hold variables for use in patterns. This is handy for a technique that establishes a set of euclidean rhythms like above, and them unifies them under a central rhythm, which can be deviated from during performance.  Here are the basics of the technique. Variable  l  is used to carry a pattern, which is evaluated alongside each pattern that it contains.  //give a central rhythm to be used by other patterns\nl = Pbjorklund2(Pseq([3,3,3,4,3,3,3,5],inf),8)/4;\n//block-execute (Ctrl/Cmd+Enter) between these brackets\n(\np.clock.tempo = 2.1;\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1,\\rate,0.9);\n~c3 = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1,\\rate,1.1);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1);\n~c.play;\n~c2.play;\n~c3.play;\n)\n//now individually execute (Shift+Enter) some of these lines to refresh the 'dur'. Listen for variations in rhythm.\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1,\\rate,0.9);\n~c3 = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1,\\rate,1.1);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1);\n//if you want to reset, execute the block again  Here is a fully fleshed-out example. As  Pwhite  creates random values, each pattern will create random rhythms independently of one another. Then when they are unified under a  Pseq , they will all sound at the same time. With this technique I build up complex rhythms, then convert them to one single rhythm and texture, which I can then build structures on top of.  //A more fleshed-out example\n//Start with a random central rhythm, to keep all of the individual parts\n//also using a scale as a one-letter variable for quickness\n(\np.clock.tempo = 2.32;\nl = Pbjorklund2(Pwhite(3,10),16)/4;\ne = Scale.chromatic(\\et53);\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,30),\\amp,0.5,\\pan,1);\n~ring2 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,30),\\amp,0.5,\\pan,-1);\n~ring3 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([4,5],[0.8,0.2],inf),\\dur,l,\\d,0.5,\\a,Pexprand(0.5,30),\\amp,0.5,\\pan,0);\n~ring4 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,l,\\d,0.2,\\a,Pexprand(0.5,200),\\amp,0.9,\\pan,0);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,l,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,l,\\amp,Pwhite(0.2,1));\n~ring1.play;~ring2.play;~ring3.play;~ring4.play;~sn.play;~c.play;~h.play;\n)\n//unify all of these rhythms\n//sounds very different\n//execute individual lines to make them diverge from this pattern\n(\np.clock.tempo = 2.32;\nl = Pbjorklund2(Pseq([3,8,2,5,9,10,14,3,5,5,4,9,14],inf),16)/4;\ne = Scale.chromatic(\\et53);\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,90),\\amp,0.5,\\pan,1);\n~ring2 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-2,2),\\octave,Pwrand([3,4],[0.8,0.2],inf),\\dur,l,\\d,0.4,\\a,Pexprand(0.5,90),\\amp,0.5,\\pan,-1);\n~ring3 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([4,5],[0.8,0.2],inf),\\dur,l,\\d,0.5,\\a,Pexprand(0.5,90),\\amp,0.5,\\pan,0);\n~ring4 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,e,\\root,0,\\degree,Pwhite(-5,5),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,l,\\d,Pexprand(0.2,0.6),\\a,Pexprand(1,200),\\amp,0.9,\\pan,0);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,l,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,l,\\amp,1);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,l,\\amp,Pwhite(0.2,1))\n)\n//throw some straight rhythms in to show where the beat lies\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][1],\\dur,1,\\rate,1,\\amp,3);\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,Pwhite(0.5,1),\\rate,0.8);\n~k.play;\n~oh.play;\n)", 
            "title": "Convergence &amp; Divergence, using variables inside ProxySpace"
        }, 
        {
            "location": "/3-5-StageLimiter-Abuse/", 
            "text": "StageLimiter abuse and 'The Guetta Effect'\n\n\n\n\nListen to the chorus of \n'Titanium' by David Guetta ft Sia\n. That 'pumping' sound heard around the kick drums in the synth parts is (probably) a result of \nSidechain Compression\n, an effect that's pretty common in dance music which (essentially) uses the volume of a track to duck the volume of other tracks. \n\n\nI've found it very helpful to employ this technique at various points during performance to reinforce the dominant rhythmic pulse of a set. Take \nthis rehearsal excerpt for example\n, where the 'bell' sounds are being 'pumped' by the bass drum, it's not too subtle. Or skip to 1.22 in \nthis glitchy excerpt\n, the irregular pitched-up clap is literally cutting off the atonal chimes underneath it. There's also the first half of \nthis set\n where I am attempting to riff on some tropes from \nPsytrance\n, using the kick drum to modulate the two interlocking distorted synth riffs that are being played.\n\n\nWith the exception of the 'Psytrance' riff, I almost always achieve this pseudo-sidechaining effect in the most brutal way possible - by abusing StageLimiter. As StageLimiter is just a \nLimiter.ar on the output\n, any sounds over an amplitude of 0dB in the mix will reduce the volume of any other sounds in the mix without distorting. As I tend to use percussion that is normalised to 0dB, any percussion that is played with an \n\\amp\n value of greater than \n1\n will compress the rest of the mix in proportion to the volume that they hit above 0dB. This can range from subtle to completely ridiculous. \n\n\nHere are a few examples of this.\n\n\n//1:\n//a complex polyrhythm - no need to worry about the construction of this.\n(\np.clock.tempo = 2.3;\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,0.75,\\amp,1);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,Pbjorklund2(Pseq([3,3,3,5],inf),8)/4,\\amp,1);\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1,\\stretch,Pwhite(1,0.25).round(0.25));\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,Pbjorklund2(Pwhite(3,10),16),\\amp,1);\n~t1 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,1/5*4,\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,1/9*4.5,\\amp,1,\\rate,2);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/8,\\amp,Pwhite(0.2,1.4));\n~fx1 = Pbind(\\instrument,\\bplay,\\buf,d[\nsfx\n][0],\\dur,Pwhite(1,4.0).round(0.5),\\amp,1);\n~fx2 = Pbind(\\instrument,\\bplay,\\buf,d[\nsfx\n][1],\\dur,Pwhite(1,8.0).round(0.25),\\amp,1);\n~c.play;~c2.play;~oh.play;~sn.play;~t1.play;~t2.play;~h.play;~fx1.play;~fx2.play;\n)\n//A 0db kick which doesn't really do anything in the mix\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~k.play;\n)\n//A \n0dB kick which compresses everything else and audibly 'centers' everything around it because it is so loud.\n//There's probably some psychoacoustics involved in this that i'm not qualified to talk about.\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,4);\n~k.play;\n)\n//a really *really* loud, very occasional percussion which silences everything else (slowed down for exaggerated effect)\n(\n~hugesnare = Pbind(\\instrument,\\bplay,\\buf,d[\nmt\n][0],\\dur,Pwhite(8,16),\\amp,4000000,\\rate,1);\n~hugesnare.play;\n)\n\n//2:\n//some beautiful pads\n//thanks Eli Fieldsteel\n(\np.clock.tempo = 2;\n(\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(4.5,7.0,inf),\n    \\midinote,Pxrand([\n        [23,35,54,63,64],\n        [45,52,54,59,61,64],\n        [28,40,47,56,59,63],\n        [42,52,57,61,63],\n    ],inf),\n    \\detune, Pexprand(0.0001,0.1,inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,6,\n    \\amp,Pwhite(0.8,2.0),\n    \\out,0)\n);\n~chords.play;\n)\n//pulse them slightly with a low-passed kick\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nsk\n][0],\\dur,Pbjorklund2(3,8)/2,\\amp,2);\n//Low Pass\n~lpfSend = {[~k]};\n~lpf = {RLPF.ar(Mix.ar([~lpfSend]),SinOsc.kr(0.1).range(200,100),1)};\n~lpf.play;\n)\n//eliminate them completely with an absurdly loud low-passed kick (those with subwoofers be careful!)\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,Pbjorklund2(3,8)/4,\\amp,9000,\\rate,5);\n//Low Pass\n~lpfSend = {[~k]};\n~lpf = {RLPF.ar(Mix.ar([~lpfSend]),SinOsc.kr(0.1).range(100,80),0.3)};\n~lpf.play;\n)", 
            "title": "StageLimiter Abuse"
        }, 
        {
            "location": "/3-5-StageLimiter-Abuse/#stagelimiter-abuse-and-the-guetta-effect", 
            "text": "Listen to the chorus of  'Titanium' by David Guetta ft Sia . That 'pumping' sound heard around the kick drums in the synth parts is (probably) a result of  Sidechain Compression , an effect that's pretty common in dance music which (essentially) uses the volume of a track to duck the volume of other tracks.   I've found it very helpful to employ this technique at various points during performance to reinforce the dominant rhythmic pulse of a set. Take  this rehearsal excerpt for example , where the 'bell' sounds are being 'pumped' by the bass drum, it's not too subtle. Or skip to 1.22 in  this glitchy excerpt , the irregular pitched-up clap is literally cutting off the atonal chimes underneath it. There's also the first half of  this set  where I am attempting to riff on some tropes from  Psytrance , using the kick drum to modulate the two interlocking distorted synth riffs that are being played.  With the exception of the 'Psytrance' riff, I almost always achieve this pseudo-sidechaining effect in the most brutal way possible - by abusing StageLimiter. As StageLimiter is just a  Limiter.ar on the output , any sounds over an amplitude of 0dB in the mix will reduce the volume of any other sounds in the mix without distorting. As I tend to use percussion that is normalised to 0dB, any percussion that is played with an  \\amp  value of greater than  1  will compress the rest of the mix in proportion to the volume that they hit above 0dB. This can range from subtle to completely ridiculous.   Here are a few examples of this.  //1:\n//a complex polyrhythm - no need to worry about the construction of this.\n(\np.clock.tempo = 2.3;\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,0.75,\\amp,1);\n~c2 = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,Pbjorklund2(Pseq([3,3,3,5],inf),8)/4,\\amp,1);\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1,\\stretch,Pwhite(1,0.25).round(0.25));\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,Pbjorklund2(Pwhite(3,10),16),\\amp,1);\n~t1 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,1/5*4,\\amp,1);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,1/9*4.5,\\amp,1,\\rate,2);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/8,\\amp,Pwhite(0.2,1.4));\n~fx1 = Pbind(\\instrument,\\bplay,\\buf,d[ sfx ][0],\\dur,Pwhite(1,4.0).round(0.5),\\amp,1);\n~fx2 = Pbind(\\instrument,\\bplay,\\buf,d[ sfx ][1],\\dur,Pwhite(1,8.0).round(0.25),\\amp,1);\n~c.play;~c2.play;~oh.play;~sn.play;~t1.play;~t2.play;~h.play;~fx1.play;~fx2.play;\n)\n//A 0db kick which doesn't really do anything in the mix\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~k.play;\n)\n//A  0dB kick which compresses everything else and audibly 'centers' everything around it because it is so loud.\n//There's probably some psychoacoustics involved in this that i'm not qualified to talk about.\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,4);\n~k.play;\n)\n//a really *really* loud, very occasional percussion which silences everything else (slowed down for exaggerated effect)\n(\n~hugesnare = Pbind(\\instrument,\\bplay,\\buf,d[ mt ][0],\\dur,Pwhite(8,16),\\amp,4000000,\\rate,1);\n~hugesnare.play;\n)\n\n//2:\n//some beautiful pads\n//thanks Eli Fieldsteel\n(\np.clock.tempo = 2;\n(\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(4.5,7.0,inf),\n    \\midinote,Pxrand([\n        [23,35,54,63,64],\n        [45,52,54,59,61,64],\n        [28,40,47,56,59,63],\n        [42,52,57,61,63],\n    ],inf),\n    \\detune, Pexprand(0.0001,0.1,inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,6,\n    \\amp,Pwhite(0.8,2.0),\n    \\out,0)\n);\n~chords.play;\n)\n//pulse them slightly with a low-passed kick\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ sk ][0],\\dur,Pbjorklund2(3,8)/2,\\amp,2);\n//Low Pass\n~lpfSend = {[~k]};\n~lpf = {RLPF.ar(Mix.ar([~lpfSend]),SinOsc.kr(0.1).range(200,100),1)};\n~lpf.play;\n)\n//eliminate them completely with an absurdly loud low-passed kick (those with subwoofers be careful!)\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,Pbjorklund2(3,8)/4,\\amp,9000,\\rate,5);\n//Low Pass\n~lpfSend = {[~k]};\n~lpf = {RLPF.ar(Mix.ar([~lpfSend]),SinOsc.kr(0.1).range(100,80),0.3)};\n~lpf.play;\n)", 
            "title": "StageLimiter abuse and 'The Guetta Effect'"
        }, 
        {
            "location": "/3-6-L-Systems-For-Rhythm/", 
            "text": "L-systems for Rhythm\n\n\n\n\nAs of early April 2017 I haven't been doing this for too long, so this section will be brief.\n\n\nL-systems\n are, according to Wikipedia:\n\n\n\n\na parallel rewriting system and a type of formal grammar. An L-system consists of an alphabet of symbols that can be used to make strings, a collection of production rules that expand each symbol into some larger string of symbols, an initial \"axiom\" string from which to begin construction, and a mechanism for translating the generated strings into geometric structures.\n\n\n\n\nFor a good example to visualise what this means, \nthis\n was one I found very helpful.\n\n\nI was inspired to start using L-systems for rhythm after hearing one of \nRenick Bell's\n \nFractal Beats\n tracks on SoundCloud, and in turn reading his paper about \nrhythmic density in live coding\n for the Linux Audio Conference. The approach to rhythm in this Fractal Beats track is unlike any I have heard - the rhythms are complex and don't appear to lock into common divisions of a regular beat, but do not seem to fall into the trappings of being 'random'. This stochastic approach to rhythm appears to yield something that, to me, resembles 'free techno'\u2020. \n\n\nWhile I have no idea how to use \nConductive\n, there are some useful implementations of L-systems in SuperCollider. \nPrewrite\n is SuperCollider's class for implementing L-systems within patterns. Prewrite takes a rule set and an initial axiom, and will expand the axiom within a Pbind.\n\n\nFor example:\n\n\n//use L-system as a duration value for a kickdrum\n(\nl = Prewrite(1, // start with 1\n        (    1: [0.25,2],\n            0.25: [3,3,2]/4,\n        3/4: [0.25,1,0.125,0.125],\n        ), 4);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,l,\\amp,1);\n~k.play;\n)\n/*\n\nWith that grammar:\n\n1 -\n 0.25,2 -\n 3/4,3/4,2/4 -\n 0.25,1,0.125,0.125,0.25,1,0.125,0.125 -\n etc.\n\n*/\n//much like with the euclidean rhythm convergence/divergence pattern, you can use variable l for different patterns too\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,l,\\amp,1,\\rate,Pseq((1..4)/2,inf));\n~sn.play;\n)\n//and transform it\n(\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,l,\\stretch,Pwhite(0.5,2).round(0.5),\\amp,Pwhite(0.2,1));\n~h.play;\n)\n//an off-beat open hat for reference\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1);\n~oh.play;\n)\n\n\n\n\n\u2020 With the 'free' in 'free techno' I am referring specifically to \nfree improvisation\n, a music style involving one or more instrumentalists improvising with no (or little) preordained structure. Some examples from a few styles of Free Improvisation are \nThe Ornette Coleman and Eric Dolphy Octet\n, \nYeah You\n, \nOkkyung Lee\n, \n\u9ed2\u96fb\u8a71666\n and \nUsurper\n. While the stochastic approach obviously has preordained structure, it is much less idiomatic than the kinds of regular rhythms common in dance music. The rhythms in the linked Fractal Beats track appear to 'wander' in a way that I can only describe as being reminiscent of Free Improvisation, while still appearing to co-ordinate around the central rhythm of the track. The divergence in the rhythms present is profound, but at no point does any of it seem to be 'out of control' or 'random'. This isn't the most articulate explanation of my thoughts, but it's the closest I've come to describing Renick's music.", 
            "title": "L-Systems for Rhythm"
        }, 
        {
            "location": "/3-6-L-Systems-For-Rhythm/#l-systems-for-rhythm", 
            "text": "As of early April 2017 I haven't been doing this for too long, so this section will be brief.  L-systems  are, according to Wikipedia:   a parallel rewriting system and a type of formal grammar. An L-system consists of an alphabet of symbols that can be used to make strings, a collection of production rules that expand each symbol into some larger string of symbols, an initial \"axiom\" string from which to begin construction, and a mechanism for translating the generated strings into geometric structures.   For a good example to visualise what this means,  this  was one I found very helpful.  I was inspired to start using L-systems for rhythm after hearing one of  Renick Bell's   Fractal Beats  tracks on SoundCloud, and in turn reading his paper about  rhythmic density in live coding  for the Linux Audio Conference. The approach to rhythm in this Fractal Beats track is unlike any I have heard - the rhythms are complex and don't appear to lock into common divisions of a regular beat, but do not seem to fall into the trappings of being 'random'. This stochastic approach to rhythm appears to yield something that, to me, resembles 'free techno'\u2020.   While I have no idea how to use  Conductive , there are some useful implementations of L-systems in SuperCollider.  Prewrite  is SuperCollider's class for implementing L-systems within patterns. Prewrite takes a rule set and an initial axiom, and will expand the axiom within a Pbind.  For example:  //use L-system as a duration value for a kickdrum\n(\nl = Prewrite(1, // start with 1\n        (    1: [0.25,2],\n            0.25: [3,3,2]/4,\n        3/4: [0.25,1,0.125,0.125],\n        ), 4);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,l,\\amp,1);\n~k.play;\n)\n/*\n\nWith that grammar:\n\n1 -  0.25,2 -  3/4,3/4,2/4 -  0.25,1,0.125,0.125,0.25,1,0.125,0.125 -  etc.\n\n*/\n//much like with the euclidean rhythm convergence/divergence pattern, you can use variable l for different patterns too\n(\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,l,\\amp,1,\\rate,Pseq((1..4)/2,inf));\n~sn.play;\n)\n//and transform it\n(\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,l,\\stretch,Pwhite(0.5,2).round(0.5),\\amp,Pwhite(0.2,1));\n~h.play;\n)\n//an off-beat open hat for reference\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,1);\n~oh.play;\n)  \u2020 With the 'free' in 'free techno' I am referring specifically to  free improvisation , a music style involving one or more instrumentalists improvising with no (or little) preordained structure. Some examples from a few styles of Free Improvisation are  The Ornette Coleman and Eric Dolphy Octet ,  Yeah You ,  Okkyung Lee ,  \u9ed2\u96fb\u8a71666  and  Usurper . While the stochastic approach obviously has preordained structure, it is much less idiomatic than the kinds of regular rhythms common in dance music. The rhythms in the linked Fractal Beats track appear to 'wander' in a way that I can only describe as being reminiscent of Free Improvisation, while still appearing to co-ordinate around the central rhythm of the track. The divergence in the rhythms present is profound, but at no point does any of it seem to be 'out of control' or 'random'. This isn't the most articulate explanation of my thoughts, but it's the closest I've come to describing Renick's music.", 
            "title": "L-systems for Rhythm"
        }, 
        {
            "location": "/3-7-Looping/", 
            "text": "Looping rhythms and samples with the \nlplay\n SynthDef\n\n\n\n\nA part of rhythmic electronic music that SuperCollider isn't so great at dealing with are loops. In the Pattern library there isn't a defaulting to 'loop'-based musical structures as is the default in DAW environments such as Ableton live.\n\n\nThis is of course extremely powerful, but sometimes for more complex rhythmic forms, loops are a reasonable practical substitute.\n\n\nI use loops particularly when there are some rhythms that I find hard to articulate by specifying duration values - an example being the classic \nAmen Break\n when I'm trying to make some fast Drum-and-bass style music.\n\n\nFor this, I wrote \nlplay\n, a variation on the \nbplay\n Synthdef that is ubiquitous in my Live Coding setup.\n\n\n(\nSynthDef(\\lplay,\n    {arg out = 0, buf = 0, amp = 0.5, pan = 0 rel=15, dur = 8;\n        var sig,env ;\n        sig = Mix.ar(PlayBuf.ar(2,buf,BufRateScale.ir(buf) * ((BufFrames.ir(buf)/s.sampleRate)*p.clock.tempo/dur),1,0,doneAction:2));\n        env = EnvGen.ar(Env.linen(0.0,rel,0),doneAction:2);\n        sig = sig * env;\n        sig = sig * amp;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n)\n\n\n\n\nlplay\n takes a \ndur\n value and plays a buffer exactly over the time period speficied by the \ndur\n value. For example, if you have a 8 beat drum loop, and you created this \nPbind\n:\n\n\np.clock.tempo = 175/60\n~loop = Pbind(\\instrument,\\lplay,\\buf,d[\nbreaks175\n][0],\\dur,16)\n\n\n\n\nThe loop would play over 8 cycles of the \nProxySpace\n \nTempoClock\n (p.clock.tempo). This is achieved by using this equation for the \nrate\n control:\n\n\n((BufFrames.ir(buf)/s.sampleRate)*p.clock.tempo/dur)\n\n\n\n\nNote that the looping is tied to the rate of playback, so the faster the tempo, the faster the sample will be played. If you try and play a 120bpm sample at 175bpm, it will sound very high-pitched! - Be aware of this when using it during performance.\n\n\nAn important note is that you will have to \nreload the synthdef\n when the tempo is changed if you want looping to work with the updated tempo", 
            "title": "Looping and using Looped Samples"
        }, 
        {
            "location": "/3-7-Looping/#looping-rhythms-and-samples-with-the-lplay-synthdef", 
            "text": "A part of rhythmic electronic music that SuperCollider isn't so great at dealing with are loops. In the Pattern library there isn't a defaulting to 'loop'-based musical structures as is the default in DAW environments such as Ableton live.  This is of course extremely powerful, but sometimes for more complex rhythmic forms, loops are a reasonable practical substitute.  I use loops particularly when there are some rhythms that I find hard to articulate by specifying duration values - an example being the classic  Amen Break  when I'm trying to make some fast Drum-and-bass style music.  For this, I wrote  lplay , a variation on the  bplay  Synthdef that is ubiquitous in my Live Coding setup.  (\nSynthDef(\\lplay,\n    {arg out = 0, buf = 0, amp = 0.5, pan = 0 rel=15, dur = 8;\n        var sig,env ;\n        sig = Mix.ar(PlayBuf.ar(2,buf,BufRateScale.ir(buf) * ((BufFrames.ir(buf)/s.sampleRate)*p.clock.tempo/dur),1,0,doneAction:2));\n        env = EnvGen.ar(Env.linen(0.0,rel,0),doneAction:2);\n        sig = sig * env;\n        sig = sig * amp;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n)  lplay  takes a  dur  value and plays a buffer exactly over the time period speficied by the  dur  value. For example, if you have a 8 beat drum loop, and you created this  Pbind :  p.clock.tempo = 175/60\n~loop = Pbind(\\instrument,\\lplay,\\buf,d[ breaks175 ][0],\\dur,16)  The loop would play over 8 cycles of the  ProxySpace   TempoClock  (p.clock.tempo). This is achieved by using this equation for the  rate  control:  ((BufFrames.ir(buf)/s.sampleRate)*p.clock.tempo/dur)  Note that the looping is tied to the rate of playback, so the faster the tempo, the faster the sample will be played. If you try and play a 120bpm sample at 175bpm, it will sound very high-pitched! - Be aware of this when using it during performance.  An important note is that you will have to  reload the synthdef  when the tempo is changed if you want looping to work with the updated tempo", 
            "title": "Looping rhythms and samples with the lplay SynthDef"
        }, 
        {
            "location": "/4-1-Pitch-And-Patterns/", 
            "text": "Pitch and Patterns\n\n\n\n\nA preamble - How is pitch handled?\n\n\nThere are a number of different ways to arrange pitch - a brief history of pitch.\n\n\nFor some context, my musical background is in the western classical music tradition, but I regularly use non-'standard' pitch arrangement techniques in my music.\n\n\nHow Patterns handle pitch\n\n\nMost times I'm specifying pitch for a synth or sound I will be specifying it as part of a Pbind. Pbinds are set up to handle pitch using the \nfreq\n argument of a SynthDef, with various Pbind arguments designed to 'plug in' to create various kinds of pitch structures:\n\n\nfreq\n can be used to specify a raw frequency value, and \ndetune\n is added to it:\n\n\n//freq specifying a raw pitch value\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pwhite(100,800),\\dur,0.1,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n//frequency being detuned gradually\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..8),inf)*100,\\dur,0.1,\\amp,0.3,\\fb,0.4,\\rel,1,\\detune,Pseq((1..400),inf));\n)\n\n\n\n\nscale\n, \noctave\n and \ndegree\n work together to easily give the ability to use a specific scale/tuning pitch arrangement inside of a Pbind, for example:\n\n\n//using scales inside of Pbinds\n//Minor scale in Just intonation, octave varying between 4 and 6, root note varying between 0 and 4 each scale repetition.\n//\\detune can also be used on top of this to detune scale degrees\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor(\\just),\\root,Pwhite(0,4).stutter(8),\\octave,Pwhite(4,6).stutter(8),\\degree,Pseq((0..7),inf),\\dur,0.25,\\amp,0.3,\\fb,1,\\rel,0.2);\n~sinfb.play;\n)\n\n\n\n\nArrays can also be used to create chords:\n\n\n//Chords used by specifying a 2-dimensional array in \\degree argument.\n//same can be done for the \\octave argument\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\n    \\scale,Scale.major,\n    \\root,0,\n    \\octave,Pwrand([4,[3,4],[2,3,4]],[0.9,0.08,0.02],inf),\n    \\degree,Prand([[0,2,4],[2,4,6],[7,2,4],[1,2,3],[0,-2,-4]],inf),\n    \\dur,Pwhite(5,10),\n    \\atk,2,\\sus,1,\\rel,3,\\amp,0.3,\\fb,0.1);\n~modulation = {SinOsc.kr(0.1).range(0.01,1.41)};\n~sinfb.play;\n~sinfb.set(\\fb,~modulation);\n)\n\n\n\n\nIt's important to note that the degrees of a scale start from \n0\n when using patterns, with \n(0..7)\n being a full octave of a major or minor scale.", 
            "title": "Pitch and Patterns"
        }, 
        {
            "location": "/4-1-Pitch-And-Patterns/#pitch-and-patterns", 
            "text": "", 
            "title": "Pitch and Patterns"
        }, 
        {
            "location": "/4-1-Pitch-And-Patterns/#a-preamble-how-is-pitch-handled", 
            "text": "There are a number of different ways to arrange pitch - a brief history of pitch.  For some context, my musical background is in the western classical music tradition, but I regularly use non-'standard' pitch arrangement techniques in my music.", 
            "title": "A preamble - How is pitch handled?"
        }, 
        {
            "location": "/4-1-Pitch-And-Patterns/#how-patterns-handle-pitch", 
            "text": "Most times I'm specifying pitch for a synth or sound I will be specifying it as part of a Pbind. Pbinds are set up to handle pitch using the  freq  argument of a SynthDef, with various Pbind arguments designed to 'plug in' to create various kinds of pitch structures:  freq  can be used to specify a raw frequency value, and  detune  is added to it:  //freq specifying a raw pitch value\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pwhite(100,800),\\dur,0.1,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n//frequency being detuned gradually\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..8),inf)*100,\\dur,0.1,\\amp,0.3,\\fb,0.4,\\rel,1,\\detune,Pseq((1..400),inf));\n)  scale ,  octave  and  degree  work together to easily give the ability to use a specific scale/tuning pitch arrangement inside of a Pbind, for example:  //using scales inside of Pbinds\n//Minor scale in Just intonation, octave varying between 4 and 6, root note varying between 0 and 4 each scale repetition.\n//\\detune can also be used on top of this to detune scale degrees\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor(\\just),\\root,Pwhite(0,4).stutter(8),\\octave,Pwhite(4,6).stutter(8),\\degree,Pseq((0..7),inf),\\dur,0.25,\\amp,0.3,\\fb,1,\\rel,0.2);\n~sinfb.play;\n)  Arrays can also be used to create chords:  //Chords used by specifying a 2-dimensional array in \\degree argument.\n//same can be done for the \\octave argument\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\n    \\scale,Scale.major,\n    \\root,0,\n    \\octave,Pwrand([4,[3,4],[2,3,4]],[0.9,0.08,0.02],inf),\n    \\degree,Prand([[0,2,4],[2,4,6],[7,2,4],[1,2,3],[0,-2,-4]],inf),\n    \\dur,Pwhite(5,10),\n    \\atk,2,\\sus,1,\\rel,3,\\amp,0.3,\\fb,0.1);\n~modulation = {SinOsc.kr(0.1).range(0.01,1.41)};\n~sinfb.play;\n~sinfb.set(\\fb,~modulation);\n)  It's important to note that the degrees of a scale start from  0  when using patterns, with  (0..7)  being a full octave of a major or minor scale.", 
            "title": "How Patterns handle pitch"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/", 
            "text": "Types of Pitch Arrangement\n\n\n\n\nMajor/Minor scales\n\n\nThe bedrock of the western musical canon is major and minor scales. Generally it's taught in British music education that the major scale is a '\nhappy\n' sound and the minor scale is a '\nserious\n' or '\nsad\n'. Generally though Minor tends to be used in most music I hear on a day-to-day basis, so if I'm going to be drawing on standard musical scale I will use that. For information on what they are from a music theory perspective check \nthis article\n.\n\n\nA few good chords to use that will work with the Major and Minor scale very well at any point will be the following:\n\n\n//chords I, IV and V\n//in Major and Minor - re-evaluate for a different scale (using the .choose method)\n(\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(4.5,7.0,inf),\n    \\scale,[Scale.minor,Scale.major].choose,\n    \\degree,Pwrand([[0,2,4],[3,5,7],[4,6,8]],[0.5,0.25,0.25],inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,6,\n    \\lsf,1000,\n    \\octave,Pwrand([4,3,5],[0.6,0.3,0.1],inf),\n    \\amp,Pwhite(0.8,2.0),\n    \\out,0);\n~chords.play;\n);\n\n\n\n\nThe chords I, IV and V are fundamental parts of the vast majority of chord progressions in major or minor scales, with chord ii also being very common. If you randomly play these four chords over a random melody of the same (major/minor) scale, it'll sound \npretty\n good:\n\n\n//major/minor scale chords with a fairly melody which meanders around the major/minor scale, but sounds consonant at the vast majority of points\n//scale stored in a dictionary key so that it can be used in both Pbinds easily\n(\nd[\\scale] = [Scale.major,Scale.minor].choose;\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(4.5,7.0,inf),\n    \\scale,d[\\scale],\n    \\degree,Pwrand([[0,2,4],[3,5,7],[4,6,8]],[0.5,0.25,0.25],inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,6,\n    \\lsf,1000,\n    \\octave,Pwrand([4,3,5],[0.6,0.3,0.1],inf),\n    \\amp,Pwhite(0.8,2.0),\n    \\out,0);\n~chords.play;\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\root,0,\\octave,[4,5],\\degree,Place([0,0,2,[4,5,6],[7,1,2],[6,7,8,9],[10,12,14,15],7,6,5],inf),\\dur,Pbjorklund2(Pwhite(6,8),8)/4,\\amp,0.4,\\fb,0.9,\\rel,0.2);\n~sinfb.play\n);\n\n\n\n\nThe Major and Minor \nPentatonic scales\n are also good for 'sounding good', and are very popular on Guitar for easily creating solo lines.\n\n\nChordSymbol - chord notation in SuperCollider\n\n\nIf you have a specific set of chords you would like to play using Patterns, the \nChordSymbol addon\n by triss is a great way to do this, with the chords in arrays I specified in the previous section replaced by a dictionary of chord names, which are automatically translated into their note values. This is very useful if you're working with an instrumentalist and you're not too quick in translating numbers to named chords (which I am not)\n\n\n//ChordProg - house chords with chord names in an array to make a chord sequence...\n//Today is gonna be the day that they're gonna throw it back to you...\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.chromatic,\\octave,4,\\degree,Pseq([\\Em7,\\G,\\Dsus4,\\A7sus4].chordProg,inf).stutter(6),\\dur,1,\\atk,0.8,\\amp,0.3,\\fb,0.1,\\rel,1);\n~sinfb.play\n)\n\n//giant steps. Apparently.\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.chromatic,\\octave,4,\\degree,Pseq([\\Bmajor7,\\D7,\\Gmajor7,\\Bb7,\\Ebmajor7,\\Am7,\\D7,\\Gmajor7,\\Bb7,\\Ebmajor7,\\Gb7,\\Bmajor7,\\Fm7,\\Bb7,\\Ebmajor7,\\Am7,\\D7,\\Gmajor7,\\Dbm7,\\Gb7,\\Bmajor7,\\Fm7,\\Bb7,\\Ebmajor7,\\Dbm7,\\Gbm7].chordProg,inf),\\dur,1,\\atk,0.1,\\amp,0.3,\\fb,0.1,\\rel,1);\n~sinfb.play;\n)\n\n//a musical example in context - Adapted from a set for Manchester Algorave\n(\np.clock.tempo = 180/60;\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(9.5,15.0,inf),\n    \\scale,Scale.chromatic,\n    \\degree,Pxrand([\\Em,\\Am7,\\Bm7].chordProg,inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\detune,Pexprand(0.0001,1),\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,13,\n    \\lsf,1000,\n    \\octave,Pwrand([4,5,6],[0.8,0.15,0.05],inf),\n    \\amp,Pwhite(0.8,1.5),\n    \\out,0);\n~chords.play;\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/4,\\amp,0.4,\\pan,0.2,\\rate,Pwhite(1.7,2));\n~t = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/4,\\amp,0.8,\\pan,-0.2,\\rate,2);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[\nt\n][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/4,\\amp,0.8,\\pan,-0.2,\\rate,4);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,Pbjorklund2(Pwrand([3,6],[0.8,0.2],inf),8)/4,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[\nc\n][0],\\dur,4,\\amp,4);\n~oh.play;~t.play;~k.play;~c.play;~t2.play;\n)\n\n\n\n\nChromatic Scales\n\n\nMicrotonal/Alternative scales\n\n\nSuperCollider has a bunch of built-in scales (which can be found by evaluating \nScale.directory\n), all of which can be used in patterns by using them as part of the \n\\scale\n argument.\n\n\n//Alternative scales\n//Evaluate to select a scale using the ET12 tuning and run it in ascending order, there are a number of scales so evaluate this a bunch of times\n//scales are stored in a dictionary to be referred to multiple times within the ~sinfb pbind\n(\np.clock.tempo = 1;\nd[\\scale] = Scale.choose.postln;\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,4,\\degree,Pseq((0..d[\\scale].degrees.size-1),inf),\\dur,0.25,\\amp,0.3,\\fb,0.6,\\rel,0.3);\n~sinfb.play;\n)\n\n//Microtonal scales\n(\np.clock.tempo = 1;\nd[\\scale] = [Scale.zamzam,Scale.chromatic24,Scale.partch_o1,Scale.husseini,Scale.zanjaran,Scale.bhairav].choose.postln;\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,4,\\degree,Pseq((0..d[\\scale].degrees.size-1),inf),\\dur,0.25,\\amp,0.3,\\fb,0.6,\\rel,0.3);\n~sinfb.play;\n)\n\n\n\n\nAlternative tunings\n\n\nSuperCollider also has a bunch of built-in tunings (which can be found by evaluating \nTuning.directory\n). These are specified as part of the \nscale\n argument after the scale that is used.\n\n\n//Alternative Tunings\n//Chromatic scale in a random tuning - some relatively subtle differences here\n(\np.clock.tempo = 1;\nd[\\scale] = Scale.chromatic(Tuning.choose);\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,4,\\degree,Pseq((0..d[\\scale].degrees.size-1),inf),\\dur,0.25,\\amp,0.3,\\fb,0.6,\\rel,0.3);\n~sinfb.play;\n)\n\n//A musical example of alternative tunings\n//one of my favourites is the et53 tuning, using it to slightly disturb a central pitch on multiple instruments, sounds really nice in acid-type music\n//by selectively deploying et53, a very narrow pitch range can become normal, making large pitch leaps within an octave seem huge when used.\n(\np.clock.tempo = 150/60;\nd[\\scale] = Scale.chromatic(\\et53);\nl = Pbjorklund2(Pwhite(1,13),16)/4;\n//notice the \\degree argument - ranges from -8 to +8, but this difference is nowhere near an octave\n~ring3 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-8,8),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,l,\\d,0.24,\\a,Pexprand(10,400),\\pan,0,\\amp,1.5);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][1],\\dur,l,\\amp,0.8);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][1],\\dur,l,\\amp,0.8);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][1],\\dur,1,\\amp,2);\n~ring3.play;~sn.play;~h.play;~k.play;\n)\n//adding more acid lines which diverge even less. Also adding percussion\n(\n~ring2 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-4,4),\\octave,5,\\dur,l,\\d,0.37,\\a,Pexprand(1,40),\\pan,1,\\amp,0.5);\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-4,4),\\octave,4,\\dur,l,\\d,0.38,\\a,Pexprand(1,40),\\pan,-1,\\amp,0.5);\n~ring2.play;~ring1.play;\n)\n//another acid line that diverges quite a bit. also open hats\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[\noh\n][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,2);\n~oh.play;\n~ring4 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-8,8),\\octave,7,\\dur,l,\\d,0.21,\\a,Pexprand(1,100),\\pan,1,\\amp,0.8);\n~ring4.play;\n)\n//repetive distorted \\sinfb riff, using the whole octave\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,[5,6],\\degree,Place([0,0,-52,[30,20,10],[52,40,25,20],[10,11,9,3,6],[30,36,39,40]],inf),\\dur,0.25,\\amp,0.5,\\fb,Pwhite(10.5,900.5),\\rel,Pexprand(0.1,0.5));\n~sinfb.play;\n)\n//remove percussion\n(\n~k.stop;~sn.stop;~h.stop;\n)\n\n\n\n\nHarmonic (overtone) series\n\n\nFrom \nWikipedia\n:\n\n\n\n\nA harmonic series is the sequence of sounds where the base frequency of each sound is an integer multiple of the lowest base frequency\n\n\n\n\nI generally use the Harmonic Series in SuperCollider by setting a fundamental (base) frequency as a NodeProxy and referring other NodeProxies to it. This way all of the playing elements can follow the same fundamental frequency, and the fundamental frequency can be modulated.\n\n\n//Harmonic series\n//setting up a fundamental frequency as a NodeProxy so that it can be referenced on the fly\n(\n~r = {75}\n)\n//a straight run up the harmonic series to 10 partials. Notice how the notes converge the higher up the harmonic series due out perception of frequency being logarithimic\n//note that the \\freq argument is a multiplation of a Pkr - a BenoitLib addon which references an active NodeProxy inside of a pattern\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n//modulate the fundamental frequency to modulate the entire scale\n(\n~r = {SinOsc.kr(0.1).range(75,80)}\n)\n//raise the fundamental freqency from 75Hz to 1000Hz over two minutes\n(\n~r = {XLine.kr(75,1000,120)}\n)\n\n\n\n\nThe 'sound' of the harmonic series is different to scales, as the further up the harmonic series is played (or the more times the fundamental frequency is multiplied), the closer the intervals 'sound' to each other:\n\n\n//a run up the harmonic series from 1 to 50 partials - note how close together the notes become\n(\n~r = {50};\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..50),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n\n\n\n\nThis can be changed by changing the granularity of the multiplication of the fundamental frequency:\n\n\n//Multiple identical harmonic frequency riffs that use a different multiplication of the fundamental frequency\n(\n~r = {50};\n//1x fundamental\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n(\n//2x fundamental\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*2),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb2.play;\n)\n(\n//4x fundamental\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*4),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb3.play;\n)\n(\n//8x fundamental\n~sinfb4 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*8),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb4.play;\n)\n//all together to 30:\n(\n~r = {50};\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*2),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*4),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb4 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*8),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n)", 
            "title": "Types of Pitch Arrangement"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#types-of-pitch-arrangement", 
            "text": "", 
            "title": "Types of Pitch Arrangement"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#majorminor-scales", 
            "text": "The bedrock of the western musical canon is major and minor scales. Generally it's taught in British music education that the major scale is a ' happy ' sound and the minor scale is a ' serious ' or ' sad '. Generally though Minor tends to be used in most music I hear on a day-to-day basis, so if I'm going to be drawing on standard musical scale I will use that. For information on what they are from a music theory perspective check  this article .  A few good chords to use that will work with the Major and Minor scale very well at any point will be the following:  //chords I, IV and V\n//in Major and Minor - re-evaluate for a different scale (using the .choose method)\n(\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(4.5,7.0,inf),\n    \\scale,[Scale.minor,Scale.major].choose,\n    \\degree,Pwrand([[0,2,4],[3,5,7],[4,6,8]],[0.5,0.25,0.25],inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,6,\n    \\lsf,1000,\n    \\octave,Pwrand([4,3,5],[0.6,0.3,0.1],inf),\n    \\amp,Pwhite(0.8,2.0),\n    \\out,0);\n~chords.play;\n);  The chords I, IV and V are fundamental parts of the vast majority of chord progressions in major or minor scales, with chord ii also being very common. If you randomly play these four chords over a random melody of the same (major/minor) scale, it'll sound  pretty  good:  //major/minor scale chords with a fairly melody which meanders around the major/minor scale, but sounds consonant at the vast majority of points\n//scale stored in a dictionary key so that it can be used in both Pbinds easily\n(\nd[\\scale] = [Scale.major,Scale.minor].choose;\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(4.5,7.0,inf),\n    \\scale,d[\\scale],\n    \\degree,Pwrand([[0,2,4],[3,5,7],[4,6,8]],[0.5,0.25,0.25],inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,6,\n    \\lsf,1000,\n    \\octave,Pwrand([4,3,5],[0.6,0.3,0.1],inf),\n    \\amp,Pwhite(0.8,2.0),\n    \\out,0);\n~chords.play;\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\root,0,\\octave,[4,5],\\degree,Place([0,0,2,[4,5,6],[7,1,2],[6,7,8,9],[10,12,14,15],7,6,5],inf),\\dur,Pbjorklund2(Pwhite(6,8),8)/4,\\amp,0.4,\\fb,0.9,\\rel,0.2);\n~sinfb.play\n);  The Major and Minor  Pentatonic scales  are also good for 'sounding good', and are very popular on Guitar for easily creating solo lines.", 
            "title": "Major/Minor scales"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#chordsymbol-chord-notation-in-supercollider", 
            "text": "If you have a specific set of chords you would like to play using Patterns, the  ChordSymbol addon  by triss is a great way to do this, with the chords in arrays I specified in the previous section replaced by a dictionary of chord names, which are automatically translated into their note values. This is very useful if you're working with an instrumentalist and you're not too quick in translating numbers to named chords (which I am not)  //ChordProg - house chords with chord names in an array to make a chord sequence...\n//Today is gonna be the day that they're gonna throw it back to you...\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.chromatic,\\octave,4,\\degree,Pseq([\\Em7,\\G,\\Dsus4,\\A7sus4].chordProg,inf).stutter(6),\\dur,1,\\atk,0.8,\\amp,0.3,\\fb,0.1,\\rel,1);\n~sinfb.play\n)\n\n//giant steps. Apparently.\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.chromatic,\\octave,4,\\degree,Pseq([\\Bmajor7,\\D7,\\Gmajor7,\\Bb7,\\Ebmajor7,\\Am7,\\D7,\\Gmajor7,\\Bb7,\\Ebmajor7,\\Gb7,\\Bmajor7,\\Fm7,\\Bb7,\\Ebmajor7,\\Am7,\\D7,\\Gmajor7,\\Dbm7,\\Gb7,\\Bmajor7,\\Fm7,\\Bb7,\\Ebmajor7,\\Dbm7,\\Gbm7].chordProg,inf),\\dur,1,\\atk,0.1,\\amp,0.3,\\fb,0.1,\\rel,1);\n~sinfb.play;\n)\n\n//a musical example in context - Adapted from a set for Manchester Algorave\n(\np.clock.tempo = 180/60;\n~chords = Pbind(\\instrument,\\bpfsaw,\n    \\dur,Pwhite(9.5,15.0,inf),\n    \\scale,Scale.chromatic,\n    \\degree,Pxrand([\\Em,\\Am7,\\Bm7].chordProg,inf),\n    \\cfmin,100,\n    \\cfmax,1500,\n    \\detune,Pexprand(0.0001,1),\n    \\rqmin,Pexprand(0.02,0.15,inf),\n    \\atk,Pwhite(2.0,4.5,inf),\n    \\rel,Pwhite(6.5,10.0,inf),\n    \\ldb,13,\n    \\lsf,1000,\n    \\octave,Pwrand([4,5,6],[0.8,0.15,0.05],inf),\n    \\amp,Pwhite(0.8,1.5),\n    \\out,0);\n~chords.play;\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/4,\\amp,0.4,\\pan,0.2,\\rate,Pwhite(1.7,2));\n~t = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/4,\\amp,0.8,\\pan,-0.2,\\rate,2);\n~t2 = Pbind(\\instrument,\\bplay,\\buf,d[ t ][0],\\dur,Pbjorklund2(Pwhite(10,16),16)/4,\\amp,0.8,\\pan,-0.2,\\rate,4);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,Pbjorklund2(Pwrand([3,6],[0.8,0.2],inf),8)/4,\\amp,1);\n~c = Pbind(\\instrument,\\bplay,\\buf,d[ c ][0],\\dur,4,\\amp,4);\n~oh.play;~t.play;~k.play;~c.play;~t2.play;\n)", 
            "title": "ChordSymbol - chord notation in SuperCollider"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#chromatic-scales", 
            "text": "", 
            "title": "Chromatic Scales"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#microtonalalternative-scales", 
            "text": "SuperCollider has a bunch of built-in scales (which can be found by evaluating  Scale.directory ), all of which can be used in patterns by using them as part of the  \\scale  argument.  //Alternative scales\n//Evaluate to select a scale using the ET12 tuning and run it in ascending order, there are a number of scales so evaluate this a bunch of times\n//scales are stored in a dictionary to be referred to multiple times within the ~sinfb pbind\n(\np.clock.tempo = 1;\nd[\\scale] = Scale.choose.postln;\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,4,\\degree,Pseq((0..d[\\scale].degrees.size-1),inf),\\dur,0.25,\\amp,0.3,\\fb,0.6,\\rel,0.3);\n~sinfb.play;\n)\n\n//Microtonal scales\n(\np.clock.tempo = 1;\nd[\\scale] = [Scale.zamzam,Scale.chromatic24,Scale.partch_o1,Scale.husseini,Scale.zanjaran,Scale.bhairav].choose.postln;\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,4,\\degree,Pseq((0..d[\\scale].degrees.size-1),inf),\\dur,0.25,\\amp,0.3,\\fb,0.6,\\rel,0.3);\n~sinfb.play;\n)", 
            "title": "Microtonal/Alternative scales"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#alternative-tunings", 
            "text": "SuperCollider also has a bunch of built-in tunings (which can be found by evaluating  Tuning.directory ). These are specified as part of the  scale  argument after the scale that is used.  //Alternative Tunings\n//Chromatic scale in a random tuning - some relatively subtle differences here\n(\np.clock.tempo = 1;\nd[\\scale] = Scale.chromatic(Tuning.choose);\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,4,\\degree,Pseq((0..d[\\scale].degrees.size-1),inf),\\dur,0.25,\\amp,0.3,\\fb,0.6,\\rel,0.3);\n~sinfb.play;\n)\n\n//A musical example of alternative tunings\n//one of my favourites is the et53 tuning, using it to slightly disturb a central pitch on multiple instruments, sounds really nice in acid-type music\n//by selectively deploying et53, a very narrow pitch range can become normal, making large pitch leaps within an octave seem huge when used.\n(\np.clock.tempo = 150/60;\nd[\\scale] = Scale.chromatic(\\et53);\nl = Pbjorklund2(Pwhite(1,13),16)/4;\n//notice the \\degree argument - ranges from -8 to +8, but this difference is nowhere near an octave\n~ring3 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-8,8),\\octave,Pwrand([2,3],[0.8,0.2],inf),\\dur,l,\\d,0.24,\\a,Pexprand(10,400),\\pan,0,\\amp,1.5);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][1],\\dur,l,\\amp,0.8);\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][1],\\dur,l,\\amp,0.8);\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][1],\\dur,1,\\amp,2);\n~ring3.play;~sn.play;~h.play;~k.play;\n)\n//adding more acid lines which diverge even less. Also adding percussion\n(\n~ring2 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-4,4),\\octave,5,\\dur,l,\\d,0.37,\\a,Pexprand(1,40),\\pan,1,\\amp,0.5);\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-4,4),\\octave,4,\\dur,l,\\d,0.38,\\a,Pexprand(1,40),\\pan,-1,\\amp,0.5);\n~ring2.play;~ring1.play;\n)\n//another acid line that diverges quite a bit. also open hats\n(\n~oh = Pbind(\\instrument,\\bplay,\\buf,d[ oh ][1],\\dur,Pseq([0.5,Pseq([1],inf)],inf),\\amp,2);\n~oh.play;\n~ring4 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,d[\\scale],\\degree,Pwhite(-8,8),\\octave,7,\\dur,l,\\d,0.21,\\a,Pexprand(1,100),\\pan,1,\\amp,0.8);\n~ring4.play;\n)\n//repetive distorted \\sinfb riff, using the whole octave\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,d[\\scale],\\octave,[5,6],\\degree,Place([0,0,-52,[30,20,10],[52,40,25,20],[10,11,9,3,6],[30,36,39,40]],inf),\\dur,0.25,\\amp,0.5,\\fb,Pwhite(10.5,900.5),\\rel,Pexprand(0.1,0.5));\n~sinfb.play;\n)\n//remove percussion\n(\n~k.stop;~sn.stop;~h.stop;\n)", 
            "title": "Alternative tunings"
        }, 
        {
            "location": "/4-2-Types-of-Pitch-Arrangement/#harmonic-overtone-series", 
            "text": "From  Wikipedia :   A harmonic series is the sequence of sounds where the base frequency of each sound is an integer multiple of the lowest base frequency   I generally use the Harmonic Series in SuperCollider by setting a fundamental (base) frequency as a NodeProxy and referring other NodeProxies to it. This way all of the playing elements can follow the same fundamental frequency, and the fundamental frequency can be modulated.  //Harmonic series\n//setting up a fundamental frequency as a NodeProxy so that it can be referenced on the fly\n(\n~r = {75}\n)\n//a straight run up the harmonic series to 10 partials. Notice how the notes converge the higher up the harmonic series due out perception of frequency being logarithimic\n//note that the \\freq argument is a multiplation of a Pkr - a BenoitLib addon which references an active NodeProxy inside of a pattern\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n//modulate the fundamental frequency to modulate the entire scale\n(\n~r = {SinOsc.kr(0.1).range(75,80)}\n)\n//raise the fundamental freqency from 75Hz to 1000Hz over two minutes\n(\n~r = {XLine.kr(75,1000,120)}\n)  The 'sound' of the harmonic series is different to scales, as the further up the harmonic series is played (or the more times the fundamental frequency is multiplied), the closer the intervals 'sound' to each other:  //a run up the harmonic series from 1 to 50 partials - note how close together the notes become\n(\n~r = {50};\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..50),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)  This can be changed by changing the granularity of the multiplication of the fundamental frequency:  //Multiple identical harmonic frequency riffs that use a different multiplication of the fundamental frequency\n(\n~r = {50};\n//1x fundamental\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb.play;\n)\n(\n//2x fundamental\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*2),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb2.play;\n)\n(\n//4x fundamental\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*4),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb3.play;\n)\n(\n//8x fundamental\n~sinfb4 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*8),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb4.play;\n)\n//all together to 30:\n(\n~r = {50};\n~sinfb = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*2),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*4),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb4 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..20),inf)*(Pkr(~r)*8),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n)", 
            "title": "Harmonic (overtone) series"
        }, 
        {
            "location": "/4-3-Riffs/", 
            "text": "Riffs\n\n\n\n\nExamples in music\n\n\nA \nriff\n is a short, repeated musical phrase that is used as an anchor or a refrain in a piece of music.\n\n\nI've always been drawn to guitar music with \nriffs\n, and riff-heavy electronic music \nis\n \nno\n \nexception\n. A \ngreat\n example of riff-heavy live coding is the music of \nBelisha Beacon's\n, who makes a network of shifting riffs using \nixi lang\n.\n\n\nHere are a few ways I use riffs\n\n\nThe 'up-down' riff\n\n\nA technique I've probably ended up using an awful lot is an 'up-down' riff, which is a way of producing a set of interlocking riffs very quickly on the spot. It can be used with any form of pitch organisation, but more common scales and the harmonic series tend to work the best.\n\n\nThe 'up-down' riff uses SuperCollider's \nrange\n method to generate a sequential set of degrees of a scale playing on a SynthDef and running it alongside the same set of degrees \n.reverse\n-d, creating a palindrome which runs continuously. A third layer, which uses SuperCollider's \n.scramble\n method to create a random riff to play against the 'up-down' riff, all played in a uniform rhythm:\n\n\n//up-down riff\n//harmonic series version\n//re-evaluate individual directions to create a different riff\n(\n//up\np.clock.tempo = 1.5;\n~r = {75};\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.3);\n~sinfb1.play;\n)\n(\n//down\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).reverse,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.3);\n~sinfb2.play;\n)\n(\n//random\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).scramble,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.0),\\rel,0.3);\n~sinfb3.play;\n)\n\n//up-down riff\n//minor scale version\n//re-evaluate individual directions to create a different riff\n(\np.clock.tempo = 1.5;\n//up\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7),inf),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb1.play;\n)\n(\n//down\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7).reverse,inf),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb2.play;\n)\n(\n//random, an octave higher\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,6,\\degree,Pseq((0..7).scramble,inf),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.0),\\rel,0.2);\n~sinfb3.play;\n)\n\n\n\n\n\nAn important part of this technique is that by re-evaluating individual riffs the overall structure of the riffs as a whole can be changed, giving the resulting sound a different character each time.\n\n\nIt can also be combined with some \nPwrand\n based probabilistic rhythmic change to automatically shift the character of the riff:\n\n\n//replacing duration of 0.25 with a Pwrand which will automatically shift the riffs\n(\np.clock.tempo = 1.5;\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7),inf),\\dur,Pwrand([0.25,Pseq([0.125],2)],[0.9,0.1],inf),\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7).reverse,inf),\\dur,Pwrand([0.25,Pseq([0.125],2)],[0.9,0.1],inf),\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7).scramble,inf),\\dur,Pwrand([0.25,Pseq([0.125],2)],[0.9,0.1],inf),\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.2);\n)\n~sinfb1.play;\n~sinfb2.play;\n~sinfb3.play;\n\n\n\n\n\n'Phasing'\n\n\n'Phasing' was used extensively by \nSteve\n \nReich\n in his early works, and refers to two or more similar or identical musical forms which are played at slightly differing tempi so that they shift and begin to interfere with each other (\nmore information\n).\n\n\nThere are a few ways to emulate this during sets, both through subtle interference with playing riffs, rhythmic disturbances and omitting notes. Another example can be seen in the section on Euclidean Rhythms and Offsets.\n\n\n//Phasing\n//Using the riff from Reich's Piano Phase\n//inspired by https://ccrma.stanford.edu/courses/tu/cm2008/topics/piano_phase/index.shtml\n(\np.clock.tempo = 1.8;\n//riff 1 and 2 evaluated at once so that they start together.\n//riff 2 will sometimes play 0.125 duration which will knock the two out of phase\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,Pwrand([0.25,0.125],[0.99,0.01],inf),\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb1.play;\n)\n//play riff 2\n~sinfb2.play;\n\n//another version which uses a second riff which has a slightly different tempo constantly\n(\np.clock.tempo = 1.8;\n//riff 1 and 2 evaluated at once so that they start together.\n//riff 2 will sometimes play 0.125 duration which will knock the two out of phase\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,0.25,\\amp,0.3,\\fb,0.8,\\rel,0.3);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,0.255,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb1.play;\n)\n//play riff 2\n~sinfb2.play;\n\n\n\n\nSample stabs\n\n\nAnother way to make riffs is to use pitched samples, and define the pitch of the riff using the \n\\rate\n argument of \nbplay\n.\n\n\nA version of this I use quite a lot is derived from '90s rave music:\n\n\n//synth stabs - try this with both stab 0 and 1.\n(\n//stab 1\np.clock.tempo = 2.4;\n~stab1 = Pbind(\\instrument,\\bplay,\\buf,d[\nstab\n][1],\\euclidNum,Pwhite(3,3),\\dur,Pbjorklund2(Pkey(\\euclidNum),8)/4,\\amp,2,\\rate,Pseq([1,1,1,1,1,1,0.9,1.1],inf).stutter(3));\n~stab1.play;\n)\n(\n//stab 2 - double speed and greater possible number of onsets\n~stab2 = Pbind(\\instrument,\\bplay,\\buf,d[\nstab\n][1],\\euclidNum,Pwhite(3,11),\\dur,Pbjorklund2(Pkey(\\euclidNum),16)/4,\\amp,1,\\rate,Pseq([1,1,1,1,1,1,0.9,1.1],inf).stutter(3)*2);\n~stab2.play;\n)\n(\n//stab 3 - double speed again and greater possible number of onsets again\n~stab3 = Pbind(\\instrument,\\bplay,\\buf,d[\nstab\n][1],\\euclidNum,Pwhite(6,16),\\dur,Pbjorklund2(Pkey(\\euclidNum),16)/4,\\amp,1,\\rate,Pseq([1,1,1,1,1,1,0.9,1.1],inf).stutter(3)*4);\n~stab3.play;\n)\n//drums\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1.1,1.9],inf));\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1.1,1.9],inf)*2);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[\ns\n][0],\\dur,Pbjorklund2(Pwhite(1,6),16)/4,\\amp,1);\n~fx = Pbind(\\instrument,\\bplay,\\buf,d[\nfx\n][0],\\dur,Pwhite(1,6),\\amp,1);\n~k.play;~sn.play;~fx.play;~k2.play;\n)\n\n\n\n\n\nPlace and compound riffs\n\n\nPlace\n is 'interlaced embedding of subarrays'. Simply put, if you put a riff inside of another riff (or an array inside of another array), the first level of the array will be played over, and each subsequent value of the subarrays will be iterated over once every time the first level is played. This is really difficult to explain, so have a look at the first numerical example of the Place documentation for this one. Here is an example of how two riffs can be layered together using Place:\n\n\n//Place - riffs that contain riffs\n(\n//first riff\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Place([0,7],inf),\\octave,3,\\dur,0.25,\\d,0.6,\\a,Pseq((1..40),inf),\\pan,0,\\amp,0.5);\n~ring1.play;\n)\n//stop\n~ring1.stop;\n(\n//second riff\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Place([2,4,3,5,4,6,8,11],inf),\\octave,3,\\dur,0.25,\\d,0.6,\\a,Pseq((1..40),inf),\\pan,0,\\amp,0.5);\n~ring1.play;\n)\n//stop\n~ring1.stop;\n(\n//two riffs laced together with the longer one on the inner level, playing the first riff and then a note of the second\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Place([0,7,[2,4,3,5,4,6,8,11]],inf),\\octave,3,\\dur,0.25,\\d,0.6,\\a,Pseq((1..40),inf),\\pan,0,\\amp,0.5);\n~ring1.play\n)", 
            "title": "Riffs"
        }, 
        {
            "location": "/4-3-Riffs/#riffs", 
            "text": "", 
            "title": "Riffs"
        }, 
        {
            "location": "/4-3-Riffs/#examples-in-music", 
            "text": "A  riff  is a short, repeated musical phrase that is used as an anchor or a refrain in a piece of music.  I've always been drawn to guitar music with  riffs , and riff-heavy electronic music  is   no   exception . A  great  example of riff-heavy live coding is the music of  Belisha Beacon's , who makes a network of shifting riffs using  ixi lang .  Here are a few ways I use riffs", 
            "title": "Examples in music"
        }, 
        {
            "location": "/4-3-Riffs/#the-up-down-riff", 
            "text": "A technique I've probably ended up using an awful lot is an 'up-down' riff, which is a way of producing a set of interlocking riffs very quickly on the spot. It can be used with any form of pitch organisation, but more common scales and the harmonic series tend to work the best.  The 'up-down' riff uses SuperCollider's  range  method to generate a sequential set of degrees of a scale playing on a SynthDef and running it alongside the same set of degrees  .reverse -d, creating a palindrome which runs continuously. A third layer, which uses SuperCollider's  .scramble  method to create a random riff to play against the 'up-down' riff, all played in a uniform rhythm:  //up-down riff\n//harmonic series version\n//re-evaluate individual directions to create a different riff\n(\n//up\np.clock.tempo = 1.5;\n~r = {75};\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.3);\n~sinfb1.play;\n)\n(\n//down\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).reverse,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.3);\n~sinfb2.play;\n)\n(\n//random\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).scramble,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.0),\\rel,0.3);\n~sinfb3.play;\n)\n\n//up-down riff\n//minor scale version\n//re-evaluate individual directions to create a different riff\n(\np.clock.tempo = 1.5;\n//up\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7),inf),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb1.play;\n)\n(\n//down\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7).reverse,inf),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb2.play;\n)\n(\n//random, an octave higher\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,6,\\degree,Pseq((0..7).scramble,inf),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.0),\\rel,0.2);\n~sinfb3.play;\n)  An important part of this technique is that by re-evaluating individual riffs the overall structure of the riffs as a whole can be changed, giving the resulting sound a different character each time.  It can also be combined with some  Pwrand  based probabilistic rhythmic change to automatically shift the character of the riff:  //replacing duration of 0.25 with a Pwrand which will automatically shift the riffs\n(\np.clock.tempo = 1.5;\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7),inf),\\dur,Pwrand([0.25,Pseq([0.125],2)],[0.9,0.1],inf),\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7).reverse,inf),\\dur,Pwrand([0.25,Pseq([0.125],2)],[0.9,0.1],inf),\\amp,0.3,\\fb,Pwhite(0.1,0.4),\\rel,0.2);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,5,\\degree,Pseq((0..7).scramble,inf),\\dur,Pwrand([0.25,Pseq([0.125],2)],[0.9,0.1],inf),\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.2);\n)\n~sinfb1.play;\n~sinfb2.play;\n~sinfb3.play;", 
            "title": "The 'up-down' riff"
        }, 
        {
            "location": "/4-3-Riffs/#phasing", 
            "text": "'Phasing' was used extensively by  Steve   Reich  in his early works, and refers to two or more similar or identical musical forms which are played at slightly differing tempi so that they shift and begin to interfere with each other ( more information ).  There are a few ways to emulate this during sets, both through subtle interference with playing riffs, rhythmic disturbances and omitting notes. Another example can be seen in the section on Euclidean Rhythms and Offsets.  //Phasing\n//Using the riff from Reich's Piano Phase\n//inspired by https://ccrma.stanford.edu/courses/tu/cm2008/topics/piano_phase/index.shtml\n(\np.clock.tempo = 1.8;\n//riff 1 and 2 evaluated at once so that they start together.\n//riff 2 will sometimes play 0.125 duration which will knock the two out of phase\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,0.25,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,Pwrand([0.25,0.125],[0.99,0.01],inf),\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb1.play;\n)\n//play riff 2\n~sinfb2.play;\n\n//another version which uses a second riff which has a slightly different tempo constantly\n(\np.clock.tempo = 1.8;\n//riff 1 and 2 evaluated at once so that they start together.\n//riff 2 will sometimes play 0.125 duration which will knock the two out of phase\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,0.25,\\amp,0.3,\\fb,0.8,\\rel,0.3);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\octave,4,\\freq,Pseq([64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73].midicps,inf),\\dur,0.255,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~sinfb1.play;\n)\n//play riff 2\n~sinfb2.play;", 
            "title": "'Phasing'"
        }, 
        {
            "location": "/4-3-Riffs/#sample-stabs", 
            "text": "Another way to make riffs is to use pitched samples, and define the pitch of the riff using the  \\rate  argument of  bplay .  A version of this I use quite a lot is derived from '90s rave music:  //synth stabs - try this with both stab 0 and 1.\n(\n//stab 1\np.clock.tempo = 2.4;\n~stab1 = Pbind(\\instrument,\\bplay,\\buf,d[ stab ][1],\\euclidNum,Pwhite(3,3),\\dur,Pbjorklund2(Pkey(\\euclidNum),8)/4,\\amp,2,\\rate,Pseq([1,1,1,1,1,1,0.9,1.1],inf).stutter(3));\n~stab1.play;\n)\n(\n//stab 2 - double speed and greater possible number of onsets\n~stab2 = Pbind(\\instrument,\\bplay,\\buf,d[ stab ][1],\\euclidNum,Pwhite(3,11),\\dur,Pbjorklund2(Pkey(\\euclidNum),16)/4,\\amp,1,\\rate,Pseq([1,1,1,1,1,1,0.9,1.1],inf).stutter(3)*2);\n~stab2.play;\n)\n(\n//stab 3 - double speed again and greater possible number of onsets again\n~stab3 = Pbind(\\instrument,\\bplay,\\buf,d[ stab ][1],\\euclidNum,Pwhite(6,16),\\dur,Pbjorklund2(Pkey(\\euclidNum),16)/4,\\amp,1,\\rate,Pseq([1,1,1,1,1,1,0.9,1.1],inf).stutter(3)*4);\n~stab3.play;\n)\n//drums\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1.1,1.9],inf));\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,Pbjorklund2(3,8)/4,\\amp,1,\\rate,Pseq([1.1,1.9],inf)*2);\n~sn = Pbind(\\instrument,\\bplay,\\buf,d[ s ][0],\\dur,Pbjorklund2(Pwhite(1,6),16)/4,\\amp,1);\n~fx = Pbind(\\instrument,\\bplay,\\buf,d[ fx ][0],\\dur,Pwhite(1,6),\\amp,1);\n~k.play;~sn.play;~fx.play;~k2.play;\n)", 
            "title": "Sample stabs"
        }, 
        {
            "location": "/4-3-Riffs/#place-and-compound-riffs", 
            "text": "Place  is 'interlaced embedding of subarrays'. Simply put, if you put a riff inside of another riff (or an array inside of another array), the first level of the array will be played over, and each subsequent value of the subarrays will be iterated over once every time the first level is played. This is really difficult to explain, so have a look at the first numerical example of the Place documentation for this one. Here is an example of how two riffs can be layered together using Place:  //Place - riffs that contain riffs\n(\n//first riff\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Place([0,7],inf),\\octave,3,\\dur,0.25,\\d,0.6,\\a,Pseq((1..40),inf),\\pan,0,\\amp,0.5);\n~ring1.play;\n)\n//stop\n~ring1.stop;\n(\n//second riff\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Place([2,4,3,5,4,6,8,11],inf),\\octave,3,\\dur,0.25,\\d,0.6,\\a,Pseq((1..40),inf),\\pan,0,\\amp,0.5);\n~ring1.play;\n)\n//stop\n~ring1.stop;\n(\n//two riffs laced together with the longer one on the inner level, playing the first riff and then a note of the second\n~ring1 = Pbind(\\instrument,\\ring1,\\f,Pkey(\\freq),\\scale,Scale.minor,\\degree,Place([0,7,[2,4,3,5,4,6,8,11]],inf),\\octave,3,\\dur,0.25,\\d,0.6,\\a,Pseq((1..40),inf),\\pan,0,\\amp,0.5);\n~ring1.play\n)", 
            "title": "Place and compound riffs"
        }, 
        {
            "location": "/4-4-Pitch-and-Static-Synths/", 
            "text": "Pitch and 'Static Synths'\n\n\n\n\nOutside of patterns, pitch is handled primarily using the \nfreq\n argument of UGens - for example:\n\n\n~sin = {SinOsc.ar(440,0,0.1)};\n~sin.play;\n\n\n\n\nWith \n440\n being the frequency.\n\n\nThis \nfreq\n argument can easily be fitted to the harmonic series by using multiplication and the .range and .round methods applied to various Ugens:\n\n\n//set a fundamental frequency\n~f = {70}\n//a fixed pitch sine wave, using a fundamental frequency\n(\n~sin = {SinOscFB.ar([~f,~f*1.01],0.7,0.3)};\n~sin.play;\n)\n//4 saw waves that are modulated by LFNoise1 Ugens and arranged around the stereo field\n//the frequency of the saw waves is a LFNoise1 that is ranged between the fundamental and ten times the fundamental\n(\n~lfn1 = {Splay.ar(Saw.ar(Array.fill(4,{LFNoise1.kr(0.3).range(~f,~f*10)}),0.3))}\n~lfn1.play;\n)\n//now round this LFNoise1 to the fundamental frequency to get the frequency to sweep the harmonic frequency\n(\n~lfn1 = {Splay.ar(Saw.ar(Array.fill(4,{LFNoise1.kr(0.3).range(~f,~f*10).round(~f)}),0.3))}\n~lfn1.play;\n)\n//the frequencies are now tuned and sound GREAT (an X/Y scope also looks amazing)\ns.scope\n//This .range and .round method can be applied to any signal UGen, and also at any multiplication level. Here's a silly extreme example that sounds like shrill bees\n(\n~lfn1 = {Splay.ar(Saw.ar(Array.fill(40,{SinOscFB.kr(rrand(0.1,0.3),rrand(0.1,2)).range(~f,~f*100).round(~f*4)}),0.4))}\n~lfn1.play;\n)\n\n//Triggered random frequency changes, using something like TRand\n(\n~f = {81};\n~tChange = {Pulse.ar(TRand.kr(~f,~f*10,Dust.kr(4)).round(~f),SinOsc.kr(0.1).abs,0.6)*SinOsc.ar([~f,~f*1.01])};\n~tChange.play;\n)\n\n//specific and on-demand frequency changes using Demand.kr - Note that this is *really* verbose for something to be used live.\n//I've used an Impulse.kr that recieves the tempo clock as a trigger to show how these synths can be synced to a central tempo clock\n//Demand is a lot like having a Pattern inside of a UGen's arguments. Look at the helpfile, it's really cool\n(\n~f = {66.6};\n~dChange = {SawDPW.ar([~f,~f*1.02]*Demand.kr(Impulse.kr(p.clock.tempo*3),0,Dseq([1,8,2,7,3,6,4,5],inf)),SinOsc.kr(40),0.8)};\n~dChange.play;\n)\n//and a kick to show it's synced\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,1,\\amp,1);\n~k.play;\n)\n//and more kicks because i really liked this one\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][2],\\dur,Pbjorklund2(Pwhite(1,15),16)/6,\\amp,2,\\rate,Pwrand([1,1.2,1.4,2],[0.6,0.2,0.1,0.1],inf)*1.5);\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[\nsk\n][0],\\dur,1,\\amp,2);\n~k2.play;\n)\n\n\n\n\nScales are a bit of a pain to use outside of patterns, but it's possible using the \nScale\n class and its degreeToFreq method, although it is quite inflexible\n\n\n//Scale and DegreeToFreq\n//using the Demand example again\n//a fifth\n(\n~scale = {SinOscFB.ar(Scale.minor(\\just).degreeToFreq([0,4],48.midicps,1),0.7,0.2)};\n~scale.play;\n)\n//Note that the above does not allow scale notes to be changed once the synth is initiated\n~scale = {SinOscFB.ar(Scale.minor(\\just).degreeToFreq(TRand.kr(1,10,Impulse.kr(1)),48.midicps,1),0.7,0.2)};\n\n\n\n\n.midicps can also be used, if you know the MIDI note numbers of a scale that you want to play\n\n\n//using .midicps to determine pitch\n~scale = {SinOscFB.ar(TRand.kr(50,80,Impulse.kr([3,3.01])).midicps,0.7,0.5)};\n~scale.play", 
            "title": "Pitch and Static Synths"
        }, 
        {
            "location": "/4-4-Pitch-and-Static-Synths/#pitch-and-static-synths", 
            "text": "Outside of patterns, pitch is handled primarily using the  freq  argument of UGens - for example:  ~sin = {SinOsc.ar(440,0,0.1)};\n~sin.play;  With  440  being the frequency.  This  freq  argument can easily be fitted to the harmonic series by using multiplication and the .range and .round methods applied to various Ugens:  //set a fundamental frequency\n~f = {70}\n//a fixed pitch sine wave, using a fundamental frequency\n(\n~sin = {SinOscFB.ar([~f,~f*1.01],0.7,0.3)};\n~sin.play;\n)\n//4 saw waves that are modulated by LFNoise1 Ugens and arranged around the stereo field\n//the frequency of the saw waves is a LFNoise1 that is ranged between the fundamental and ten times the fundamental\n(\n~lfn1 = {Splay.ar(Saw.ar(Array.fill(4,{LFNoise1.kr(0.3).range(~f,~f*10)}),0.3))}\n~lfn1.play;\n)\n//now round this LFNoise1 to the fundamental frequency to get the frequency to sweep the harmonic frequency\n(\n~lfn1 = {Splay.ar(Saw.ar(Array.fill(4,{LFNoise1.kr(0.3).range(~f,~f*10).round(~f)}),0.3))}\n~lfn1.play;\n)\n//the frequencies are now tuned and sound GREAT (an X/Y scope also looks amazing)\ns.scope\n//This .range and .round method can be applied to any signal UGen, and also at any multiplication level. Here's a silly extreme example that sounds like shrill bees\n(\n~lfn1 = {Splay.ar(Saw.ar(Array.fill(40,{SinOscFB.kr(rrand(0.1,0.3),rrand(0.1,2)).range(~f,~f*100).round(~f*4)}),0.4))}\n~lfn1.play;\n)\n\n//Triggered random frequency changes, using something like TRand\n(\n~f = {81};\n~tChange = {Pulse.ar(TRand.kr(~f,~f*10,Dust.kr(4)).round(~f),SinOsc.kr(0.1).abs,0.6)*SinOsc.ar([~f,~f*1.01])};\n~tChange.play;\n)\n\n//specific and on-demand frequency changes using Demand.kr - Note that this is *really* verbose for something to be used live.\n//I've used an Impulse.kr that recieves the tempo clock as a trigger to show how these synths can be synced to a central tempo clock\n//Demand is a lot like having a Pattern inside of a UGen's arguments. Look at the helpfile, it's really cool\n(\n~f = {66.6};\n~dChange = {SawDPW.ar([~f,~f*1.02]*Demand.kr(Impulse.kr(p.clock.tempo*3),0,Dseq([1,8,2,7,3,6,4,5],inf)),SinOsc.kr(40),0.8)};\n~dChange.play;\n)\n//and a kick to show it's synced\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,1,\\amp,1);\n~k.play;\n)\n//and more kicks because i really liked this one\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][2],\\dur,Pbjorklund2(Pwhite(1,15),16)/6,\\amp,2,\\rate,Pwrand([1,1.2,1.4,2],[0.6,0.2,0.1,0.1],inf)*1.5);\n~k2 = Pbind(\\instrument,\\bplay,\\buf,d[ sk ][0],\\dur,1,\\amp,2);\n~k2.play;\n)  Scales are a bit of a pain to use outside of patterns, but it's possible using the  Scale  class and its degreeToFreq method, although it is quite inflexible  //Scale and DegreeToFreq\n//using the Demand example again\n//a fifth\n(\n~scale = {SinOscFB.ar(Scale.minor(\\just).degreeToFreq([0,4],48.midicps,1),0.7,0.2)};\n~scale.play;\n)\n//Note that the above does not allow scale notes to be changed once the synth is initiated\n~scale = {SinOscFB.ar(Scale.minor(\\just).degreeToFreq(TRand.kr(1,10,Impulse.kr(1)),48.midicps,1),0.7,0.2)};  .midicps can also be used, if you know the MIDI note numbers of a scale that you want to play  //using .midicps to determine pitch\n~scale = {SinOscFB.ar(TRand.kr(50,80,Impulse.kr([3,3.01])).midicps,0.7,0.5)};\n~scale.play", 
            "title": "Pitch and 'Static Synths'"
        }, 
        {
            "location": "/4-5-Between-Pitch-And-Noise/", 
            "text": "Between Pitch and Noise\n\n\n\n\nPreamble\n\n\nAn important corollary when talking about pitch is to talk about unpitched sound or noise. In periods of music dominated by pitched sounds, disintegration or erosion of pitch into noise can be an important technique to drive a set forward, or just provide sonic interest. I find a lot of this kind of thing in the transformations of instruments within \nHolden's Music\n for example. Here are some techniques to achieve this.\n\n\nSinOscFB\n\n\nA Ugen I use a lot (read: far too much) is \nSinOscFB\n, a 'sine oscillator that has phase modulation feedback'. I've always been a big fan of bare sine waves, and SinOscFB's \nfeedback\n argument allows a sine wave to be modulated into noise and back very easily, with extreme modulations creating a strange-sounding degraded sine wave.\n\n\n//SinOscFB - A sine wave that can move between pitch and noise and noisy pitch\n(\n//polling the modulation of the 'feedback' argument, to show the way in which SinOscFB degrades sine waves\n~sinfbstatic = {SinOscFB.ar([330,440],XLine.kr(0.1,500,60).poll(10),0.6)};\n~sinfbstatic.play;\n)\n\n\n\n\nA stalwart of my SynthDef arsenal is \nsinfb\n, a \nSinOscFB\n Ugen inside of an \nEnv.perc\n which is used to control its amplitude curve. This SynthDef is very flexible -  great for basses, melodies and chords, but also great for flexibly turning melodic riffs into textural noise, as well as blending the two. Notice that from values \n0.0\n to \n20.0\n there is a full spectrum from clean sine wave to noise feedback, any values above \n30.0\n will blend the two and are what I would consider 'extreme modulation'. In general usage during sets I tend to use the range 0.0 to 3.0, as anything above tends to be too noisy and interferes with the percussion i'm using.\n\n\n//a pattern I use regularly with its feedback being modulated from 0 to 20. Notice the difference in sound across the spectrum\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,[3,4,5],\\degree,Pseq([0,0,4,5],inf),\\dur,Pbjorklund2(3,8)/4,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~feedback = {SinOsc.kr(0.1,-1,1).range(0,20.0).poll(30)};\n~sinfb.set(\\fb,~feedback);\n~sinfb.play;\n)\n\n\n\n\nHarmonic series and extreme pitch values\n\n\nIn 4.2 I talked about the Harmonic Series. An interesting quality of using a fundamental frequency to determine the pitch of various NodeProxies by multiplying that fundamental frequency to create a scale structure.\n\n\nSome interesting techniques for distorting this harmonic series technique into the territory of noise are extreme modulation, which pushes the frequency into supersonics (and sometimes back again):\n\n\n//Extreme modulation of fundamental frequency\n//taking the up-down scale given in the 'riffs' section\n(\n//up\np.clock.tempo = 2.4;\n~r = {75};\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).reverse,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).scramble,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,2.0),\\rel,0.1);\n~sinfb1.play;~sinfb2.play;~sinfb3.play;\n)\n//moving the frequency up and beyond sensible into supersonics - after reading around 5000Hz some interesting aliasing starts to happen\n(\n~r = {XLine.kr(75,8000,60).poll(10)}\n)\n//and even further, lower frequencies start reappearing\n(\n~r = {XLine.kr(8000,30000,60).poll(10)};\n)\n//using very extreme modulation also gives some interesing results\n(\n~r = {LFNoise1.kr(0.2).range(30000,90000).poll(10)};\n)\n\n\n\n\nAnd extreme pitch values - which appear to rise continually into supersonic frequencies and aliasing, and then looping back to the bottom of the pitch scale:\n\n\n//extreme multiplaction of fundamental frequency\n//using the previous example, a NodeProxy holding a second multiplier is added onto the \\freq argument of each Pbind\n(\n~r = {75};\n~mult = {1};\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*(Pkr(~r)*Pkr(~mult)),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).reverse,inf)*(Pkr(~r)*Pkr(~mult)),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).scramble,inf)*(Pkr(~r)*Pkr(~mult)),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,2.0),\\rel,0.1);\n~sinfb1.play;~sinfb2.play;~sinfb3.play;\n)\n//increase the multiplcation over time using a .round on a Line.kr UGen. Listen to how the scale is distorted as the multiplcation increases, eventually ending as a series of pulses\n(\n~mult = {Line.kr(1,60,60).round(1).poll(5)}\n)\n\n\n\n\nChaos UGens\n\n\nSuperCollider has support for UGens that use \nChaos Theory\n for synthesis - the \nChaos UGens\n (note that there are also a number of additional Chaos UGens in \nsc3-plugins\n which are worth having).\n\n\nWhile (at the time of writing) I don't know a whole lot about the particularities of chaos theory works, but the Chaos UGens are great for creating musical structures that move freely between pitched sound and noise, and these are usually handled both in the equation variables of the UGens as well as the initial conditions.\n\n\nI'll use HenonN as an example of the use of chaos theory to move between melody, noise and percussion:\n\n\n//HenonN - Chaos synths and moving between pitch and noise\n(\n//henon using a minor pentatonic scale at a high octave.\n//The chaos Ugens will need some experimentations if you want subtle variance in sound\n//For Henon I found that an a value of 1.3 and a b value of 0.3 renders a pitch in a pattern pretty reliably\n//note that the pitches aren't quite the same as 'concert pitch'\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.3),\\b,Pexprand(0.3,0.3),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n~henon.play;\n)\n//increase the variation in the a and b arguments to add more noise to the mix\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.31),\\b,Pexprand(0.3,0.31),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n)\n//notice that this gets very noisy VERY fast.\n//adding a little more possiblity to the Pexprands in a and b turns it into pure noise very very fast, while still retaining a little of its pitched character\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.35),\\b,Pexprand(0.3,0.35),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n)\n//even more and noises become cut off and non-sounding.\n//the cut off sounds would sound as DC bias, but the SynthDef \\henon has a LeakDC on its output to prevent this as it can damage sound systems and is generally quite an unpleasant thing to deal with.\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.45),\\b,Pexprand(0.3,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n)\n//at this point decreasing the \\dur and \\rel value turns it into rhythmic percussion\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,0.25,\\a,Pexprand(1.3,1.45),\\b,Pexprand(0.3,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.01,0.1),\\amp,1);\n)\n//more extreme possible values - \\dur varied, octaves doubled up, more variation in a and b values, more octaves\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,[8,12,9,10],\\dur,Pwrand([0.25,Pbjorklund2(Pwhite(3,5),8,1)/4,Pseq([0.125],4)],[7,4,1].normalizeSum,inf),\\a,Pexprand(1.2,1.55),\\b,Pexprand(0.21,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.01,0.6),\\amp,1);\n)\n//against a kick drum it takes on a really strange character\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nk\n][0],\\dur,1,\\amp,1);\n~k.play;\n)\n\n\n\n\nA thing to note about the Chaos synths is the type of interpolation used - taking Henon as an example; HenonC, HenonL and HenonN stand for Cubic, Linear and None respectively. The sonic effect of the type of interpolation used is in the 'smoothness' of the sound, with Cubic being the most smooth and None being the least.\n\n\n//sound of different types of interpolation\n//the default in my SynthDefs.scd file is currently to use none:\n(\nSynthDef(\\henon,\n    {arg freq=440,mfreq=440,a=1.3,b=0.3,x0=0.30501993062401,y0=0.20938865431933,atk=0.01,sus=1,rel=1,ts=1,out=0,pan=0,amp=0.3;\n        var sig,env;\n        sig = Henon2DN.ar(freq,freq+mfreq,a,b,x0,y0,amp);\n        env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,ts,2);\n        sig = LeakDC.ar(sig);\n        sig = sig*env;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n);\n//the example earlier, with no interpolation (default)\n(\np.clock.tempo = 2.2;\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,[8,12,9,10],\\dur,Pwrand([0.25,Pbjorklund2(Pwhite(3,5),8,1)/4,Pseq([0.125],4)],[7,4,1].normalizeSum,inf),\\a,Pexprand(1.2,1.55),\\b,Pexprand(0.21,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.01,0.6),\\amp,1);\n~henon.play;\n)\n//now with Linear interpolation\n(\nSynthDef(\\henon,\n    {arg freq=440,mfreq=440,a=1.3,b=0.3,x0=0.30501993062401,y0=0.20938865431933,atk=0.01,sus=1,rel=1,ts=1,out=0,pan=0,amp=0.3;\n        var sig,env;\n        sig = Henon2DL.ar(freq,freq+mfreq,a,b,x0,y0,amp);\n        env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,ts,2);\n        sig = LeakDC.ar(sig);\n        sig = sig*env;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n);\n//now with Cubic interpolation\n(\nSynthDef(\\henon,\n    {arg freq=440,mfreq=440,a=1.3,b=0.3,x0=0.30501993062401,y0=0.20938865431933,atk=0.01,sus=1,rel=1,ts=1,out=0,pan=0,amp=0.3;\n        var sig,env;\n        sig = Henon2DC.ar(freq,freq+mfreq,a,b,x0,y0,amp);\n        env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,ts,2);\n        sig = LeakDC.ar(sig);\n        sig = sig*env;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n);", 
            "title": "Between Pitch and Noise"
        }, 
        {
            "location": "/4-5-Between-Pitch-And-Noise/#between-pitch-and-noise", 
            "text": "", 
            "title": "Between Pitch and Noise"
        }, 
        {
            "location": "/4-5-Between-Pitch-And-Noise/#preamble", 
            "text": "An important corollary when talking about pitch is to talk about unpitched sound or noise. In periods of music dominated by pitched sounds, disintegration or erosion of pitch into noise can be an important technique to drive a set forward, or just provide sonic interest. I find a lot of this kind of thing in the transformations of instruments within  Holden's Music  for example. Here are some techniques to achieve this.", 
            "title": "Preamble"
        }, 
        {
            "location": "/4-5-Between-Pitch-And-Noise/#sinoscfb", 
            "text": "A Ugen I use a lot (read: far too much) is  SinOscFB , a 'sine oscillator that has phase modulation feedback'. I've always been a big fan of bare sine waves, and SinOscFB's  feedback  argument allows a sine wave to be modulated into noise and back very easily, with extreme modulations creating a strange-sounding degraded sine wave.  //SinOscFB - A sine wave that can move between pitch and noise and noisy pitch\n(\n//polling the modulation of the 'feedback' argument, to show the way in which SinOscFB degrades sine waves\n~sinfbstatic = {SinOscFB.ar([330,440],XLine.kr(0.1,500,60).poll(10),0.6)};\n~sinfbstatic.play;\n)  A stalwart of my SynthDef arsenal is  sinfb , a  SinOscFB  Ugen inside of an  Env.perc  which is used to control its amplitude curve. This SynthDef is very flexible -  great for basses, melodies and chords, but also great for flexibly turning melodic riffs into textural noise, as well as blending the two. Notice that from values  0.0  to  20.0  there is a full spectrum from clean sine wave to noise feedback, any values above  30.0  will blend the two and are what I would consider 'extreme modulation'. In general usage during sets I tend to use the range 0.0 to 3.0, as anything above tends to be too noisy and interferes with the percussion i'm using.  //a pattern I use regularly with its feedback being modulated from 0 to 20. Notice the difference in sound across the spectrum\n(\n~sinfb = Pbind(\\instrument,\\sinfb,\\scale,Scale.minor,\\octave,[3,4,5],\\degree,Pseq([0,0,4,5],inf),\\dur,Pbjorklund2(3,8)/4,\\amp,0.3,\\fb,0.1,\\rel,0.3);\n~feedback = {SinOsc.kr(0.1,-1,1).range(0,20.0).poll(30)};\n~sinfb.set(\\fb,~feedback);\n~sinfb.play;\n)", 
            "title": "SinOscFB"
        }, 
        {
            "location": "/4-5-Between-Pitch-And-Noise/#harmonic-series-and-extreme-pitch-values", 
            "text": "In 4.2 I talked about the Harmonic Series. An interesting quality of using a fundamental frequency to determine the pitch of various NodeProxies by multiplying that fundamental frequency to create a scale structure.  Some interesting techniques for distorting this harmonic series technique into the territory of noise are extreme modulation, which pushes the frequency into supersonics (and sometimes back again):  //Extreme modulation of fundamental frequency\n//taking the up-down scale given in the 'riffs' section\n(\n//up\np.clock.tempo = 2.4;\n~r = {75};\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).reverse,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).scramble,inf)*Pkr(~r),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,2.0),\\rel,0.1);\n~sinfb1.play;~sinfb2.play;~sinfb3.play;\n)\n//moving the frequency up and beyond sensible into supersonics - after reading around 5000Hz some interesting aliasing starts to happen\n(\n~r = {XLine.kr(75,8000,60).poll(10)}\n)\n//and even further, lower frequencies start reappearing\n(\n~r = {XLine.kr(8000,30000,60).poll(10)};\n)\n//using very extreme modulation also gives some interesing results\n(\n~r = {LFNoise1.kr(0.2).range(30000,90000).poll(10)};\n)  And extreme pitch values - which appear to rise continually into supersonic frequencies and aliasing, and then looping back to the bottom of the pitch scale:  //extreme multiplaction of fundamental frequency\n//using the previous example, a NodeProxy holding a second multiplier is added onto the \\freq argument of each Pbind\n(\n~r = {75};\n~mult = {1};\n~sinfb1 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10),inf)*(Pkr(~r)*Pkr(~mult)),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb2 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).reverse,inf)*(Pkr(~r)*Pkr(~mult)),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,1.4),\\rel,0.1);\n~sinfb3 = Pbind(\\instrument,\\sinfb,\\freq,Pseq((1..10).scramble,inf)*(Pkr(~r)*Pkr(~mult)),\\dur,0.25,\\amp,0.3,\\fb,Pwhite(0.1,2.0),\\rel,0.1);\n~sinfb1.play;~sinfb2.play;~sinfb3.play;\n)\n//increase the multiplcation over time using a .round on a Line.kr UGen. Listen to how the scale is distorted as the multiplcation increases, eventually ending as a series of pulses\n(\n~mult = {Line.kr(1,60,60).round(1).poll(5)}\n)", 
            "title": "Harmonic series and extreme pitch values"
        }, 
        {
            "location": "/4-5-Between-Pitch-And-Noise/#chaos-ugens", 
            "text": "SuperCollider has support for UGens that use  Chaos Theory  for synthesis - the  Chaos UGens  (note that there are also a number of additional Chaos UGens in  sc3-plugins  which are worth having).  While (at the time of writing) I don't know a whole lot about the particularities of chaos theory works, but the Chaos UGens are great for creating musical structures that move freely between pitched sound and noise, and these are usually handled both in the equation variables of the UGens as well as the initial conditions.  I'll use HenonN as an example of the use of chaos theory to move between melody, noise and percussion:  //HenonN - Chaos synths and moving between pitch and noise\n(\n//henon using a minor pentatonic scale at a high octave.\n//The chaos Ugens will need some experimentations if you want subtle variance in sound\n//For Henon I found that an a value of 1.3 and a b value of 0.3 renders a pitch in a pattern pretty reliably\n//note that the pitches aren't quite the same as 'concert pitch'\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.3),\\b,Pexprand(0.3,0.3),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n~henon.play;\n)\n//increase the variation in the a and b arguments to add more noise to the mix\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.31),\\b,Pexprand(0.3,0.31),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n)\n//notice that this gets very noisy VERY fast.\n//adding a little more possiblity to the Pexprands in a and b turns it into pure noise very very fast, while still retaining a little of its pitched character\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.35),\\b,Pexprand(0.3,0.35),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n)\n//even more and noises become cut off and non-sounding.\n//the cut off sounds would sound as DC bias, but the SynthDef \\henon has a LeakDC on its output to prevent this as it can damage sound systems and is generally quite an unpleasant thing to deal with.\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,Pbjorklund2(3,8)/4,\\a,Pexprand(1.3,1.45),\\b,Pexprand(0.3,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.1,0.1),\\amp,1);\n)\n//at this point decreasing the \\dur and \\rel value turns it into rhythmic percussion\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,8,\\dur,0.25,\\a,Pexprand(1.3,1.45),\\b,Pexprand(0.3,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.01,0.1),\\amp,1);\n)\n//more extreme possible values - \\dur varied, octaves doubled up, more variation in a and b values, more octaves\n(\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,[8,12,9,10],\\dur,Pwrand([0.25,Pbjorklund2(Pwhite(3,5),8,1)/4,Pseq([0.125],4)],[7,4,1].normalizeSum,inf),\\a,Pexprand(1.2,1.55),\\b,Pexprand(0.21,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.01,0.6),\\amp,1);\n)\n//against a kick drum it takes on a really strange character\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ k ][0],\\dur,1,\\amp,1);\n~k.play;\n)  A thing to note about the Chaos synths is the type of interpolation used - taking Henon as an example; HenonC, HenonL and HenonN stand for Cubic, Linear and None respectively. The sonic effect of the type of interpolation used is in the 'smoothness' of the sound, with Cubic being the most smooth and None being the least.  //sound of different types of interpolation\n//the default in my SynthDefs.scd file is currently to use none:\n(\nSynthDef(\\henon,\n    {arg freq=440,mfreq=440,a=1.3,b=0.3,x0=0.30501993062401,y0=0.20938865431933,atk=0.01,sus=1,rel=1,ts=1,out=0,pan=0,amp=0.3;\n        var sig,env;\n        sig = Henon2DN.ar(freq,freq+mfreq,a,b,x0,y0,amp);\n        env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,ts,2);\n        sig = LeakDC.ar(sig);\n        sig = sig*env;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n);\n//the example earlier, with no interpolation (default)\n(\np.clock.tempo = 2.2;\n~henon = Pbind(\\instrument,\\henon,\\scale,Scale.minorPentatonic,\\degree,Pseq([0,2,4,6,7],inf),\\octave,[8,12,9,10],\\dur,Pwrand([0.25,Pbjorklund2(Pwhite(3,5),8,1)/4,Pseq([0.125],4)],[7,4,1].normalizeSum,inf),\\a,Pexprand(1.2,1.55),\\b,Pexprand(0.21,0.55),\\atk,0,\\sus,0,\\rel,Pexprand(0.01,0.6),\\amp,1);\n~henon.play;\n)\n//now with Linear interpolation\n(\nSynthDef(\\henon,\n    {arg freq=440,mfreq=440,a=1.3,b=0.3,x0=0.30501993062401,y0=0.20938865431933,atk=0.01,sus=1,rel=1,ts=1,out=0,pan=0,amp=0.3;\n        var sig,env;\n        sig = Henon2DL.ar(freq,freq+mfreq,a,b,x0,y0,amp);\n        env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,ts,2);\n        sig = LeakDC.ar(sig);\n        sig = sig*env;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n);\n//now with Cubic interpolation\n(\nSynthDef(\\henon,\n    {arg freq=440,mfreq=440,a=1.3,b=0.3,x0=0.30501993062401,y0=0.20938865431933,atk=0.01,sus=1,rel=1,ts=1,out=0,pan=0,amp=0.3;\n        var sig,env;\n        sig = Henon2DC.ar(freq,freq+mfreq,a,b,x0,y0,amp);\n        env = EnvGen.ar(Env.linen(atk,sus,rel),1,1,0,ts,2);\n        sig = LeakDC.ar(sig);\n        sig = sig*env;\n        Out.ar(out,Pan2.ar(sig,pan));\n}).add;\n);", 
            "title": "Chaos UGens"
        }, 
        {
            "location": "/4-6-Good-SynthDef-Writing-for-co34pt_LiveCode/", 
            "text": "Good SynthDef writing for co34pt_LiveCode\n\n\n\n\nI won't cover the fundamentals of \nsynthesis\n or \nsynthdef\n writing, as others have done so much better than I ever will.\n\n\nIf you're going to be writing SynthDefs for Patterns in the format I use in these guides and in my sets, there's a few rules to ensure that things run reasonably smoothly.\n\n\nIt's also worth reading the \nSynthDef documentation\n and \nPbind documentation\n\n\nfreq\n and frequency\n\n\nThe carrier of a main frequency of a SynthDef should have the argument name \nfreq\n - this will allow for the use of scales, tunings and detuning within Pattern arguments, from the documentation:\n\n\n\n\ndetunedFreq\nactual \"pitch\" of a synth, determined by:\nfreq + detune;\nfreq is determined by:\n(midinote + ctranspose).midicps * harmonic;\nmidinote is determined by:\n(note + gtranspose + root)/stepsPerOctave * octave * 12;\nnote is determined by:\n(degree + mtranspose).degreeToKey(scale, stepsPerOctave)\n\n\n\n\nThere are a couple of instances where you can't use \nfreq\n as the actual frequency, so in which case, use Pkey to reroute the frequency argument like this:\n\n\n//where x is the frequency argument\nPbind(\\instrument,\\foo,\\x,Pkey(\\freq),\\scale,Scale.minor,\\degree,Pseq([4,5,6],inf))\n\n\n\n\nout\n\n\nEach SynthDef should have an argument \nout\n in its \nOut.ar\n. I always leave it as 0, but it can be used to handle effects routing. I don't know why, but if it doesn't have it, it won't work inside of ProxySpace.\n\n\nEnvelopes\n\n\nEnvelopes will be automatically triggered as part of patterns, on the assumption that the trigger of any envelope is set to \n1\n. It's also much easier to use envelopes where it does not need a release trigger. I generally use \nEnv.perc\n and \nEnv.linen\n. It's also important to use a \ndoneAction\n which will free the synth once the envelope has completed.", 
            "title": "Good SynthDef Writing for co34pt_LiveCode"
        }, 
        {
            "location": "/4-6-Good-SynthDef-Writing-for-co34pt_LiveCode/#good-synthdef-writing-for-co34pt_livecode", 
            "text": "I won't cover the fundamentals of  synthesis  or  synthdef  writing, as others have done so much better than I ever will.  If you're going to be writing SynthDefs for Patterns in the format I use in these guides and in my sets, there's a few rules to ensure that things run reasonably smoothly.  It's also worth reading the  SynthDef documentation  and  Pbind documentation", 
            "title": "Good SynthDef writing for co34pt_LiveCode"
        }, 
        {
            "location": "/4-6-Good-SynthDef-Writing-for-co34pt_LiveCode/#freq-and-frequency", 
            "text": "The carrier of a main frequency of a SynthDef should have the argument name  freq  - this will allow for the use of scales, tunings and detuning within Pattern arguments, from the documentation:   detunedFreq\nactual \"pitch\" of a synth, determined by:\nfreq + detune;\nfreq is determined by:\n(midinote + ctranspose).midicps * harmonic;\nmidinote is determined by:\n(note + gtranspose + root)/stepsPerOctave * octave * 12;\nnote is determined by:\n(degree + mtranspose).degreeToKey(scale, stepsPerOctave)   There are a couple of instances where you can't use  freq  as the actual frequency, so in which case, use Pkey to reroute the frequency argument like this:  //where x is the frequency argument\nPbind(\\instrument,\\foo,\\x,Pkey(\\freq),\\scale,Scale.minor,\\degree,Pseq([4,5,6],inf))", 
            "title": "freq and frequency"
        }, 
        {
            "location": "/4-6-Good-SynthDef-Writing-for-co34pt_LiveCode/#out", 
            "text": "Each SynthDef should have an argument  out  in its  Out.ar . I always leave it as 0, but it can be used to handle effects routing. I don't know why, but if it doesn't have it, it won't work inside of ProxySpace.", 
            "title": "out"
        }, 
        {
            "location": "/4-6-Good-SynthDef-Writing-for-co34pt_LiveCode/#envelopes", 
            "text": "Envelopes will be automatically triggered as part of patterns, on the assumption that the trigger of any envelope is set to  1 . It's also much easier to use envelopes where it does not need a release trigger. I generally use  Env.perc  and  Env.linen . It's also important to use a  doneAction  which will free the synth once the envelope has completed.", 
            "title": "Envelopes"
        }, 
        {
            "location": "/4-7-MIDI/", 
            "text": "Sequencing MIDI using ProxySpace and Pbind\n\n\n\n\nI didn't get into live coding with MIDI initially, and it's only after a couple of years of performing that I decided to get a synth to work into my sets - and while the examples I am providing here \nshould\n work with any MIDI Synth, I've probably only tested them on mine (a \nMake Noise 0 Coast\n).\n\n\nFortunately it's relatively easy to get MIDI sequences working in conjunction with the standard ProxySpace patterns described all over this repo. I based these instructions on the ones in the \nPattern Guide Cookbook\n.\n\n\nIMPORTANT! - This is a guide for setting up MIDI using \nLinux\n. OSX is probably similar, but Windows I am really not too sure about.\n\n\nFirst you need to initialise MIDI on the server with \nMIDIClient.init\n. This will initialise MIDI on the server and print available MIDI devices to the post window, on my system they are listed as the following:\n\n\nMIDI Sources:\n    MIDIEndPoint(\nSystem\n, \nTimer\n)\n    MIDIEndPoint(\nSystem\n, \nAnnounce\n)\n    MIDIEndPoint(\nMidi Through\n, \nMidi Through Port-0\n)\n    MIDIEndPoint(\nScarlett 2i4 USB\n, \nScarlett 2i4 USB MIDI 1\n)\n    MIDIEndPoint(\nSuperCollider\n, \nout0\n)\n    MIDIEndPoint(\nSuperCollider\n, \nout1\n)\n    MIDIEndPoint(\nSuperCollider\n, \nout2\n)\n    MIDIEndPoint(\nSuperCollider\n, \nout3\n)\n    MIDIEndPoint(\nSuperCollider\n, \nout4\n)\n    MIDIEndPoint(\nSuperCollider\n, \nout5\n)\nMIDI Destinations:\n    MIDIEndPoint(\nMidi Through\n, \nMidi Through Port-0\n)\n    MIDIEndPoint(\nScarlett 2i4 USB\n, \nScarlett 2i4 USB MIDI 1\n)\n    MIDIEndPoint(\nTiMidity\n, \nTiMidity port 0\n)\n    MIDIEndPoint(\nTiMidity\n, \nTiMidity port 1\n)\n    MIDIEndPoint(\nTiMidity\n, \nTiMidity port 2\n)\n    MIDIEndPoint(\nTiMidity\n, \nTiMidity port 3\n)\n    MIDIEndPoint(\nSuperCollider\n, \nin0\n)\n    MIDIEndPoint(\nSuperCollider\n, \nin1\n)\n    MIDIEndPoint(\nSuperCollider\n, \nin2\n)\n    MIDIEndPoint(\nSuperCollider\n, \nin3\n)\n\n\n\n\nThen use the \nMIDIOut\n class to create a MIDI Output, specifying the MIDI output you would like to use as a string. I add this to the dictionary that I store samples in, like this:\n\n\nd[\\m2] = MIDIOut.newByName(\nScarlett 2i4 USB\n, \nScarlett 2i4 USB MIDI 1\n).latency = (0.2555)\n\n\n\n\nThe \nlatency\n method is used to create latency in the MIDI signal, in order to sync the MIDI notes played by SuperCollider to the latency of the audio server - this will need some tweaking (see the accompanying \n.scd\n file).\n\n\nMIDI sequences can then be sent from within ProxySpace as a \nPbind\n, the same as any other pattern, with a few extra values necessary:\n\n\n(\n~midiPattern = Pbind(\n    //specifies type of message sent\n    \\type, \\midi,\n    //specifies type of midi message\n    \\midicmd, \\noteOn,\n    //the MIDI Out used\n    \\midiout, d[\\m],\n  //the MIDI channel\n    \\chan, 0,\n    //The rest of the pattern\n    \\scale,Scale.minor,\n    \\degree, Pseq([0,2,4],inf),\n    \\octave, 3,\n    \\dur, 0.5,\n    \\legato, 0.4\n)\n)\n\n\n\n\nIf this doesn't work, there's possibly a routing issue. If you're using Linux, load up \nQjackctl\n, select \nconnect\n, then go to \nALSA\n and connect output \nSuperCollider\n to your MIDI interface:\n\n\n\n\nYou should now be patterning your MIDI device, Enjoy.\n\n\nI don't really like MIDI as a technology because it is quite restrictive, particularly  as it only takes 'note' messages rather than frequencies (messages are often limited to 0-127 ints). The result of this is that microtones of any kind are hard to specify. One way to create microtones is to use the \n\\bend\n feature, which takes values from \n0\n to \n16,383\n (with \n8,192\n being the middle, or default).\n\n\n(\n~midiBend = Pbind(\n\\type,\\midi,\n\\midicmd,\\bend,\n\\midiout,d[\\m],\n\\chan,0,\n\\dur,0.25,\n\\val,Pwhite(0,16383)\n)\n)\n\n\n\n\nThe amount that the pitch bend affects the pitch of the synth is set within the synth itself, in my case it is +/- 1 semitone. The code above results in a semi-microtonal scale, played out across one tone.\n\n\nNote that the pitch bend \ncannot\n be specified at the same time as the notes, it must be specified separately, for reasons I don't quite understand.\n\n\nIn the setup file of this repo I have included a Setup_MIDI file, for setting up the SuperCollider server and MIDI with one execution. This will need to be edited to your MIDI device.", 
            "title": "Using MIDI"
        }, 
        {
            "location": "/4-7-MIDI/#sequencing-midi-using-proxyspace-and-pbind", 
            "text": "I didn't get into live coding with MIDI initially, and it's only after a couple of years of performing that I decided to get a synth to work into my sets - and while the examples I am providing here  should  work with any MIDI Synth, I've probably only tested them on mine (a  Make Noise 0 Coast ).  Fortunately it's relatively easy to get MIDI sequences working in conjunction with the standard ProxySpace patterns described all over this repo. I based these instructions on the ones in the  Pattern Guide Cookbook .  IMPORTANT! - This is a guide for setting up MIDI using  Linux . OSX is probably similar, but Windows I am really not too sure about.  First you need to initialise MIDI on the server with  MIDIClient.init . This will initialise MIDI on the server and print available MIDI devices to the post window, on my system they are listed as the following:  MIDI Sources:\n    MIDIEndPoint( System ,  Timer )\n    MIDIEndPoint( System ,  Announce )\n    MIDIEndPoint( Midi Through ,  Midi Through Port-0 )\n    MIDIEndPoint( Scarlett 2i4 USB ,  Scarlett 2i4 USB MIDI 1 )\n    MIDIEndPoint( SuperCollider ,  out0 )\n    MIDIEndPoint( SuperCollider ,  out1 )\n    MIDIEndPoint( SuperCollider ,  out2 )\n    MIDIEndPoint( SuperCollider ,  out3 )\n    MIDIEndPoint( SuperCollider ,  out4 )\n    MIDIEndPoint( SuperCollider ,  out5 )\nMIDI Destinations:\n    MIDIEndPoint( Midi Through ,  Midi Through Port-0 )\n    MIDIEndPoint( Scarlett 2i4 USB ,  Scarlett 2i4 USB MIDI 1 )\n    MIDIEndPoint( TiMidity ,  TiMidity port 0 )\n    MIDIEndPoint( TiMidity ,  TiMidity port 1 )\n    MIDIEndPoint( TiMidity ,  TiMidity port 2 )\n    MIDIEndPoint( TiMidity ,  TiMidity port 3 )\n    MIDIEndPoint( SuperCollider ,  in0 )\n    MIDIEndPoint( SuperCollider ,  in1 )\n    MIDIEndPoint( SuperCollider ,  in2 )\n    MIDIEndPoint( SuperCollider ,  in3 )  Then use the  MIDIOut  class to create a MIDI Output, specifying the MIDI output you would like to use as a string. I add this to the dictionary that I store samples in, like this:  d[\\m2] = MIDIOut.newByName( Scarlett 2i4 USB ,  Scarlett 2i4 USB MIDI 1 ).latency = (0.2555)  The  latency  method is used to create latency in the MIDI signal, in order to sync the MIDI notes played by SuperCollider to the latency of the audio server - this will need some tweaking (see the accompanying  .scd  file).  MIDI sequences can then be sent from within ProxySpace as a  Pbind , the same as any other pattern, with a few extra values necessary:  (\n~midiPattern = Pbind(\n    //specifies type of message sent\n    \\type, \\midi,\n    //specifies type of midi message\n    \\midicmd, \\noteOn,\n    //the MIDI Out used\n    \\midiout, d[\\m],\n  //the MIDI channel\n    \\chan, 0,\n    //The rest of the pattern\n    \\scale,Scale.minor,\n    \\degree, Pseq([0,2,4],inf),\n    \\octave, 3,\n    \\dur, 0.5,\n    \\legato, 0.4\n)\n)  If this doesn't work, there's possibly a routing issue. If you're using Linux, load up  Qjackctl , select  connect , then go to  ALSA  and connect output  SuperCollider  to your MIDI interface:   You should now be patterning your MIDI device, Enjoy.  I don't really like MIDI as a technology because it is quite restrictive, particularly  as it only takes 'note' messages rather than frequencies (messages are often limited to 0-127 ints). The result of this is that microtones of any kind are hard to specify. One way to create microtones is to use the  \\bend  feature, which takes values from  0  to  16,383  (with  8,192  being the middle, or default).  (\n~midiBend = Pbind(\n\\type,\\midi,\n\\midicmd,\\bend,\n\\midiout,d[\\m],\n\\chan,0,\n\\dur,0.25,\n\\val,Pwhite(0,16383)\n)\n)  The amount that the pitch bend affects the pitch of the synth is set within the synth itself, in my case it is +/- 1 semitone. The code above results in a semi-microtonal scale, played out across one tone.  Note that the pitch bend  cannot  be specified at the same time as the notes, it must be specified separately, for reasons I don't quite understand.  In the setup file of this repo I have included a Setup_MIDI file, for setting up the SuperCollider server and MIDI with one execution. This will need to be edited to your MIDI device.", 
            "title": "Sequencing MIDI using ProxySpace and Pbind"
        }, 
        {
            "location": "/5-1-Drones/", 
            "text": "Drones\n\n\n\n\nDrones\n \nare\n \ngreat\n, both standing on their own as drone music or within other forms of music.\n\n\nI've always found SuperCollider to be a really strong tool for making drones of all kinds as the types of subtle, durational modulations that can be achieved with \n.kr\n UGens allows for the creation of drones that vary over time very easily. The variation of these drones makes the background of sets interesting without having to maintain them directly - especially if the modulation in multiple drones are out of sync for instance. This sustained background interest can keep a set moving forward while time is spent working on preparing foreground elements without the background becoming boring too quickly (which is a problem I've come across a lot when performing live coding sets).\n\n\nDFM1\n\n\nsc3-plugins\n contains a great filter - \nDFM1\n. A 'Digitally Modelled Analog Filter', it is packed with features. It can be used a high pass and low pass, has a variable noise setting, and can self-oscillate at high resonances.\n\n\nThe most important feature of this for me is the self-oscillation. When overdriven, DFM1 produces a gorgeous 'warm' tone, which tends to distort softly the harder it is driven.\n\n\nWhen this self-oscillating distortion is paired with a sine wave using the same fundamental frequency as the filter, some rich drones are created:\n\n\n/*\nA standard DFM1 drone I use an awful lot.\nThe filter self-oscillates at a 'res' value of \n1, so here I have used a SinOsc moving from 0.9-1.1, so the self-oscillated distortion fades in and out.\nHere I am using the harmonic series to organise pitch. with the frequency of the filter being double that of the SinOsc.\n!!!!NOTE!!!!! - In my installation of SuperCollider, DFM1 is buggy and NodeProxies it contains need to be evaluated twice slowly otherwise they will cut all sound from the server when played. I don't know why this is (or whether it is a version/platform/OS specific issue), but if the experience is any different for you please raise an issue on GitHub or otherwise let me know. This only happens once per NodeProxy, once it is initialised and playing it can be re-evaluated and changed with no effect on the sound in the rest of the server\n*/\n//set the fundamental frequency\n~r = {80}\n//evaluate this twice with a couple of seconds of gap in between\n//the stereo sine wave creates a 'beating' in stereo. For more information see https://en.wikipedia.org/wiki/Beat_(acoustics)\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n//play\n~dfm1.play;\n//changing the resonance changes the character of the self-oscillation, detuning it and distorting it\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,1.6),1,0,0.0003,0.5)};\n//The higher the resonance value gets, the more distortion\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,5.6),1,0,0.0003,0.5)};\n//extreme resonance values get LOUD, but don't really change sonically past around the 10 mark\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(1).range(10,400),1,0,0.0003,0.5)};\n\n//DFM1 multiple drones\n//Using the harmonic series technique, a number of drones at various multiplications layered together\n//Note - the modulation of the resonance is a slightly different speed for each, to create an overall variation and non-repetition in sound\n//set fundamental frequency\n~r = {54};\n(\n//evaluate this twice with a couple of seconds of gap in between\n//the argument 'mult' is used for speed - to copy and paste the entire NodeProxy and set multiplications quickly during performance\n~dfm1 = {arg mult = 1; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n~dfm2 = {arg mult = 2; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.06).range(0.9,1.1),1,0,0.0003,0.5)};\n~dfm3 = {arg mult = 3; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.056).range(0.9,1.1),1,0,0.0003,0.5)};\n~dfm4 = {arg mult = 4; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.07).range(0.9,1.1),1,0,0.0003,0.5)};\n)\n//now play all\n~dfm1.play;~dfm2.play;~dfm3.play;~dfm4.play;\n//changing modulation from a SinOsc to an LFNoise, increasing modulation scope in lower multiples\n(\n//evaluate this twice with a couple of seconds of gap in between\n//the argument 'mult' is used for speed - to copy and paste the entire NodeProxy and set multiplications quickly during performance\n//this sounds like distorted guitars and is VERY rich.\n~dfm1 = {arg mult = 1; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.05).range(0.9,4.5),1,0,0.0003,0.5)};\n~dfm2 = {arg mult = 2; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.06).range(0.9,2.3),1,0,0.0003,0.5)};\n~dfm3 = {arg mult = 3; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.056).range(0.9,1.9),1,0,0.0003,0.5)};\n~dfm4 = {arg mult = 4; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.07).range(0.9,1.5),1,0,0.0003,0.5)};\n)\n\n\n\n\nAnother way to use DFM1 as an oscillator is to run it up and down the harmonic series and use it as a 'melody' alongside some already running drones, and smooth it out by using the \nnoiselevel\n argument:\n\n\n//using DFM1 as a melody\n//set harmonic frequency\n~r = {60};\n//start the first drone from the first example in this document\n//evate this twice with a couple of seconds in between\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n//play\n~dfm1.play\n//another drone, but one that contains a LFNoise1 used to give sweeps around the harmonic series\n//evaluate this twice with a couple of seconds in between\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(0.1).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n//play\n~dfmharm.play;\n//up the resonance\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(0.1).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.4),1,0,0.0003,0.5)};\n//up the speed of pitch change\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(1.4).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.4),1,0,0.0003,0.5)};\n//up the noise\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(1.4).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.4),1,0,0.1,0.5)};", 
            "title": "Drones"
        }, 
        {
            "location": "/5-1-Drones/#drones", 
            "text": "Drones   are   great , both standing on their own as drone music or within other forms of music.  I've always found SuperCollider to be a really strong tool for making drones of all kinds as the types of subtle, durational modulations that can be achieved with  .kr  UGens allows for the creation of drones that vary over time very easily. The variation of these drones makes the background of sets interesting without having to maintain them directly - especially if the modulation in multiple drones are out of sync for instance. This sustained background interest can keep a set moving forward while time is spent working on preparing foreground elements without the background becoming boring too quickly (which is a problem I've come across a lot when performing live coding sets).", 
            "title": "Drones"
        }, 
        {
            "location": "/5-1-Drones/#dfm1", 
            "text": "sc3-plugins  contains a great filter -  DFM1 . A 'Digitally Modelled Analog Filter', it is packed with features. It can be used a high pass and low pass, has a variable noise setting, and can self-oscillate at high resonances.  The most important feature of this for me is the self-oscillation. When overdriven, DFM1 produces a gorgeous 'warm' tone, which tends to distort softly the harder it is driven.  When this self-oscillating distortion is paired with a sine wave using the same fundamental frequency as the filter, some rich drones are created:  /*\nA standard DFM1 drone I use an awful lot.\nThe filter self-oscillates at a 'res' value of  1, so here I have used a SinOsc moving from 0.9-1.1, so the self-oscillated distortion fades in and out.\nHere I am using the harmonic series to organise pitch. with the frequency of the filter being double that of the SinOsc.\n!!!!NOTE!!!!! - In my installation of SuperCollider, DFM1 is buggy and NodeProxies it contains need to be evaluated twice slowly otherwise they will cut all sound from the server when played. I don't know why this is (or whether it is a version/platform/OS specific issue), but if the experience is any different for you please raise an issue on GitHub or otherwise let me know. This only happens once per NodeProxy, once it is initialised and playing it can be re-evaluated and changed with no effect on the sound in the rest of the server\n*/\n//set the fundamental frequency\n~r = {80}\n//evaluate this twice with a couple of seconds of gap in between\n//the stereo sine wave creates a 'beating' in stereo. For more information see https://en.wikipedia.org/wiki/Beat_(acoustics)\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n//play\n~dfm1.play;\n//changing the resonance changes the character of the self-oscillation, detuning it and distorting it\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,1.6),1,0,0.0003,0.5)};\n//The higher the resonance value gets, the more distortion\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,5.6),1,0,0.0003,0.5)};\n//extreme resonance values get LOUD, but don't really change sonically past around the 10 mark\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(1).range(10,400),1,0,0.0003,0.5)};\n\n//DFM1 multiple drones\n//Using the harmonic series technique, a number of drones at various multiplications layered together\n//Note - the modulation of the resonance is a slightly different speed for each, to create an overall variation and non-repetition in sound\n//set fundamental frequency\n~r = {54};\n(\n//evaluate this twice with a couple of seconds of gap in between\n//the argument 'mult' is used for speed - to copy and paste the entire NodeProxy and set multiplications quickly during performance\n~dfm1 = {arg mult = 1; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n~dfm2 = {arg mult = 2; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.06).range(0.9,1.1),1,0,0.0003,0.5)};\n~dfm3 = {arg mult = 3; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.056).range(0.9,1.1),1,0,0.0003,0.5)};\n~dfm4 = {arg mult = 4; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,SinOsc.kr(0.07).range(0.9,1.1),1,0,0.0003,0.5)};\n)\n//now play all\n~dfm1.play;~dfm2.play;~dfm3.play;~dfm4.play;\n//changing modulation from a SinOsc to an LFNoise, increasing modulation scope in lower multiples\n(\n//evaluate this twice with a couple of seconds of gap in between\n//the argument 'mult' is used for speed - to copy and paste the entire NodeProxy and set multiplications quickly during performance\n//this sounds like distorted guitars and is VERY rich.\n~dfm1 = {arg mult = 1; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.05).range(0.9,4.5),1,0,0.0003,0.5)};\n~dfm2 = {arg mult = 2; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.06).range(0.9,2.3),1,0,0.0003,0.5)};\n~dfm3 = {arg mult = 3; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.056).range(0.9,1.9),1,0,0.0003,0.5)};\n~dfm4 = {arg mult = 4; DFM1.ar(SinOsc.ar([~r,~r*1.01]*mult,0,0.1),(~r*2)*mult,LFNoise1.kr(0.07).range(0.9,1.5),1,0,0.0003,0.5)};\n)  Another way to use DFM1 as an oscillator is to run it up and down the harmonic series and use it as a 'melody' alongside some already running drones, and smooth it out by using the  noiselevel  argument:  //using DFM1 as a melody\n//set harmonic frequency\n~r = {60};\n//start the first drone from the first example in this document\n//evate this twice with a couple of seconds in between\n~dfm1 = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),~r*2,SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n//play\n~dfm1.play\n//another drone, but one that contains a LFNoise1 used to give sweeps around the harmonic series\n//evaluate this twice with a couple of seconds in between\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(0.1).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.1),1,0,0.0003,0.5)};\n//play\n~dfmharm.play;\n//up the resonance\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(0.1).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.4),1,0,0.0003,0.5)};\n//up the speed of pitch change\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(1.4).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.4),1,0,0.0003,0.5)};\n//up the noise\n~dfmharm = {DFM1.ar(SinOsc.ar([~r,~r*1.01],0,0.1),LFNoise1.kr(1.4).range(100,1000).round(~r),SinOsc.kr(0.05).range(0.9,1.4),1,0,0.1,0.5)};", 
            "title": "DFM1"
        }, 
        {
            "location": "/5-2-SuperCollider-as-a-Modular-Synth/", 
            "text": "SuperCollider as a Modular Synth\n\n\n\n\nA performance technique I don't generally employ a whole lot during Algorave-type sets using SuperCollider as a \nmodular synth\n - and ProxySpace is \nvery\n strong in this regard too. Each NodeProxy can be seen as an individual module, and each module can be plugged into others to create a complex network of interconnected musical and control elements. This is achieved by setting up audio (.ar) and control (*.kr) proxies - for more info on audio vs control rate see \nthis\n and \nthis\n\n\nI can't really talk about this in any great depth, so here is an in-depth example of how SuperCollider can be used as a live-codeable modular synth. An important thing to note though is that if you want a lot of freedom in this approach, a lot of familiarity with types of UGens available (as well as some of the stranger quirks of SuperCollider syntax) will be very helpful.\n\n\n//load setup\n(\n../../Setup/Setup.scd\n).loadRelative\n\n//run this to smooth out transitions\np.fadeTime=5\n\n//Using SuperCollider as a Modular synth\n//snippets help with building these sets a LOT, as standard elements such as modulation signals can be called upon very quickly\n//NOTE: this will get !!! L O U D !!! - there's protection from StageLimiter of course, but be aware.\n//NOTE II: There may also be some DC bias. Be prepared for this. more information here - http://en.wikiaudio.org/DC_offset\n\n//a sine wave\n~sin = {SinOsc.ar([80,82],0,0.5)}\n//a pulse wave\n~pulse = {Pulse.ar([20,21],SinOsc.kr(0.1).range(0.01,1),0.5)}\n//a new proxy multiplying sine and pulse waves\n~sinpulse = {~sin.ar * ~pulse.ar}\n~sinpulse.play\n//feed this into a delay with its delay line modulated slightly\n~delay = {CombC.ar(~sinpulse.ar,1,LFNoise1.kr(0.1).range(0.1,0.3),4)}\n~delay.play\n//increase the pulse speed and decrease the width, play it alongside the original\n~pulse2 = {Pulse.ar([40,41],SinOsc.kr(0.1).range(0.001,0.1),0.5)}\n~pulse2.play;\n//actually no that would sound much better just in the delay, so ~pulse2 from playing and add it into ~delay by using Mix.ar\n(\n~pulse2.stop;\n~delay = {CombC.ar(Mix.ar([~sinpulse.ar,~pulse2.ar]),1,LFNoise1.kr(0.1).range(0.1,0.3),4)};\n)\n//now we have some drones, some heavily gated and filtered noise would be good.\n(\n~noise = {RLPF.ar(WhiteNoise.ar(1),LFNoise1.kr(0.1).range(100,2000),SinOsc.kr(0.1).range(0.1,0.4),1)};\n~noiseEnv = {EnvGen.ar(Env.perc(0.0001,0.1),Dust.kr(4))};\n~totalNoise = {~noise.ar*~noiseEnv.ar};\n~totalNoise.play;\n)\n//oh no. it is mono. i'm going to pan it over 2.\n//In order to make a mono proxy stereo, I will have to .clear it and then evaluate a stereo version, as the number of channels is set at initialisation time.\n//luckily with Pan2 I will only have to re-evaluate the ~totalNoise proxy\n~totalNoise.clear;\n(\n~totalNoise = {Pan2.ar(~noise.ar*~noiseEnv.ar,SinOsc.kr(0.1))};\n~totalNoise.play;\n)\n//the filtering on the noise isn't extreme enough, change it!\n~noise = {RLPF.ar(WhiteNoise.ar(1),LFNoise1.kr(0.6).range(100,2000),SinOsc.kr(0.04).range(0.00001,0.2),1)};\n//the noise could also do with some delay, which would sound nice if it was fed back through a pitch shifter:\n//set up the delay, and play it\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar]),1,0.4,7,1)}\n~noiseDelay.play;\n//establish the pitch shifter\n~pitchShift = {PitchShift.ar(~noiseDelay,0.2,TRand.kr(0.1,2,Dust.kr(0.5)))}\n//play the pitch shifter, it will slow the delay speed by half\n~pitchShift.play\n//if we then put the results of ~pitchShift back into ~noiseDelay, then things get interesting.\n//NB - this is bad practice and gets very loud before ending up in being DC bias, but i'm doing it here to prove a point.\n//If you have super high end audio equipment or just don't want any DC bias then skip this step\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,~pitchShift.ar]),1,0.4,7,1)}\n//in order to avoid this getting totally out of control, reduce the volume of ~pitchShift inside of ~noiseDelay\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*0.11)]),1,0.4,7,1)}\n//or modulate it to get varying amounts of feedback\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,0.4,7,1)}\n//modulating the delay time too will make things get a bit wild\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1)}\n//~noiseDelay seems to be glitching a bit and throwing DC bias - add a LeakDC around it\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))}\n//let's cut the original pulse/sine waves over a few seconds\n~delay.stop(20)\n~sinpulse.stop(20)\n//then put them inside of a DFM1 that can self-oscillate\n//make sure you evaluate ~noiseDelayAdd twice before you .play it\n~noiseDelayAdd = {DFM1.ar(Mix.ar([~delay.ar,~sinpulse.ar]),500,SinOsc.kr(0.1).range(0.5,2),1,0,0.03)}\n//if you've evaluated the above line twice, play it\n~noiseDelayAdd.play\n//a lot of these sounds are quite degraded, some harsh sounds would be nice, let's have some chaos\n//go to the help file for Henon2DC and copy-paste the second example but don't evaluate it (you'll need sc3-plugins for this)\n/*\n(\n{ Henon2DN.ar(\n    2200, 8800,\n    LFNoise2.kr(1, 0.2, 1.2),\n    LFNoise2.kr(1, 0.15, 0.15)\n) * 0.2 }.play(s);\n)\n*/\n//turn it into a node proxy and remove the .play(s) from the end\n(\n~henon = { Henon2DN.ar(\n    2200, 8800,\n    LFNoise2.kr(1, 0.2, 1.2),\n    LFNoise2.kr(1, 0.15, 0.15)\n) * 0.2 };\n)\n//make an envelope that has a long sweeping modulation on the amount of envelopes triggered\n~chaosEnv = {EnvGen.ar(Env.perc(0,0.02),Dust.kr(SinOsc.kr(0.01).range(1,10)))}\n//and combine in stereo\n~chaos = {Pan2.ar(~henon*~chaosEnv)}\n~chaos.play\n//it is SUPER quiet, up the volume on ~henon\n(\n~henon = { Henon2DN.ar(\n    2200, 8800,\n    LFNoise2.kr(1, 0.2, 1.2),\n    LFNoise2.kr(1, 0.15, 0.15)\n) * 3.5 };\n)\n//add some reverb which will work in parallel\n//if you want to change the parameters of any effect without re-evaluating it - set up that value as another NodeProxy\n~room = {30};\n~time = {3};\n~verb = {GVerb.ar(~chaosEnv,~room,~time)}\n~verb.play\n//increase the reverb time\n~time = {40};\n//this needs some melody - add two melodies in stereo, slightly out of phase:\n~saws = {LFSaw.ar([LFSaw.kr(0.1).range(100,1000).round(50),LFSaw.kr(0.11).range(100,1000).round(50)],0,0.3)}\n~saws.play\n//too harsh, needs filtering\n~saws = {RLPFD.ar(LFSaw.ar([LFSaw.kr(0.1).range(100,1000).round(50),LFSaw.kr(0.101).range(100,1000).round(50)],0,0.8),1000,0.8,0.6,10)};\n//another delay would be nice\n~sawDelay = {CombC.ar(~saws.ar,1,0.5,10)};\n~sawDelay.play;\n//some heavy decimation on the delay\n~sawDelay = {Decimator.ar(CombC.ar(~saws.ar,1,0.5,10),2200,10)};\n//further bit reduction\n~sawDelay = {Decimator.ar(CombC.ar(~saws.ar,1,0.5,10),2200,5)};\n//even further\n~sawDelay = {Decimator.ar(CombC.ar(~saws.ar,1,0.5,10),2020,3)};\n//plugging the ~sawDelay into the original for more noise\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))}\n//plugging ChaosEnv into ~noiseDelay too\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))};\n//then plugging it also into a more intense ~noiseDelayAdd for more mad effects\n~noiseDelayAdd = {DFM1.ar(Mix.ar([~delay.ar,~sinpulse.ar,~noiseDelay]),LFNoise1.kr(100).range(100,10000),SinOsc.kr(0.1).range(0.5,100),1,0,0.03)}\n~noiseDelayAdd.play\n//it doesn't appear to be playing, probablt because ~noiseDelay is SO loud. Multiply it by half\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1)) * 0.3};\n//then plug ~noiseDelayAdd into ~noiseDelay and roll off the multiplication for maximum damage\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs),~noiseDelayAdd.ar]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))};\n//increase the ridiculousness of the modulation of the delaytime\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs),~noiseDelayAdd.ar]),1,LFNoise1.kr(1).range(0.001,4),7,1))};\n//put another delay on top of that?\n~delay2 = {CombC.ar(~noiseDelay.ar,1,0.4,30)}\n~delay2.play\n//then plug that back into ~noiseDelay (which by now contains most things that are playing.\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs),~noiseDelayAdd.ar,~delay2.ar]),1,LFNoise1.kr(1).range(0.001,4),7,1))};\n//also modulate ~delay2, really slowly\n~delay2 = {LeakDC.ar(CombC.ar(~noiseDelay.ar,1,SinOsc.kr(0.01).range(0.0001,0.2),80))}\n//things broke up for me here and I have no idea why, there's multiple things feeding back through each other here.\n//and you have noise music!", 
            "title": "SuperCollider as a Modular Synth"
        }, 
        {
            "location": "/5-2-SuperCollider-as-a-Modular-Synth/#supercollider-as-a-modular-synth", 
            "text": "A performance technique I don't generally employ a whole lot during Algorave-type sets using SuperCollider as a  modular synth  - and ProxySpace is  very  strong in this regard too. Each NodeProxy can be seen as an individual module, and each module can be plugged into others to create a complex network of interconnected musical and control elements. This is achieved by setting up audio (.ar) and control (*.kr) proxies - for more info on audio vs control rate see  this  and  this  I can't really talk about this in any great depth, so here is an in-depth example of how SuperCollider can be used as a live-codeable modular synth. An important thing to note though is that if you want a lot of freedom in this approach, a lot of familiarity with types of UGens available (as well as some of the stranger quirks of SuperCollider syntax) will be very helpful.  //load setup\n( ../../Setup/Setup.scd ).loadRelative\n\n//run this to smooth out transitions\np.fadeTime=5\n\n//Using SuperCollider as a Modular synth\n//snippets help with building these sets a LOT, as standard elements such as modulation signals can be called upon very quickly\n//NOTE: this will get !!! L O U D !!! - there's protection from StageLimiter of course, but be aware.\n//NOTE II: There may also be some DC bias. Be prepared for this. more information here - http://en.wikiaudio.org/DC_offset\n\n//a sine wave\n~sin = {SinOsc.ar([80,82],0,0.5)}\n//a pulse wave\n~pulse = {Pulse.ar([20,21],SinOsc.kr(0.1).range(0.01,1),0.5)}\n//a new proxy multiplying sine and pulse waves\n~sinpulse = {~sin.ar * ~pulse.ar}\n~sinpulse.play\n//feed this into a delay with its delay line modulated slightly\n~delay = {CombC.ar(~sinpulse.ar,1,LFNoise1.kr(0.1).range(0.1,0.3),4)}\n~delay.play\n//increase the pulse speed and decrease the width, play it alongside the original\n~pulse2 = {Pulse.ar([40,41],SinOsc.kr(0.1).range(0.001,0.1),0.5)}\n~pulse2.play;\n//actually no that would sound much better just in the delay, so ~pulse2 from playing and add it into ~delay by using Mix.ar\n(\n~pulse2.stop;\n~delay = {CombC.ar(Mix.ar([~sinpulse.ar,~pulse2.ar]),1,LFNoise1.kr(0.1).range(0.1,0.3),4)};\n)\n//now we have some drones, some heavily gated and filtered noise would be good.\n(\n~noise = {RLPF.ar(WhiteNoise.ar(1),LFNoise1.kr(0.1).range(100,2000),SinOsc.kr(0.1).range(0.1,0.4),1)};\n~noiseEnv = {EnvGen.ar(Env.perc(0.0001,0.1),Dust.kr(4))};\n~totalNoise = {~noise.ar*~noiseEnv.ar};\n~totalNoise.play;\n)\n//oh no. it is mono. i'm going to pan it over 2.\n//In order to make a mono proxy stereo, I will have to .clear it and then evaluate a stereo version, as the number of channels is set at initialisation time.\n//luckily with Pan2 I will only have to re-evaluate the ~totalNoise proxy\n~totalNoise.clear;\n(\n~totalNoise = {Pan2.ar(~noise.ar*~noiseEnv.ar,SinOsc.kr(0.1))};\n~totalNoise.play;\n)\n//the filtering on the noise isn't extreme enough, change it!\n~noise = {RLPF.ar(WhiteNoise.ar(1),LFNoise1.kr(0.6).range(100,2000),SinOsc.kr(0.04).range(0.00001,0.2),1)};\n//the noise could also do with some delay, which would sound nice if it was fed back through a pitch shifter:\n//set up the delay, and play it\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar]),1,0.4,7,1)}\n~noiseDelay.play;\n//establish the pitch shifter\n~pitchShift = {PitchShift.ar(~noiseDelay,0.2,TRand.kr(0.1,2,Dust.kr(0.5)))}\n//play the pitch shifter, it will slow the delay speed by half\n~pitchShift.play\n//if we then put the results of ~pitchShift back into ~noiseDelay, then things get interesting.\n//NB - this is bad practice and gets very loud before ending up in being DC bias, but i'm doing it here to prove a point.\n//If you have super high end audio equipment or just don't want any DC bias then skip this step\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,~pitchShift.ar]),1,0.4,7,1)}\n//in order to avoid this getting totally out of control, reduce the volume of ~pitchShift inside of ~noiseDelay\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*0.11)]),1,0.4,7,1)}\n//or modulate it to get varying amounts of feedback\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,0.4,7,1)}\n//modulating the delay time too will make things get a bit wild\n~noiseDelay = {CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1)}\n//~noiseDelay seems to be glitching a bit and throwing DC bias - add a LeakDC around it\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))}\n//let's cut the original pulse/sine waves over a few seconds\n~delay.stop(20)\n~sinpulse.stop(20)\n//then put them inside of a DFM1 that can self-oscillate\n//make sure you evaluate ~noiseDelayAdd twice before you .play it\n~noiseDelayAdd = {DFM1.ar(Mix.ar([~delay.ar,~sinpulse.ar]),500,SinOsc.kr(0.1).range(0.5,2),1,0,0.03)}\n//if you've evaluated the above line twice, play it\n~noiseDelayAdd.play\n//a lot of these sounds are quite degraded, some harsh sounds would be nice, let's have some chaos\n//go to the help file for Henon2DC and copy-paste the second example but don't evaluate it (you'll need sc3-plugins for this)\n/*\n(\n{ Henon2DN.ar(\n    2200, 8800,\n    LFNoise2.kr(1, 0.2, 1.2),\n    LFNoise2.kr(1, 0.15, 0.15)\n) * 0.2 }.play(s);\n)\n*/\n//turn it into a node proxy and remove the .play(s) from the end\n(\n~henon = { Henon2DN.ar(\n    2200, 8800,\n    LFNoise2.kr(1, 0.2, 1.2),\n    LFNoise2.kr(1, 0.15, 0.15)\n) * 0.2 };\n)\n//make an envelope that has a long sweeping modulation on the amount of envelopes triggered\n~chaosEnv = {EnvGen.ar(Env.perc(0,0.02),Dust.kr(SinOsc.kr(0.01).range(1,10)))}\n//and combine in stereo\n~chaos = {Pan2.ar(~henon*~chaosEnv)}\n~chaos.play\n//it is SUPER quiet, up the volume on ~henon\n(\n~henon = { Henon2DN.ar(\n    2200, 8800,\n    LFNoise2.kr(1, 0.2, 1.2),\n    LFNoise2.kr(1, 0.15, 0.15)\n) * 3.5 };\n)\n//add some reverb which will work in parallel\n//if you want to change the parameters of any effect without re-evaluating it - set up that value as another NodeProxy\n~room = {30};\n~time = {3};\n~verb = {GVerb.ar(~chaosEnv,~room,~time)}\n~verb.play\n//increase the reverb time\n~time = {40};\n//this needs some melody - add two melodies in stereo, slightly out of phase:\n~saws = {LFSaw.ar([LFSaw.kr(0.1).range(100,1000).round(50),LFSaw.kr(0.11).range(100,1000).round(50)],0,0.3)}\n~saws.play\n//too harsh, needs filtering\n~saws = {RLPFD.ar(LFSaw.ar([LFSaw.kr(0.1).range(100,1000).round(50),LFSaw.kr(0.101).range(100,1000).round(50)],0,0.8),1000,0.8,0.6,10)};\n//another delay would be nice\n~sawDelay = {CombC.ar(~saws.ar,1,0.5,10)};\n~sawDelay.play;\n//some heavy decimation on the delay\n~sawDelay = {Decimator.ar(CombC.ar(~saws.ar,1,0.5,10),2200,10)};\n//further bit reduction\n~sawDelay = {Decimator.ar(CombC.ar(~saws.ar,1,0.5,10),2200,5)};\n//even further\n~sawDelay = {Decimator.ar(CombC.ar(~saws.ar,1,0.5,10),2020,3)};\n//plugging the ~sawDelay into the original for more noise\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))}\n//plugging ChaosEnv into ~noiseDelay too\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))};\n//then plugging it also into a more intense ~noiseDelayAdd for more mad effects\n~noiseDelayAdd = {DFM1.ar(Mix.ar([~delay.ar,~sinpulse.ar,~noiseDelay]),LFNoise1.kr(100).range(100,10000),SinOsc.kr(0.1).range(0.5,100),1,0,0.03)}\n~noiseDelayAdd.play\n//it doesn't appear to be playing, probablt because ~noiseDelay is SO loud. Multiply it by half\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs)]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1)) * 0.3};\n//then plug ~noiseDelayAdd into ~noiseDelay and roll off the multiplication for maximum damage\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs),~noiseDelayAdd.ar]),1,LFNoise1.kr(0.1).range(0.01,0.6),7,1))};\n//increase the ridiculousness of the modulation of the delaytime\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs),~noiseDelayAdd.ar]),1,LFNoise1.kr(1).range(0.001,4),7,1))};\n//put another delay on top of that?\n~delay2 = {CombC.ar(~noiseDelay.ar,1,0.4,30)}\n~delay2.play\n//then plug that back into ~noiseDelay (which by now contains most things that are playing.\n~noiseDelay = {LeakDC.ar(CombC.ar(Mix.ar([~chaosEnv.ar,~sawDelay.ar,~totalNoise.ar,(~pitchShift.ar*LFNoise1.kr(0.01,0.2).abs),~noiseDelayAdd.ar,~delay2.ar]),1,LFNoise1.kr(1).range(0.001,4),7,1))};\n//also modulate ~delay2, really slowly\n~delay2 = {LeakDC.ar(CombC.ar(~noiseDelay.ar,1,SinOsc.kr(0.01).range(0.0001,0.2),80))}\n//things broke up for me here and I have no idea why, there's multiple things feeding back through each other here.\n//and you have noise music!", 
            "title": "SuperCollider as a Modular Synth"
        }, 
        {
            "location": "/6-1-FreqScope-and-Visuals/", 
            "text": "FreqScope and Visuals\n\n\n\n\nNote: This guide is in general terms because it is platform-specific. I'd recommend some research on how this can be realised on your particular platform\n\n\nI really like having visuals as part of my sets, I think it adds a lot of energy to sets, regardless of how 'audio-responsive' they are. In addition to adding a bunch of colour to my projection, it gives some relief from just looking at code, and can serve as a low-budget light show in absence of actual lighting.\n\n\nI have written my own programs to make visuals for sets before in \nopenFrameworks\n, which was a lot of effort. While this in itself was not an issue, I find it \nextremely\n difficult to live code visuals and sound at the same time, as it involves a lot of parallel thinking, which disturbs my flow when live coding music. What I've found is that SuperCollider's \nFreqScope\n is a great way of instantly adding visuals to sets with very little actual effort.\n\n\nInside ProxySpace, a \nFreqScope\n can be started to monitor all sound by evaluating \ns.scope\n (which is contained within this repo's \nSetup.scd\n. This will give an oscilloscope-type visualisation of the sound currently taking place, and can be shown as independent channels, an overlay, or an x/y chart of the sound on a stereo spectrum. My technique is to fullscreen the \nFreqScope\n window, and drop it behind my SuperCollider IDE window, and make the IDE window semi transparent with a black background (the black background is especially important as it will not tint the scope), showing the scope behind the code I am writing (\nas can be seen here\n). This is an effort-free way to get some responsive visuals which work alongside my code which do not need attention themselves. I won't post any guides here on how to make your IDE transparent, as this depends entirely on your platform. I found it quite hard to do on Mac OSX, and quite easy on Linux (but a little harder in Ubuntu Unity than my current KDE). I usually do this in \nTracks\n mode, although \nOverlay\n works too.\n\n\nX/Y\n is where things get more interesting. This mode plots stereo sound on a two-dimensional plane by frequency and amplitude to form a geometric shape rather than a wave. The best example of how this works can be seen in \nthis Techmoan video\n (or anything that can be found by googling Oscilloscope Music). \nX/Y\n mode can be a great way to create music that directly results in interesting visual forms by using complimentary frequencies across the stereo field. The specifics of this revolve around the harmonic series and different types of intonation \nwhich is explained in this video\n. The shapes made can be changed by the type of waves used, as well as the volume and frequency, and performing according to this is an interesting way of shaking up one's performance strategies, as normal performance techniques will not yield interesting shapes, \nhere is X/Y used as visuals on a project entirely sounding entirely sine waves\n and here are a couple of more clear code examples:\n\n\n//Example 1 - Static Frequencies\n(\n//two low sine waves at the same frequency showing a diagonal line\n~sin1 = {SinOsc.ar([80,80],0,0.3)};\n~sin1.play;\n)\n//two low sine waves at slightly different frequencies turning the line into a slowly turning disc\n~sin1 = {SinOsc.ar([80,80.1],0,0.3)};\n(\n//two sine waves at double the frequency - notice the change in shape - turning the line a number of times on itself\n~sin2 = {SinOsc.ar([80*2,80.01*2],0,0.3)};\n~sin2.play;\n)\n(\n//two sine waves at 10x the frequency - notice the change in shape - turning the line a whole bunch more times on itself\n~sin3 = {SinOsc.ar([80*10,80.01*10],0,0.3)};\n~sin3.play;\n)\n//stop everything\n~sin1.stop;~sin2.stop;~sin3.stop;\n(\n//changing the frequency difference in the lower sine waves, changing how the original circle moves\n~sin1 = {SinOsc.ar([80,80+LFNoise1.kr(0.1,4)],0,0.3)};\n~sin1.play;\n)\n//replay the other sine waves and see how the entire shape moves faster\n~sin2.play;~sin3.play;\n//stop the highest sines\n~sin3.stop;\n(\n//re-align the two low sine waves\n~sin1 = {SinOsc.ar([80,80.01],0,0.3)};\n~sin1.play;\n)\n(\n//play a sine that doesn't align with the harmonic series, notice that the shape gets much less clear\n~sin4 = {SinOsc.ar([94.234,99.1315],0,0.3)};\n~sin4.play;\n)\n//stop the non-aligning sines\n~sin4.stop;\n//stop the second sine\n~sin2.stop;\n//play some quiet width-modulated pulse waves at 2x the frequency of the low sine waves\n//notice that the shape changes according to the width of the pulse and that the 'notches' interact with each other across the stereo field\n(\n~pulse1 = {Pulse.ar([80*4,80.1*4],SinOsc.kr(0.05).abs,0.08)};\n~pulse1.play;\n)\n//change the pulse to a saw wave at the same frequency\n(\n~pulse1.stop;\n~saw1 = {Saw.ar([80*4,80.1*4],0.08)};\n~saw1.play;\n)\n//note that the higher the volume, the greater the effect a sound has on the overall shape\n~saw1 = {Saw.ar([80*4,80.1*4],0.08)};\n//also the higher the frequency, the lesser the effect on the 'overall' shape and the greater the effect on the 'detail' of the shape\n~saw1 = {Saw.ar([80*100,80.1*100],0.1)};\n//stop everything\n~sin1.stop;~saw1.stop;\n\n//Example 2 - Moving frequencies and non-standard waveforms\n//make a (really) low sine wave/spinning disc again\n(\n~sin1 = {SinOsc.ar([50,50.01],0,0.4)};\n~sin1.play;\n)\n//make a stereo sine wave that sweeps the harmonic series\n(\n~sin2 = {SinOsc.ar(Saw.kr(0.1).range(10,1000).round(50),0,0.4)!2};\n~sin2.play;\n)\n//make those two sine waves sweep the harmonic series at phasing (sightly different) rates\n(\n~sin2 = {SinOsc.ar(Saw.kr([0.1,0.11]).range(10,1000).round(50),0,0.4)};\n~sin2.play;\n)\n//turn off the original sine wave\n~sin1.stop\n//speed the sweeping and make it a sine wave\n~sin2 = {SinOsc.ar(SinOsc.kr([0.5,0.56]).range(10,1000).round(50),0,0.4)};\n//make two meandering SinOscFB Ugens around the lower end of the harmonic series and see how they interact\n(\n~sinfb1 = {SinOscFB.ar([LFNoise1.kr(0.1).range(50,100).round(25),LFNoise1.kr(0.1).range(50,100).round(25)],SinOsc.kr(0.1).range(0.01,1),0.8)};\n~sinfb1.play;\n)\n//stop the second sine waves\n~sin2.stop\n//make a big sub kick drum - notice the effect on the shape\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[\nsk\n][0],\\dur,4,\\amp,1);\n~k.play\n)\n//make a panned hi-hat\n(\n~h = Pbind(\\instrument,\\bplay,\\buf,d[\nch\n][0],\\dur,0.25,\\amp,Pexprand(0.05,1),\\pan,Pwhite(-1,1.0));\n~h.play;\n)\n//make the feedback in the sinfb much more pronounced\n~sinfb1 = {SinOscFB.ar([LFNoise1.kr(0.1).range(50,100).round(25),LFNoise1.kr(0.1).range(50,100).round(25)],SinOsc.kr(0.1).range(0.01,3),0.8)};", 
            "title": "FreqScope and Visuals"
        }, 
        {
            "location": "/6-1-FreqScope-and-Visuals/#freqscope-and-visuals", 
            "text": "Note: This guide is in general terms because it is platform-specific. I'd recommend some research on how this can be realised on your particular platform  I really like having visuals as part of my sets, I think it adds a lot of energy to sets, regardless of how 'audio-responsive' they are. In addition to adding a bunch of colour to my projection, it gives some relief from just looking at code, and can serve as a low-budget light show in absence of actual lighting.  I have written my own programs to make visuals for sets before in  openFrameworks , which was a lot of effort. While this in itself was not an issue, I find it  extremely  difficult to live code visuals and sound at the same time, as it involves a lot of parallel thinking, which disturbs my flow when live coding music. What I've found is that SuperCollider's  FreqScope  is a great way of instantly adding visuals to sets with very little actual effort.  Inside ProxySpace, a  FreqScope  can be started to monitor all sound by evaluating  s.scope  (which is contained within this repo's  Setup.scd . This will give an oscilloscope-type visualisation of the sound currently taking place, and can be shown as independent channels, an overlay, or an x/y chart of the sound on a stereo spectrum. My technique is to fullscreen the  FreqScope  window, and drop it behind my SuperCollider IDE window, and make the IDE window semi transparent with a black background (the black background is especially important as it will not tint the scope), showing the scope behind the code I am writing ( as can be seen here ). This is an effort-free way to get some responsive visuals which work alongside my code which do not need attention themselves. I won't post any guides here on how to make your IDE transparent, as this depends entirely on your platform. I found it quite hard to do on Mac OSX, and quite easy on Linux (but a little harder in Ubuntu Unity than my current KDE). I usually do this in  Tracks  mode, although  Overlay  works too.  X/Y  is where things get more interesting. This mode plots stereo sound on a two-dimensional plane by frequency and amplitude to form a geometric shape rather than a wave. The best example of how this works can be seen in  this Techmoan video  (or anything that can be found by googling Oscilloscope Music).  X/Y  mode can be a great way to create music that directly results in interesting visual forms by using complimentary frequencies across the stereo field. The specifics of this revolve around the harmonic series and different types of intonation  which is explained in this video . The shapes made can be changed by the type of waves used, as well as the volume and frequency, and performing according to this is an interesting way of shaking up one's performance strategies, as normal performance techniques will not yield interesting shapes,  here is X/Y used as visuals on a project entirely sounding entirely sine waves  and here are a couple of more clear code examples:  //Example 1 - Static Frequencies\n(\n//two low sine waves at the same frequency showing a diagonal line\n~sin1 = {SinOsc.ar([80,80],0,0.3)};\n~sin1.play;\n)\n//two low sine waves at slightly different frequencies turning the line into a slowly turning disc\n~sin1 = {SinOsc.ar([80,80.1],0,0.3)};\n(\n//two sine waves at double the frequency - notice the change in shape - turning the line a number of times on itself\n~sin2 = {SinOsc.ar([80*2,80.01*2],0,0.3)};\n~sin2.play;\n)\n(\n//two sine waves at 10x the frequency - notice the change in shape - turning the line a whole bunch more times on itself\n~sin3 = {SinOsc.ar([80*10,80.01*10],0,0.3)};\n~sin3.play;\n)\n//stop everything\n~sin1.stop;~sin2.stop;~sin3.stop;\n(\n//changing the frequency difference in the lower sine waves, changing how the original circle moves\n~sin1 = {SinOsc.ar([80,80+LFNoise1.kr(0.1,4)],0,0.3)};\n~sin1.play;\n)\n//replay the other sine waves and see how the entire shape moves faster\n~sin2.play;~sin3.play;\n//stop the highest sines\n~sin3.stop;\n(\n//re-align the two low sine waves\n~sin1 = {SinOsc.ar([80,80.01],0,0.3)};\n~sin1.play;\n)\n(\n//play a sine that doesn't align with the harmonic series, notice that the shape gets much less clear\n~sin4 = {SinOsc.ar([94.234,99.1315],0,0.3)};\n~sin4.play;\n)\n//stop the non-aligning sines\n~sin4.stop;\n//stop the second sine\n~sin2.stop;\n//play some quiet width-modulated pulse waves at 2x the frequency of the low sine waves\n//notice that the shape changes according to the width of the pulse and that the 'notches' interact with each other across the stereo field\n(\n~pulse1 = {Pulse.ar([80*4,80.1*4],SinOsc.kr(0.05).abs,0.08)};\n~pulse1.play;\n)\n//change the pulse to a saw wave at the same frequency\n(\n~pulse1.stop;\n~saw1 = {Saw.ar([80*4,80.1*4],0.08)};\n~saw1.play;\n)\n//note that the higher the volume, the greater the effect a sound has on the overall shape\n~saw1 = {Saw.ar([80*4,80.1*4],0.08)};\n//also the higher the frequency, the lesser the effect on the 'overall' shape and the greater the effect on the 'detail' of the shape\n~saw1 = {Saw.ar([80*100,80.1*100],0.1)};\n//stop everything\n~sin1.stop;~saw1.stop;\n\n//Example 2 - Moving frequencies and non-standard waveforms\n//make a (really) low sine wave/spinning disc again\n(\n~sin1 = {SinOsc.ar([50,50.01],0,0.4)};\n~sin1.play;\n)\n//make a stereo sine wave that sweeps the harmonic series\n(\n~sin2 = {SinOsc.ar(Saw.kr(0.1).range(10,1000).round(50),0,0.4)!2};\n~sin2.play;\n)\n//make those two sine waves sweep the harmonic series at phasing (sightly different) rates\n(\n~sin2 = {SinOsc.ar(Saw.kr([0.1,0.11]).range(10,1000).round(50),0,0.4)};\n~sin2.play;\n)\n//turn off the original sine wave\n~sin1.stop\n//speed the sweeping and make it a sine wave\n~sin2 = {SinOsc.ar(SinOsc.kr([0.5,0.56]).range(10,1000).round(50),0,0.4)};\n//make two meandering SinOscFB Ugens around the lower end of the harmonic series and see how they interact\n(\n~sinfb1 = {SinOscFB.ar([LFNoise1.kr(0.1).range(50,100).round(25),LFNoise1.kr(0.1).range(50,100).round(25)],SinOsc.kr(0.1).range(0.01,1),0.8)};\n~sinfb1.play;\n)\n//stop the second sine waves\n~sin2.stop\n//make a big sub kick drum - notice the effect on the shape\n(\n~k = Pbind(\\instrument,\\bplay,\\buf,d[ sk ][0],\\dur,4,\\amp,1);\n~k.play\n)\n//make a panned hi-hat\n(\n~h = Pbind(\\instrument,\\bplay,\\buf,d[ ch ][0],\\dur,0.25,\\amp,Pexprand(0.05,1),\\pan,Pwhite(-1,1.0));\n~h.play;\n)\n//make the feedback in the sinfb much more pronounced\n~sinfb1 = {SinOscFB.ar([LFNoise1.kr(0.1).range(50,100).round(25),LFNoise1.kr(0.1).range(50,100).round(25)],SinOsc.kr(0.1).range(0.01,3),0.8)};", 
            "title": "FreqScope and Visuals"
        }, 
        {
            "location": "/6-2-OSC-and-Data-Streams/", 
            "text": "OSC Communication and Data Streams\n\n\n\n\nI have done a set of performances that revolve around live coding and using data streams in the past, particularly performances where I have a continuous stream of data to interpret during the performance. These performances involve negotiating my relationship with sensor data (e.g. movement data, temperature, light levels), live-coding my interpretation of this data to deliver a performance.\n\n\nFor this type of performance I have used a particular set of technologies mutliple times, and have developed a reasonably quick way to work, which I will share here. Depending on the kind of data you want to use, some of the items described in this part of the guide may not apply to you directly, but using OSC to handle messages in SuperCollider is a really useful skill that can apply to many types of situation, so it's worth knowing if you want to work outside of SuperCollider at all.\n\n\nOpen Sound Control\n (hereafter OSC) is an \nabsurdly\n useful protocol for communicating between programs, and across networks.\n\n\nBefore I learned how to use it I heard people refer to it a lot as 'modern MIDI', which I think is a bit of a misnomer and actually confused me quite a lot while learning it as someone used to MIDI a la DAWs and plugging cables into synthesizers. While MIDI is a set of commonly understood messages between programs (plug a MIDI cable into an interface and you can expect your DAW to react in a certain way), OSC is more of a 'common language' that enables programs to communicate effectively. OSC is very useful for getting multiple programs and machines to 'talk' to each other, and I have found it very useful for performances involving multiple programs and machines running together.\n\n\nFor example:\n\n\nsampler-sampler\n uses OSC to communicate information about emulated stitching between two machines and multiple programs:\n\n\nMACHINE 1: Processing -\n MACHINE 2: SuperCollider -\n Processing\n\n\ntome.\n uses OSC to parse sensor data and manage lighting.\n\n\nSensor array (serial data) -\n [Python Serial Parser](https://github.com/theseanco/python-SerialToOSC) -\n SuperCollider -\n QLCPlus -\n OpenDMXUSB\n\n\nWhile the above setups might seem complex or convoluted, using OSC makes these connections very easy, and using OSC is very similar across platforms.\n\n\nIt's first worth understanding a bit about how OSC sends its messages:\n\n\nOSC Messages are sent over a network, and that network can be internally within a machine (to sent messages between programs), or across machines in a network of any kind (commonly a local network). Messages are sent to a particular port of a particular network address (for example, 127.0.0.1, port 51720), with an address (for example /hello), and parameters that can be \nof various types\n (for example 1, 32.32, 'message').\n\n\nSending this message from SCLang to be received by SCLang internally would look like this (adapted from the \nOSC Communication tutorial\n):\n\n\n//monitor all incoming OSC Messages\nOSCFunc.trace;\n//set the relevant IP and port - both arbitrary, but these will be sent to SuperCollider internally (assuming that NetAddr.langPort == 57120)\nb = NetAddr.new(\n127.0.0.1\n, 57120);\n//send the above message, and it should be shown in the post window\nb.sendMsg(\n/hello\n, 1, 32.32, 'message')\n// If this doesn't work, evaluate:\nNetAddr.langPort\n// Then change the port of NetAddr.new accordingly\n\n\n\n\nThis is the basic way to send OSC Messages using SuperCollider. These messages can be sent to any IP and port, and the message will be sent regardless whether or not it is received. In order for the message to mean anything, a receiver will have to be built to interpret the message.\n\n\nTaking the above example, here is a simple setup that will make a sound every time a message is sent to address \n/ding\n, it uses a class called \nOSCdef\n which triggers a particular function when an OSC message is received:\n\n\n// set address\nb = NetAddr.new(\n127.0.0.1\n,NetAddr.langPort);\n// create OSCdef (very similar syntax to SynthDef)\n(\nOSCdef(\\dinger,\n    {\n    // a simple function that triggers an envelope\n        {Pulse.ar(1000,rrand(0.01,0.5),0.3)!2 * EnvGen.ar(Env.perc,doneAction:2)}.play\n}, '/ding')\n)\n// Send a message with no parameters. It'll trigger the function within the OSCdef.\nb.sendMsg(\n/ding\n)\n\n\n\n\nThere are a few tools for diagnosing issues with OSC use in SuperCollider, and we have touched on both of them here. To check if messages from another application are being received correctly, evaluate \nOSCFunc.trace(true)\n, which will print all incoming OSC messages to the post window (incluing any internal communications within sclang). If you are expecting to recieve messages to SuperCollider and they're not coming through (the default port of 57120 is where I usually direct all my messages), evaluate \nNetAddr.langPort\n to check the internal server port, as it can be re-assigned through multiple instances of SClang.\n\n\nMessages sent over OSC can also be interpreted and passed into these functions, here is an elaboration on the above example, using a message to set the pitch of the sound:\n\n\n//set address (if you've already done this no need to do it again)\nb = NetAddr.new(\n127.0.0.1\n,NetAddr.langPort);\n//msg will receive the OSC message as an array, with index 0 being the address and index 1 onwards being the message.\n//setting msg[1] as the frequency will give the first parameter of the OSC message as an argument\n//setting msg[2] as the pulse width would allow you to send the second message parameter as the pulse width, and so on...\n(\nOSCdef(\\dinger,\n    {\n        |msg|\n        {Pulse.ar(msg[1],rrand(0.01,0.5),0.3)!2 * EnvGen.ar(Env.perc,doneAction:2)}.play\n}, '/ding')\n)\n//make a 900Hz ding\nb.sendMsg(\n/ding\n,900);\n//make a ding at a random pitch\nb.sendMsg(\n/ding\n,rrand(100,2000))\n\n\n\n\nIn terms of using live data, and live coding your response to the data, the OSCdef can be changed and re-evaluated on the fly, changing data mappings and using OSCdefs to send messages to various items running on the server, and this fits into ProxySpace very nicely. In order to use live data however, you need a live data source, which is not readily available from within SuperCollider - check the examples folder for a Python script which simulates a live data input to be used in a live coding context, covering inter-program communication and live-mapping of data.\n\n\nIf you are wanting to use data from an Arduino to get data into SuperCollider, I wrote \nthis tool\n, which generates Python scripts based on a specification you provide that parses Serial data and sends it as a high-speed OSC stream, for which you can build custom OSCdefs in SuperCollider.", 
            "title": "OSC and Data Streams"
        }, 
        {
            "location": "/6-2-OSC-and-Data-Streams/#osc-communication-and-data-streams", 
            "text": "I have done a set of performances that revolve around live coding and using data streams in the past, particularly performances where I have a continuous stream of data to interpret during the performance. These performances involve negotiating my relationship with sensor data (e.g. movement data, temperature, light levels), live-coding my interpretation of this data to deliver a performance.  For this type of performance I have used a particular set of technologies mutliple times, and have developed a reasonably quick way to work, which I will share here. Depending on the kind of data you want to use, some of the items described in this part of the guide may not apply to you directly, but using OSC to handle messages in SuperCollider is a really useful skill that can apply to many types of situation, so it's worth knowing if you want to work outside of SuperCollider at all.  Open Sound Control  (hereafter OSC) is an  absurdly  useful protocol for communicating between programs, and across networks.  Before I learned how to use it I heard people refer to it a lot as 'modern MIDI', which I think is a bit of a misnomer and actually confused me quite a lot while learning it as someone used to MIDI a la DAWs and plugging cables into synthesizers. While MIDI is a set of commonly understood messages between programs (plug a MIDI cable into an interface and you can expect your DAW to react in a certain way), OSC is more of a 'common language' that enables programs to communicate effectively. OSC is very useful for getting multiple programs and machines to 'talk' to each other, and I have found it very useful for performances involving multiple programs and machines running together.  For example:  sampler-sampler  uses OSC to communicate information about emulated stitching between two machines and multiple programs:  MACHINE 1: Processing -  MACHINE 2: SuperCollider -  Processing  tome.  uses OSC to parse sensor data and manage lighting.  Sensor array (serial data) -  [Python Serial Parser](https://github.com/theseanco/python-SerialToOSC) -  SuperCollider -  QLCPlus -  OpenDMXUSB  While the above setups might seem complex or convoluted, using OSC makes these connections very easy, and using OSC is very similar across platforms.  It's first worth understanding a bit about how OSC sends its messages:  OSC Messages are sent over a network, and that network can be internally within a machine (to sent messages between programs), or across machines in a network of any kind (commonly a local network). Messages are sent to a particular port of a particular network address (for example, 127.0.0.1, port 51720), with an address (for example /hello), and parameters that can be  of various types  (for example 1, 32.32, 'message').  Sending this message from SCLang to be received by SCLang internally would look like this (adapted from the  OSC Communication tutorial ):  //monitor all incoming OSC Messages\nOSCFunc.trace;\n//set the relevant IP and port - both arbitrary, but these will be sent to SuperCollider internally (assuming that NetAddr.langPort == 57120)\nb = NetAddr.new( 127.0.0.1 , 57120);\n//send the above message, and it should be shown in the post window\nb.sendMsg( /hello , 1, 32.32, 'message')\n// If this doesn't work, evaluate:\nNetAddr.langPort\n// Then change the port of NetAddr.new accordingly  This is the basic way to send OSC Messages using SuperCollider. These messages can be sent to any IP and port, and the message will be sent regardless whether or not it is received. In order for the message to mean anything, a receiver will have to be built to interpret the message.  Taking the above example, here is a simple setup that will make a sound every time a message is sent to address  /ding , it uses a class called  OSCdef  which triggers a particular function when an OSC message is received:  // set address\nb = NetAddr.new( 127.0.0.1 ,NetAddr.langPort);\n// create OSCdef (very similar syntax to SynthDef)\n(\nOSCdef(\\dinger,\n    {\n    // a simple function that triggers an envelope\n        {Pulse.ar(1000,rrand(0.01,0.5),0.3)!2 * EnvGen.ar(Env.perc,doneAction:2)}.play\n}, '/ding')\n)\n// Send a message with no parameters. It'll trigger the function within the OSCdef.\nb.sendMsg( /ding )  There are a few tools for diagnosing issues with OSC use in SuperCollider, and we have touched on both of them here. To check if messages from another application are being received correctly, evaluate  OSCFunc.trace(true) , which will print all incoming OSC messages to the post window (incluing any internal communications within sclang). If you are expecting to recieve messages to SuperCollider and they're not coming through (the default port of 57120 is where I usually direct all my messages), evaluate  NetAddr.langPort  to check the internal server port, as it can be re-assigned through multiple instances of SClang.  Messages sent over OSC can also be interpreted and passed into these functions, here is an elaboration on the above example, using a message to set the pitch of the sound:  //set address (if you've already done this no need to do it again)\nb = NetAddr.new( 127.0.0.1 ,NetAddr.langPort);\n//msg will receive the OSC message as an array, with index 0 being the address and index 1 onwards being the message.\n//setting msg[1] as the frequency will give the first parameter of the OSC message as an argument\n//setting msg[2] as the pulse width would allow you to send the second message parameter as the pulse width, and so on...\n(\nOSCdef(\\dinger,\n    {\n        |msg|\n        {Pulse.ar(msg[1],rrand(0.01,0.5),0.3)!2 * EnvGen.ar(Env.perc,doneAction:2)}.play\n}, '/ding')\n)\n//make a 900Hz ding\nb.sendMsg( /ding ,900);\n//make a ding at a random pitch\nb.sendMsg( /ding ,rrand(100,2000))  In terms of using live data, and live coding your response to the data, the OSCdef can be changed and re-evaluated on the fly, changing data mappings and using OSCdefs to send messages to various items running on the server, and this fits into ProxySpace very nicely. In order to use live data however, you need a live data source, which is not readily available from within SuperCollider - check the examples folder for a Python script which simulates a live data input to be used in a live coding context, covering inter-program communication and live-mapping of data.  If you are wanting to use data from an Arduino to get data into SuperCollider, I wrote  this tool , which generates Python scripts based on a specification you provide that parses Serial data and sends it as a high-speed OSC stream, for which you can build custom OSCdefs in SuperCollider.", 
            "title": "OSC Communication and Data Streams"
        }, 
        {
            "location": "/6-3-Using-Datasets/", 
            "text": "Using Datasets\n\n\n\n\nAs well as using live data, using static datasets is another technique for using external inputs.\n\n\nI've used datasets during live coding sets in the past, including for the first half of my \nChemical Algorave\n set. I've also used SuperCollider to create some works using sonification of static data sets, including \nthis\n.\n\n\nThere are a number of ways to interpret datasets as sound, part of a technique commonly referred to as Sonification.\n\n\nThere's a great resource on Sonification here\n, but i'll cover the techniques that I have used to leverage Data in SuperCollider here, for which you should refer to the example in this repo.", 
            "title": "Using Datasets"
        }, 
        {
            "location": "/6-3-Using-Datasets/#using-datasets", 
            "text": "As well as using live data, using static datasets is another technique for using external inputs.  I've used datasets during live coding sets in the past, including for the first half of my  Chemical Algorave  set. I've also used SuperCollider to create some works using sonification of static data sets, including  this .  There are a number of ways to interpret datasets as sound, part of a technique commonly referred to as Sonification.  There's a great resource on Sonification here , but i'll cover the techniques that I have used to leverage Data in SuperCollider here, for which you should refer to the example in this repo.", 
            "title": "Using Datasets"
        }
    ]
}